<?xml version="1.0" encoding="UTF-8"?>
<catalog>
	<researchers id="0001">
		<name>Marco Vieira</name>
		<area>Informatics Engineering</area>
		<affiliation>CISUC</affiliation>
		<affiliation>University of Coimbra</affiliation>
		<research_interests>
			<interest>Dependability</interest>
			<interest>Dependable Computing</interest>
			<interest>Software Reliability</interest>
			<interest>Software Security</interest>
			<interest>Benchmarking</interest>
		</research_interests>
		<email>mvieira@dei.uc.pt</email>
		<number_articles>364</number_articles>
		
		<publications id="0001">
			<title>Mapping software faults with web security vulnerabilities</title>
			<date>2008</date>
			<citation>99</citation>
			<co_authors>
				<author>Jose Fonseca</author>
			</co_authors>
			<publication_source>
				<conference>2008 IEEE international conference on dependable systems and networks With FTCS and DCC (DSN)</conference>
				<pag_min>257</pag_min>
				<pag_max>266</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>Web applications are typically developed with hard time constraints and are often deployed with critical software bugs, making them vulnerable to attacks. The classification and knowledge of the typical software bugs that lead to security vulnerabilities is of utmost importance. This paper presents a field study analyzing 655 security patches of six widely used web applications. Results are compared against other field studies on general software faults (i.e., faults not specifically related to security),showing that only a small subset of software fault types is related to security. Furthermore, the detailed analysis of the code of the patches has shown that web application vulnerabilities result from software bugs affecting a restricted collection of statements. A detailed analysis of the conditions/locations where each fault was observed in our field study is presented allowing future definition of realistic fault models that cause security vulnerabilities in web applications, which is the key element to design a realistic attack injector.</description>
		</publications>
		
		<publications id="0002">
			<title>Evaluating computer intrusion detection systems: A survey of common practices</title>
			<date>2015</date>
			<citation>249</citation>
			<co_authors>
				<author>Aleksandar Milenkoski</author>
				<author>Samuel Kounev</author> 
				<author>Alberto Avritzer</author> 
				<author>Bryan D Payne</author>
			</co_authors>
			<publication_source>
				<source>ACM Computing Surveys (CSUR)</source>
				<volume>48</volume>
				<issue>1</issue>
				<pag_min>1</pag_min>
				<pag_max>41</pag_max>
			</publication_source>
			<publisher>ACM</publisher>
			<description>The evaluation of computer intrusion detection systems (which we refer to as intrusion detection systems) is an active research area. In this article, we survey and systematize common practices in the area of evaluation of such systems. For this purpose, we define a design space structured into three parts: workload, metrics, and measurement methodology. We then provide an overview of the common practices in evaluation of intrusion detection systems by surveying evaluation approaches and methods related to each part of the design space. Finally, we discuss open issues and challenges focusing on evaluation methodologies for novel intrusion detection systems.</description>
		</publications>
		
		<publications id="0003">
			<title>Using web security scanners to detect vulnerabilities in web services</title>
			<date>2009</date>
			<citation>221</citation>
			<co_authors>
				<author>Nuno Antunes</author> 
				<author>Henrique Madeira</author>
			</co_authors>
			<publication_source>
				<conference>2009 IEEE/IFIP International Conference on Dependable Systems and Networks</conference>
				<pag_min>566</pag_min>
				<pag_max>571</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>Although Web services are becoming business-critical components, they are often deployed with critical software bugs that can be maliciously explored. Web vulnerability scanners allow detecting security vulnerabilities in Web services by stressing the service from the point of view of an attacker. However, research and practice show that different scanners have different performance on vulnerabilities detection. In this paper we present an experimental evaluation of security vulnerabilities in 300 publicly available Web services. Four well known vulnerability scanners have been used to identify security flaws in Web services implementations. A large number of vulnerabilities has been observed, which confirms that many services are deployed without proper security testing. Additionally, the differences in the vulnerabilities detected and the high number of false-positives (35% and 40% in two cases) and low coverage (less than 20% for two of the scanners) observed highlight the limitations of Web vulnerability scanners on detecting security vulnerabilities in Web services.</description>
		</publications>
		
		<publications id="0004">
			<title>On the emulation of software faults by software fault injection</title>
			<date>2000</date>
			<citation>216</citation>
			<co_authors>
				<author>Henrique Madeira</author> 
				<author>Diamantino Costa</author>
			</co_authors>
			<publication_source>
				<conference>Proceeding International Conference on Dependable Systems and Networks. DSN 2000</conference>
				<pag_min>417</pag_min>
				<pag_max>426</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>This paper presents an experimental study on the emulation of software faults by fault injection. In a first experiment, a set of real software faults has been compared with faults injected by a SWIFI tool (Xception) to evaluate the accuracy of the injected faults. Results revealed the limitations of Xception (and other SWIFI tools) in the emulation of different classes of software faults (about 44% of the software faults cannot be emulated). The use of field data about real faults was discussed and software metrics were suggested as an alternative to guide the injection process when field data is nor available. In a second experiment, a set of rules for the injection of errors meant to emulate classes of software faults was evaluated. The fault triggers used seem to be the cause for the observed strong impact of the faults in the target system and in the program results. The results also show the influence in the fault emulation of aspects such as code size, complexity of data structures, and recursive versus sequential execution.</description>
		</publications>
		
		<publications id="0005">
			<title>A data masking technique for data warehouses</title>
			<date>2011</date>
			<citation>36</citation>
			<co_authors>
				<author>Ricardo Jorge Santos</author> 
				<author>Jorge Bernardino</author>
			</co_authors>
			<publication_source>
				<book>Proceedings of the 15th Symposium on International Database Engineering and Applications</book>
				<pag_min>61</pag_min>
				<pag_max>69</pag_max>
			</publication_source>
			<description>Data Warehouses (DWs) are the enterprise's most valuable asset in what concerns critical business information, making them an appealing target for attackers. Packaged database encryption solutions are considered the best solution to protect sensitive data. However, given the volume of data typically processed by DW queries, the existing encryption solutions heavily increase storage space and introduce very large overheads in query response time, due to decryption costs. In many cases, this performance degradation makes encryption unfeasible for use in DWs. In this paper we propose a transparent data masking solution for numerical values in DWs based on the mathematical modulus operator, which can be used without changing user application and DBMS source code. Our solution provides strong data securitywhile introducing small overheads in both storage space and database performance. Several experimental evaluations using the TPC-H decision support benchmark and a real-world DW are included. The results show the overall efficiency of our proposal, demonstrating that it is a valid alternative to existing standard encryption routines for enforcing data confidentiality in DWs.</description>
		</publications>
	
	
		<publications id="0006">
			<title>Designing vulnerability testing tools for web services: approach, components, and tools</title>
			<date>2017</date>
			<citation>34</citation>
			<co_authors>
				<author>Nuno Antunes</author> 
			</co_authors>
			<publication_source>
				<journal>International Journal of Information Security</journal>
				<volume>16</volume>
				<pag_min>435</pag_min>
				<pag_max>457</pag_max>
			</publication_source>
			<publisher>Springer Berlin Heidelberg</publisher>
			<description>This paper proposes a generic approach for designing vulnerability testing tools for web services, which includes the definition of the testing procedure and the tool components. Based on the proposed approach, we present the design of three innovative testing tools that implement three complementary techniques (improved penetration testing, attack signatures and interface monitoring, and runtime anomaly  detection) for detecting injection vulnerabilities, thus offering an extensive support for different scenarios.  A case study has been designed to demonstrate the tools for the particular case of SQL Injection vulnerabilities. The experimental evaluation demonstrates that the tools can effectively be used in different scenarios and that they  outperform well-known commercial tools by achieving higher detection coverage and lower false-positive rates.</description>
		</publications>
		
		<publications id="0007">
				<title>A survey on data security in data warehousing: Issues, challenges and opportunities</title>
				<date>2011</date>
				<citation>32</citation>
				<co_authors>
					<author>Ricardo Jorge Santos</author> 
					<author>Jorge Bernardino</author>
				</co_authors>
				<publication_source>
					<source>2011 IEEE EUROCON-International Conference on Computer as a Tool</source>
					<pag_min>1</pag_min>
					<pag_max>4</pag_max>
				</publication_source>
				<publisher>IEEE</publisher>
				<description>Data Warehouses (DWs) are the enterprise's most valuable assets in what concerns critical business information, making them an appealing target for malicious inside and outside attackers. Given the volume of data and the nature of DW queries, most of the existing data security solutions for databases are inefficient, consuming too many resources and introducing too much overhead in query response time, or resulting in too many false positive alarms (i.e., incorrect detection of attacks) to be checked. In this paper, we present a survey on currently available data security techniques, focusing on specific issues and requirements concerning their use in data warehousing environments. We also point out challenges and opportunities for future research work in this field.</description>
		</publications>
	</researchers>
	
	
	
	
	
	
	<researchers id="0002">
		<name>Nuno Antunes</name>
		<area>Information Sciences and Technologies</area>
		<affiliation>University of Coimbra</affiliation>
		<research_interests>
			<interest>Software Security</interest> 
			<interest>Dependable Systems</interest> 
			<interest>Cloud Security</interest> 
			<interest>Software Vulnerabilities</interest>
		</research_interests>
		<email>nmsa@dei.uc.pt</email>
		<number_articles>158</number_articles>
		
		<publications id="0001">
			<title>Comparing the effectiveness of penetration testing and static code analysis on the detection of sql injection vulnerabilities in web services</title>
			<date>2009</date>
			<citation>141</citation>
			<co_authors>
				<author>Marco Vieira</author>
			</co_authors>
			<publication_source>
				<conference>2009 15th IEEE pacific rim international symposium on dependable computing</conference>
				<pag_min>301</pag_min>
				<pag_max>306</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>Web services are becoming business-critical components that must provide a non-vulnerabl interface to the client applications. However, previous research and practice show that many web servicesare deployed with critical vulnerabilities. SQL injection vulnerabilities are particularly relevant, as Web services frequently access a relational database using SQL commands. Penetration testing and static code analysis are two well-know techniques often used for the detection of security vulnerabilities. In this work we compare how effective these two techniques are on the detection of SQL injection vulnerabilitiesin Web services code. To understand the strengths and limitations of these techniques, we used several commercial and open source tools to detect vulnerabilities in a set of vulnerable services. Results suggest that, in general, static code analyzers are able to detect more SQL injection vulnerabilities than penetrationtesting tools. Another key observation is that tools implementing the same detection approach frequently detect different vulnerabilities. Finally, many tools provide a low coverage and a high false positives rate, making them a bad option for programmers.</description>
		</publications>
		
		<publications id="0002">
			<title>Benchmarking vulnerability detection tools for web services</title>
			<date>2010</date>
			<citation>103</citation>
			<co_authors>
				<author>Marco Vieira</author>
			</co_authors>
			<publication_source>
				<conference>2010 IEEE International Conference on Web Services</conference>
				<pag_min>203</pag_min>
				<pag_max>210</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>Vulnerability detection tools are frequently considered the silver-bullet for detecting vulnerabilities in web services. However, research shows that the effectiveness of most of those tools is very low and that using the wrong tool may lead to the deployment of services with undetected vulnerabilities. In this paper we propose a benchmarking approach to assess and compare the effectiveness of vulnerability detection tools in web services environments. This approach was used to define a concrete benchmark for SQL Injection vulnerability detection tools. This benchmark is demonstrated by a real example of benchmarking several widely used tools, including four penetration-testers, three static code analyzers, and one anomaly detector. Results show that the benchmark accurately portrays the effectiveness of vulnerability detection tools and suggest that the proposed approach can be applied in the field.</description>
		</publications>
		
		<publications id="0003">
			<title>Experience report: Evaluating the effectiveness of decision trees for detecting code smells</title>
			<date>2015</date>
			<citation>73</citation>
			<co_authors>
				<author>Lucas Amorim</author>
				<author>Evandro Costa</author>
				<author>Baldoino Fonseca</author>
				<author>Marcio Ribeiro</author>
			</co_authors>
			<publication_source>
				<conference>2015 IEEE 26th international symposium on software reliability engineering (ISSRE)</conference>
				<pag_min>261</pag_min>
				<pag_max>269</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>Developers continuously maintain software systems to adapt to new requirements and to fix bugs. Due to the complexity of maintenance tasks and the time-to-market, developers make poor implementation choices, also known as code smells. Studies indicate that code smells hinder comprehensibility, and possibly increase change- and fault-proneness. Therefore, they must be identified to enable the application of corrections. The challenge is that the inaccurate definitions of code smells make developers disagree whether a piece of code is a smell or not, consequently, making difficult creation of a universal detection solution able to recognize smells in different software projects. Several works have been proposed to identify code smells but they still report inaccurate results and use techniques that do not present to developers a comprehensive explanation how these results have been obtained. In this experimental report we study the effectiveness of the Decision Tree algorithm to recognize code smells. For this, it was applied in a dataset containing 4 open source projects and the results were compared with the manual oracle, with existing detection approaches and with other machine learning algorithms. The results showed that the approach was able to effectively learn rules for the detection of the code smells studied. The results were even better when genetic algorithms are used to pre-select the metrics to use.</description>
		</publications>
		
		<publications id="0004">
			<title>Experimenting machine learning techniques to predict vulnerabilities</title>
			<date>2016</date>
			<citation>47</citation>
			<co_authors>
				<author>Henrique Alves</author>
				<author>Baldoino Fonseca</author>
			</co_authors>
			<publication_source>
				<conference>2016 Seventh Latin-American Symposium on Dependable Computing (LADC)</conference>
				<pag_min>151</pag_min>
				<pag_max>156</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>Software metrics can be used as a indicator of the presence of software vulnerabilities. These metrics have been used with machine learning to predict source code prone to contain vulnerabilities. Although it is not possible to find the exact location of the flaws, the models can show which components require more attention during inspections and testing. Each new technique uses his own evaluation dataset, which many times has limited size and representativeness. In this experience report, we use a large and representative dataset to evaluate several state of the art vulnerability prediction techniques. This dataset was built with information of 2186 vulnerabilities from five widely used open source projects. Results show that the dataset can be used to distinguish which are the best techniques. It is also shown that some of the techniques can predict nearly all of the vulnerabilities present in the dataset, although with very low precisions. Finally, accuracy, precision and recall are not the most effective to characterize the effectiveness of this tools.</description>
		</publications>
		
		<publications id="0005">
			<title>Software metrics and security vulnerabilities: dataset and exploratory study</title>
			<date>2016</date>
			<citation>66</citation>
			<co_authors>
				<author>Henrique Alves</author>
				<author>Baldoino Fonseca</author>
			</co_authors>
			<publication_source>
				<conference>2016 12th European Dependable Computing Conference (EDCC)</conference>
				<pag_min>37</pag_min>
				<pag_max>44</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>CCode with certain characteristics is more prone to have security vulnerabilities. In fact, studies show that code not following best practices is harder to verify and maintain, and consequently is more probable to have vulnerabilities left unnoticed or inadvertently introduced. In this experience report, we study whether software metrics can reflect such characteristics, thus having some correlation with the existence of vulnerabilities. The analysis is based on 2875 security patches, used to build a dataset with metrics and vulnerabilities for all the functions, classes and files of 5750 versions of five widely used projects that are exposed to attacks: Linux Kernel, Mozilla, Xen Hypervisor, httpd and glibc. We calculated software metrics from their sources and used correlation algorithm and statistical tests on these metrics in order to identify relations between them and the existing vulnerabilities. Results show that software metrics are able to discriminate vulnerable and non vulnerable functions, but it is not possible to find strong correlations between these metrics and the number of vulnerabilities existing in the analyzed functions. Finally, the results indicate that vulnerable functions are probable to have other vulnerabilities in the future.</description>
		</publications>
		
		<publications id="0006">
			<title>Penetration testing for web services</title>
			<date>2013</date>
			<citation>40</citation>
			<co_authors>
				<author>Marco Vieira</author>
			</co_authors>
			<publication_source>
				<journal>Computer</journal>
				<volume>47</volume>
				<issue>2</issue>
				<pag_min>30</pag_min>
				<pag_max>36</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>Web services are often deployed with critical software security faults that open them to malicious attack. Penetration testing using commercially available automated tools can help avoid such faults, but new analysis of several popular testing tools reveals significant failings in their performance. The Web extra at http://youtu.be/COgKs9e679o is an audio interview in which authors Nuno Antunes and Marco Vieira describe how their analysis of popular testing tools revealed significant performance failures and provided important insights for future improvement.</description>
		</publications>
		
		<publications id="0007">
			<title>Practical evaluation of static analysis tools for cryptography: Benchmarking method and case study</title>
			<date>2017</date>
			<citation>23</citation>
			<co_authors>
				<author>Alexandre Braga</author>
				<author>Ricardo Dahab</author>
				<author>Nuno Laranjeiro</author>
				<author>Marco Vieira</author>
			</co_authors>
			<publication_source>
				<conference>2017 IEEE 28th International Symposium on Software Reliability Engineering (ISSRE)</conference>
				<pag_min>170</pag_min>
				<pag_max>181</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>The incorrect use of cryptography is a common source of critical software vulnerabilities. As developers lack knowledge in applied cryptography and support from experts is scarce, this situation is frequently addressed by adopting static code analysis tools to automatically detect cryptography misuse during coding and reviews, even if the effectiveness of such tools is far from being well understood. This paper proposes a method for benchmarking static code analysis tools for the detection of cryptography misuse, and evaluates the method in a case study, with the goal of selecting the most adequate tools for specific development contexts. Our method classifies cryptography misuse in nine categories recognized by developers (weak cryptography, poor key management, bad randomness, etc.) and provides the workload, metrics and procedure needed for a fair assessment and comparison of tools. We found that all evaluated tools together detected only 35% of cryptography misuses in our tests. Furthermore, none of the evaluated tools detected insecure elliptic curves, weak parameters in key agreement, and most insecure configurations for RSA and ECDSA. This suggests cryptography misuse is underestimated by tool builders. Despite that, we show that it is possible to benefit from an adequate tool selection during the development of cryptographic software.</description>
		</publications>
	</researchers>
	
	
	
	
	
	<researchers id="0003">
		<name>Claudia Gomes Silva</name>
		<area>Chemical and Biological Engineering</area>
		<affiliation>University of Porto</affiliation>
		<affiliation>Laboratory of Separation and Reaction Engineering</affiliation>
		<research_interests>
			<interest>Heterogeneous catalysis</interest> 
			<interest>Photocatalysis</interest> 
			<interest>Chemical Engineering</interest> 
			<interest>Thin Films and Nanotechnology</interest>
		</research_interests>
		<email>cgsilva@fe.up.pt</email>
		<number_articles>595</number_articles>
		
		<publications id="0001">
			<title>Metal–organic frameworks as semiconductors</title>
			<date>2010</date>
			<citation>482</citation>
			<co_authors>
				<author>Avelino Corma</author>
				<author>Hermenegildo Garcia</author>
			</co_authors>
			<publication_source>
				<journal>Journal of Materials Chemistry</journal>
				<volume>20</volume>
				<issue>16</issue>
				<pag_min>3141</pag_min>
				<pag_max>3156</pag_max>
			</publication_source>
			<publisher>Royal Society of Chemistry</publisher>
			<description>The aim of the present feature article is to present the current evidence in support of considering some MOFs as semiconductors. While MOFs and zeolites share common structural properties derived from the microporous crystal structure, zeolites are insulating materials and most of the attempts to exploit them in optoelectronics have met with failure. In contrast, some MOFs may have interesting photochemical properties that derive from the fundamental event of charge separation in electrons and holes upon light absorption. Photoinduced charge separation is the hallmark of a semiconductor that can behave simultaneously as an oxidizing or reducing agent. Considering the novelty of this field, most of the available data about MOFs as semiconductor have been obtained from MOF-5, a case that is complicated due to its low structural stability. Therefore, we point out that further studies showing the semiconducting properties of other MOFs are still welcome. The purpose of this feature article is to trigger intense research in this area including the synthesis of semiconducting MOFs by design and development of applications.</description>
		</publications>
		
		<publications id="0002">
			<title>Ce-doped TiO2 for photocatalytic degradation of chlorophenol</title>
			<date>2009</date>
			<citation>208</citation>
			<co_authors>
				<author>Adrian MT Silva</author>
				<author>Goran Dražić</author>
				<author>Joaquim Faria</author>
			</co_authors>
			<publication_source>
				<journal>Catalysis Today</journal>
				<volume>144</volume>
				<issue>2</issue>
				<pag_min>13</pag_min>
				<pag_max>18</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>Visible-light photoactive Ce-doped TiO2 catalysts are prepared by solvothermolysis, characterized by several techniques (DRIFT, DR UV–vis, XRD and HRTEM/SAED/EDXS) and tested in the degradation of 4-chlorophenol. TiO2 materials are modified with low amounts of cerium to extend its absorption spectrum into the visible region. The presence of cerium, in an excess of 0.6% (w/w), is found to inhibit the transition from amorphous to crystalline phase during calcination at 400 °C. The observed photocatalytic activity is strongly dependent on both visible-light response and structure of the material. Cerium has a positive effect in preventing electron–hole recombination only when the crystalline structure is maintained. Compared with the commercial P-25 TiO2 catalyst, the activity observed at the initial period of the reaction is higher for the prepared Ce-doped material. However, in the case of the latter catalyst, activity decreases after 15 min in an experiment carried out at natural pH (5.8). This decrease in activity is not observed for a higher pH value (10) and seems to be related with catalyst deactivation due to removal of ceria from the surface. Finally, in the case of Ce-doped TiO2 materials, the main intermediates of degradation are hydroquinone and benzoquinone, while with P-25 the main intermediate is 4-chlorocatechol, suggesting that different reaction mechanisms may take place depending on the nature of the catalyst.</description>
		</publications>
		
		<publications id="0003">
			<title>A review on the coal gasification wastewater treatment technologies: past, present and future outlook</title>
			<date>2016</date>
			<citation>197</citation>
			<co_authors>
				<author>Qinhong Ji</author>
				<author>Salma Tabassum</author>
				<author>Sufia Hena</author>
				<author>Guangxin Yu</author>
				<author>Zhenjia Zhang</author>
			</co_authors>
			<publication_source>
				<source>Journal of Cleaner Production</source>
				<volume>126</volume>
				<pag_min>38</pag_min>
				<pag_max>55</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>The 1977–2015 literature on treatment of the coal gasification wastewater is reviewed. An overview based on ammonia and phenol recovery, biochemical treatments and advanced treatment technologies is presented with the purpose of unveiling the reasons behind problems of high concentration of persistent organic pollutants being added in the environment from coal gasification wastewater operations. A considerable list of various methods for chemical oxygen demand (COD) abatement, phenols, ammonia recovery and other significant recalcitrant organic contaminants removal from coal gasification wastewater (CGW) has been compiled. Their removal efficiencies under various conditions have been presented with emphasis in the key advances of the development of present corresponding technologies. A summary of the literature and statistical references (156) from the year 1977–2015 showed that phenol and ammonia recovery process is about 10%, Biological treatment process is about 46% and advanced treatment process about 19% of the references. It is evident from the literature survey that physicochemical, biological along with recently added integrated advanced treatment technologies have the potential to turn coal gasification wastewater into usable recycled water. More technology gap needs to be addressed in order not only to save on consumption of freshwater but could also lead to the protection of surface water bodies from contamination by precariously coal gasification wastewater at the global level.</description>
		</publications>
		
		<publications id="0004">
			<title>Bone mineral density and depression: a community study in women</title>
			<date>1999</date>
			<citation>144</citation>
			<co_authors>
				<author>Rui Coelho</author>
				<author>Aline Maia</author>
				<author>Joana Prata</author>
				<author>Henrique Barros</author>
			</co_authors>
			<publication_source>
				<source>Journal of psychosomatic research</source>
				<volume>46</volume>
				<pag_min>29</pag_min>
				<pag_max>35</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>In a community sample of 102 Portuguese white women we evaluated the relationship between osteoporosis and indexes of psychopathology and well-being. Depressive symptoms were assessed by the Beck Depression Inventory (BDI), psychopathology by the Hopkins Symptom Checklist-90 Revised (SCL-90-R), and quality of life using the Psychological General Well-Being Index. A questionnaire comprising social, demographic, clinical, and behavioral characteristics was also used. The sample prevalence of osteoporosis was 47.1%. Women with osteoporosis presented significantly higher scores on the total BDI 16±9 vs . 13±10, p=0.045 and lower scores in the hostility 0.8±0.6 vs. 1.2±0.7, p=0.012 and phobic anxiety 1.1±0.8 vs . 1.5±0.9, p=0.041 subscales of the SCL-90-R. No differences were found regarding mean general well-being scores 62±17 vs. 64±19, p=0.665 . This study showed that women with osteoporosis have significantly higher levels of depressive symptoms and a corresponding higher prevalence of depression, independent of other factors strongly associated with osteoporosis, such as age or body mass index.</description>
		</publications>
		
		<publications id="0005">
			<title>Photocatalytic degradation of caffeine: Developing solutions for emerging pollutants</title>
			<date>2013</date>
			<citation>130</citation>
			<co_authors>
				<author>Rita RN Marques</author>
				<author>Maria J Sampaio</author>
				<author>Pedro M Carrapiço</author>
				<author>Sergio Morales-Torres</author>
				<author>Goran Dražić</author>
				<author>Joaquim L Faria</author>
				<author>Adrian MT Silva</author>
			</co_authors>
			<publication_source>
				<journal>Catalysis today</journal>
				<volume>209</volume>
				<pag_min>108</pag_min>
				<pag_max>115</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>The photocatalytic degradation of the psychoactive substance caffeine was studied using composites prepared with multi-walled carbon nanotubes and three different TiO2 materials: one synthesized by a modified sol–gel method and two others obtained from Evonik and Sigma–Aldrich. These materials were characterized by several techniques (e.g., DRIFT, XRD, N2 adsorption–desorption isotherms, TEM, SEM). The tested materials increased the caffeine degradation rate and the oxygenated groups created by acid treatment on the surface of the carbon nanotubes were crucial for the photocatalytic activity of all prepared composites. In addition, the photocatalytic activity of TiO2 from Sigma–Aldrich markedly increased with the addition of functionalized carbon nanotubes, which seems to be related with the larger TiO2 crystallite sizes and the better contact of these TiO2 particles with carbon nanotubes. Selective trapping of photogenerated holes and radicals by EDTA and tert-butanol shows that photogenerated holes are crucial on the photodegradation pathway but free radicals produced by photoexcited electrons do also participate in the mechanism and seem to be responsible for the higher activity observed for composites prepared with functionalized carbon nanotubes and TiO2 from Sigma–Aldrich in comparison to TiO2 alone. Films prepared with these composites revealed higher photocatalytic activity than films of TiO2 from Evonik.</description>
		</publications>
		
		<publications id="0006">
			<title>Layered double hydroxides as highly efficient photocatalysts for visible light oxygen generation from water</title>
			<date>2009</date>
			<citation>542</citation>
			<co_authors>
				<author>Younes Bouizi</author>
				<author>Vicente Fornes</author>
				<author>Hermenegildo Garcia</author>
			</co_authors>
			<publication_source>
				<journal>Journal of the American Chemical Society</journal>
				<volume>131</volume>
				<issue>38</issue>
				<pag_min>13833</pag_min>
				<pag_max>13839</pag_max>
			</publication_source>
			<publisher>American Chemical Society</publisher>
			<description>Oxygen generation through photocatalytic water splitting under visible light irradiation is a challenging process. In this work we have synthesized a series of Zn/Ti, Zn/Ce, and Zn/Cr layered double hydroxides (LDH) at different Zn/metal atomic ratio (from 4:2 to 4:0.25) and tested them for the visible light photocatalytic oxygen generation. The most active material was found to be (Zn/Cr)LDH with an atomic ratio of 4:2 that exhibits two absorption bands in the visible region at λmax of 410 and 570 nm. It was found that the efficiency of these chromium layered double oxides for oxygen generation increases asymptotically with the Cr content. Using iron oxalate as chemical actinometer we have determined that the apparent quantum yields for oxygen generation (Φapparent = 4 × mol oxygen/mol incident photons) are of 60.9% and 12.2% at 410 and 570 nm, respectively. These quantum yields are among the highest values ever determined with visible light for solid materials in the absence of light harvesting dye. The overall efficiency of (Zn/Cr)LDH for visible light oxygen generation was found to be 1.6 times higher than that of WO3 under the same conditions.</description>
		</publications>
		
		<publications id="0007">
			<title>Carbon nanotube–TiO2 thin films for photocatalytic applications</title>
			<date>2011</date>
			<citation>119</citation>
			<co_authors>
				<author>Maria J Sampaio</author>
				<author>Rita RN Marques</author>
				<author>Adrian MT Silva</author>
				<author>Joaquim L Faria</author>
			</co_authors>
			<publication_source>
				<journal>Catalysis Today</journal>
				<volume>161</volume>
				<issue>1</issue>
				<pag_min>91</pag_min>
				<pag_max>96</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>Films made of different types of TiO2 powders and carbon nanotube–TiO2 composites were prepared on a glass substrate by using the doctor blade technique. The electron microscope thin films ranged from 3.75 to 5.00μm in thickness. Depending on the nature of the TiO2, the composite films containing different amounts of carbon nanotubes evidenced different spectroscopic and morphological properties. The surface roughness was analyzed by atomic force microscopy. The photocatalytic activity of the films was tested in the degradation of methylene blue by near-UV to visible irradiation. The introduction of carbon nanotubes in the TiO2 matrix usually increased the rate of photocatalytic degradation.</description>
		</publications>
	</researchers>
	
	
	
	
	
	<researchers id="0004">
		<name>Humberto Varum</name>
		<area>Civil Engineering</area>
		<affiliation>University of Porto</affiliation>
		<research_interests>
			<interest>Civil Engineering</interest> 
			<interest>Earthquake Engineering</interest> 
			<interest>Seismic Assessment</interest> 
			<interest>Seismic Strengthening</interest>
			<interest>Seismic Retrofitting</interest>
		</research_interests>
		<email>hvarum@ua.pt</email>
		<number_articles>861</number_articles>
	
	
		<publications id="0001">
			<title>Seismic vulnerability and risk assessment: case study of the historic city centre of Coimbra, Portugal</title>
			<date>2011</date>
			<citation>299</citation>
			<co_authors>
				<author>Romeu Vicente</author>
				<author>Sonia Parodi</author>
				<author>Sergio Lagomarsino</author>
				<author>JAR Mendes Silva</author>
			</co_authors>
			<publication_source>
				<journal>Bulletin of Earthquake Engineering</journal>
				<volume>9</volume>
				<pag_min>1067</pag_min>
				<pag_max>1096</pag_max>
			</publication_source>
			<publisher>Springer Netherlands</publisher>
			<description>Seismic risk evaluation of built-up areas involves analysis of the level of earthquake hazard of the region, building vulnerability and exposure. Within this approach that defines seismic risk, building vulnerability assessment assumes great importance, not only because of the obvious physical consequences in the eventual occurrence of a seismic event, but also because it is the one of the few potential aspects in which engineering research can intervene. In fact, rigorous vulnerability assessment of existing buildings and the implementation of appropriate retrofitting solutions can help to reduce the levels of physical damage, loss of life and the economic impact of future seismic events. Vulnerability studies of urban centres should be developed with the aim of identifying building fragilities and reducing seismic risk. As part of the rehabilitation of the historic city centre of Coimbra, a complete identification and inspection survey of old masonry buildings has been carried out. The main purpose of this research is to discuss vulnerability assessment methodologies, particularly those of the first level, through the proposal and development of a method previously used to determine the level of vulnerability, in the assessment of physical damage and its relationship with seismic intensity. Also presented and discussed are the strategy and proposed methodology adopted for the vulnerability assessment, damage and loss scenarios for the city centre of Coimbra, Portugal, using a GIS mapping application.</description>
		</publications>
		
		<publications id="0002">
			<title>Uniaxial fiber Bragg grating accelerometer system with temperature and cross axis insensitivity</title>
			<date>2011</date>
			<citation>93</citation>
			<co_authors>
				<author>Paulo Antunes</author>
				<author>Paulo Andre</author>
			</co_authors>
			<publication_source>
				<journal>Measurement</journal>
				<volume>44</volume>
				<issue>1</issue>
				<pag_min>55</pag_min>
				<pag_max>59</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>In this paper we describe the implementation and test of an optical fiber based accelerometer with cross axis insensitivity. The accelerometer uses two matching fiber Bragg gratings which are also responsible by the demodulation of the wavelength encoded information. The accelerometer was tested in a reinforced concrete slab, of a 3-storey building located at the University of Aveiro, Portugal. A maximum error of 0.25% for the slab eigenfrequencies was obtained when compared with the values acquired by an electronic sensor.</description>
		</publications>
		
		<publications id="0003">
			<title>Simplified macro-model for infill masonry panels</title>
			<date>2010</date>
			<citation>221</citation>
			<co_authors>
				<author>Hugo Rodrigues</author>
				<author>Anibal Costa</author>
			</co_authors>
			<publication_source>
				<journal>Journal of Earthquake Engineering</journal>
				<volume>14</volume>
				<issue>3</issue>
				<pag_min>390</pag_min>
				<pag_max>416</pag_max>
			</publication_source>
			<publisher>Taylor and Francis Group</publisher>
			<description>In structural analyses, masonry infill walls are commonly considered to be non structural elements. However, the response of reinforced concrete buildings to earthquake loads can be substantially affected by the influence of infill walls. In this article, an improved numerical model for the simulation of the behavior of masonry infill walls subjected to earthquake loads is proposed and analyzed. First, the proposed model is presented. This is an upgrading of the equivalent bi-diagonal compression strut model, commonly used for the nonlinear behavior of infill masonry panels subjected to cyclic loads. Second, the main results of the calibration analyses obtained with two series of experimental tests are presented and discussed: one on a single frame with one story and one bay tested at the LNEC Laboratory; and the second, on a full-scale four story and three-bay frame tested at the ELSA laboratory.</description>
		</publications>
		
		<publications id="0004">
			<title>Seismic risk assessment for mainland Portugal</title>
			<date>2015</date>
			<citation>185</citation>
			<co_authors>
				<author>Vitor Silva</author>
				<author>Helen Crowley</author>
				<author>Rui Pinho</author>
			</co_authors>
			<publication_source>
				<journal>Bulletin of Earthquake Engineering</journal>
				<volume>13</volume>
				<pag_min>429</pag_min>
				<pag_max>457</pag_max>
			</publication_source>
			<publisher>Springer Netherlands</publisher>
			<description>The assessment of the seismic risk at a national scale represents an important resource in order to introduce measures that may reduce potential losses due to future earthquakes. This evaluation results from the combination of three components: seismic hazard, structural vulnerability and exposure data. In this study, a review of existing studies focusing on each one of these areas is carried out, and used together with data from the 2011 Building Census in Portugal to compile the required input models for the evaluation of seismic hazard and risk. In order to better characterize the epistemic uncertainty in the calculations, several approaches are considered within a logic tree structure, such as the consideration of different seismic source zonations, the employment of vulnerability functions derived based on various damage criteria and the employment of distinct spatial resolutions in the exposure model. The aim of this paper is thus to provide an overview of the recent developments regarding the different aspects that influence the seismic hazard and risk in Portugal, as well as an up-to-date identification of the regions that are more vulnerable to earthquakes, together with the expected losses for a probability of exceedance of 10 % in 50 years. The results from the present study were obtained through the OpenQuake engine, the open-source software for seismic risk and hazard assessment developed within the global earthquake model (GEM) initiative.</description>
		</publications>
		
		<publications id="0005">
			<title>Analysis of the mechanical properties of compressed earth block masonry using the sugarcane bagasse ash</title>
			<date>2012</date>
			<citation>160</citation>
			<co_authors>
				<author>Sofia A Lima</author>
				<author>Almir Sales</author>
				<author>Victor F Neto</author>
			</co_authors>
			<publication_source>
				<journal>Construction and Building Materials</journal>
				<volume>35</volume>
				<pag_min>829</pag_min>
				<pag_max>837</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>The cultivation of sugarcane and production of its derivatives are closely linked to Brazil’s history and development. The factory managers face the problem of discarding the sugarcane bagasse ash (SBA), as these ashes are the final waste resulting from the industrial processes, with no possibility to reduce it. The objective of this study is to analyze the effect of adding SBA to compressed earth blocks (CEBs). Two sets of blocks were prepared with 6% and 12% of cement in addition to the earth and with the addition of SBA at ratios of 0%, 2%, 4% and 8% each. Compressive strength and absorption tests were performed on the blocks. Additionally, masonry prisms were produced with the set of blocks that showed the best preliminary test results. The results showed that the SBA can be incorporated into the CEBs and masonry without damage to the mechanical properties.</description>
		</publications>
		
		<publications id="0006">
			<title>Experimental evaluation of rectangular reinforced concrete column behaviour under biaxial cyclic loading</title>
			<date>2013</date>
			<citation>152</citation>
			<co_authors>
				<author>Hugo Rodrigues</author>
				<author>Antonio Arede</author>
				<author>Anibal G Costa</author>
			</co_authors>
			<publication_source>
				<journal>Earthquake Engineering and Structural Dynamics</journal>
				<volume>42</volume>
				<issue>2</issue>
				<pag_min>239</pag_min>
				<pag_max>259</pag_max>
			</publication_source>
			<publisher>John Wiley and Sons, Ltd</publisher>
			<description>The cyclic behaviour of reinforced concrete columns has been the subject of many experimental studies in recent years. However, most of these studies have focused on the unidirectional loading of columns with square cross-sections under constant axial loading conditions. In the present study, four types of full-scale quadrangular building columns were tested under different types of loading, including uniaxial and biaxial loading conditions. The first two specimens of each column type were independently cyclically tested in the strong and weak directions. Bidirectional tests using different loading paths were performed on the other column specimens. All columns were tested under constant axial loading conditions. In this paper, the experimental results are presented, and the global behaviour of tested columns is discussed, particularly focusing on the stiffness and strength degradation because of the increasing cyclic demand. Finally, the deformation-based performance limits proposed in Part 3 of Eurocode 8 were calculated and compared with the experimental results.</description>
		</publications>
		
		<publications id="0007">
			<title>Biaxial optical accelerometer and high-angle inclinometer with temperature and cross-axis insensitivity</title>
			<date>2012</date>
			<citation>99</citation>
			<co_authors>
				<author>Paulo Fernando Costa Antunes</author>
				<author>Carlos Alberto Marques</author>
				<author>Paulo S Andre</author>
			</co_authors>
			<publication_source>
				<journal>IEEE Sensors Journal</journal>
				<volume>12</volume>
				<issue>7</issue>
				<pag_min>2399</pag_min>
				<pag_max>2406</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>In this paper, we describe the implementation and test of an optical fiber biaxial accelerometer and inclinometer based on fiber Bragg gratings. The proposed sensor is a low cost and simple solution to measure acceleration and relative rotation in two orthogonal directions, which can be used for structural health monitoring. Sensitivities of 87.848 and 92.351 pm.g -1 , for each sensitive direction with resonant frequencies of 846.01 and 845.33 Hz, respectively, are achieved. For the inclinometer, the tilt sensitivities are 1.43 ± 0.03 and 1.38 ± 0.03 pm/o for each orthogonal sensitive direction for angular rotations smaller than 45°. For inclinations higher than 45°, a suitable calibration expression is presented.</description>
		</publications>
	</researchers>
	
	
	
	
	
	<researchers id="0005">
		<name>Joao Mano</name>
		<area>Chemistry from the Technical</area>
		<affiliation>University of Aveiro</affiliation>
		<research_interests>
			<interest>Biomaterials</interest> 
			<interest>Tissue Engineering</interest> 
			<interest>Biomedical Engineering</interest> 
			<interest>Nanotechnology</interest>
			<interest>Regenerative Medicine</interest>
		</research_interests>
		<email>jmano@ua.pt</email>
		<number_articles>1175</number_articles>
		
		<publications id="0001">
			<title>Molecular interactions driving the layer-by-layer assembly of multilayers</title>
			<date>2014</date>
			<citation>807</citation>
			<co_authors>
				<author>Joao Borges</author>
			</co_authors>
			<publication_source>
				<source>Chemical reviews</source>
				<volume>114</volume>
				<issue>18</issue>
				<pag_min>8883</pag_min>
				<pag_max>8942</pag_max>
			</publication_source>
			<publisher>American Chemical Society</publisher>
			<description>This article presents an overview of the different types of intermolecular interactions behind the fabrication of multilayer assemblies using the layer-by-layer (LbL) assembly approach. It comments on the potential impact of each type of intermolecular interaction and materials assembled through them on the development of advanced functional systems or devices for several emerging applications. The discussion begins with a brief overview of the most commonly used bottom-up methods to modify surfaces and fabricate functional multilayer thin films, with a special focus on their main advantages and disadvantages.</description>
		</publications>
		
		<publications id="0002">
			<title>Graft copolymerized chitosan—present status and applications</title>
			<date>2005</date>
			<citation>726</citation>
			<co_authors>
				<author>R Jayakumar</author>
				<author>M Prabaharan</author>
				<author>RL Reis</author>
			</co_authors>
			<publication_source>
				<source>Carbohydrate Polymers</source>
				<volume>62</volume>
				<issue>2</issue>
				<pag_min>142</pag_min>
				<pag_max>158</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>Chitosan is a natural based polymer obtained by alkaline deacetylation of chitin, exhibiting excellent biological properties such as biodegradation in the human body, and immunological, antibacterial, and wound-healing activity. Chitosan has also been found to be a good candidate as a support material for gene delivery, cell culture and tissue engineering. However, practical use of chitosan has been mainly confined to the unmodified forms. For a breakthrough in utilization, graft copolymerization onto chitosan will be a key point, which will introduce desired properties and enlarge the field of the potential applications of chitosan by choosing various types of side chains. This article reviews the various methods such as free radical, radiation, enzymatic and cationic graft copolymerization onto chitosan, the factors influencing on the grafting parameters such as grafting percentage and grafting efficiency, and the properties of grafted chitosan. This review also screens the current applications of graft copolymerized chitosans in the field of drug delivery, tissue engineering, antibacterial, biomedical, metal adsorption and dye removal.</description>
		</publications>
		
		<publications id="0003">
			<title>Chitosan-based particles as controlled drug delivery systems</title>
			<date>2004</date>
			<citation>613</citation>
			<co_authors>
				<author>M Prabaharan</author>
			</co_authors>
			<publication_source>
				<source>Drug delivery</source>
				<volume>12</volume>
				<issue>1</issue>
				<pag_min>41</pag_min>
				<pag_max>57</pag_max>
			</publication_source>
			<publisher>Taylor and Francis</publisher>
			<description>Chitosan, a natural-based polymer obtained by alkaline deacetylation of chitin, is nontoxic, biocompatible, and biodegradable. These properties make chitosan a good candidate for conventional and novel drug delivery systems. This article reviews the approaches aimed to associate bioactive molecules to chitosan in the form of colloidal structures and analyzes the evidence of their efficacy in improving the transport of the associated molecule through mucosae and epithelia. Chitosan forms colloidal particles and entraps bioactive molecules through a number of mechanisms, including chemical crosslinking, ionic crosslinking, and ionic complexation. A possible alternative of chitosan by the chemical modification also has been useful for the association of bioactive molecules to polymer and controlling the drug release profile. Because of the high affinity of chitosan for cell membranes, it has been used as a coating agent for liposome formulations. This review also examines the advances in the application of chitosan and its derivatives to nonviral gene delivery and gives an overview of transfection studies that use chitosan as a transfection agent. From the studies reviewed, we concluded that chitosan and its derivatives are promising materials for controlled drug and nonviral gene delivery.</description>
		</publications>
		
		<publications id="0004">
			<title>Thermal properties of thermoplastic starch/synthetic polymer blends with potential biomedical applicability</title>
			<date>2003</date>
			<citation>513</citation>
			<co_authors>
				<author>D Koniarova</author>
				<author>RL Reis</author>
			</co_authors>
			<publication_source>
				<journal>Journal of materials science: Materials in medicine</journal>
				<volume>14</volume>
				<issue>2</issue>
				<pag_min>127</pag_min>
				<pag_max>135</pag_max>
			</publication_source>
			<publisher>Kluwer Academic Publishers</publisher>
			<description>Previous studies shown that thermoplastic blends of corn starch with some biodegradable synthetic polymers (poly(ε-caprolactone), cellulose acetate, poly(lactic acid) and ethylene-vinyl alcohol copolymer) have good potential to be used in a series of biomedical applications. In this work the thermal behavior of these structurally complex materials is investigated by differential scanning calorimetry (DSC) and by thermogravimetric analysis (TGA). In addition, Fourier-transform infrared (FTIR) spectroscopy was used to investigate the chemical interactions between the different components. The endothermic gelatinization process (or water evaporation) observed by DSC in starch is also observed in the blends. Special attention was paid to the structural relaxation that can occur in the blends with poly(lactic acid) at body temperature that may change the physical properties of the material during its application as a biomaterial. At least three degradation mechanisms were identified in the blends by means of using TGA, being assigned to the mass loss due to the plasticizer leaching, and to the degradation of the starch and the synthetic polymer fractions. The non-isothermal kinetics of the decomposition processes was analyzed using two different integral methods. The analysis included the calculation of the activation energy of the correspondent reactions.</description>
		</publications>
		
		<publications id="0005">
			<title>Smart thermoresponsive coatings and surfaces for tissue engineering: switching cell-material boundaries</title>
			<date>2007</date>
			<citation>379</citation>
			<co_authors>
				<author>Ricardo MP Da Silva</author>
				<author>Rui L Reis</author>
			</co_authors>
			<publication_source>
				<source>TRENDS in Biotechnology</source>
				<volume>25</volume>
				<issue>12</issue>
				<pag_min>577</pag_min>
				<pag_max>583</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>The smart thermoresponsive coatings and surfaces that have been explicitly designed for cell culture are mostly based on poly(N-isopropylacrylamide) (PNIPAAm). This polymer is characterized by a sudden precipitation on heating, switching from a hydrophilic to a hydrophobic state. Mammalian cells cultured on such thermoresponsive substrates can be recovered as confluent cell sheets, while keeping the newly deposited extracellular matrix intact, simply by lowering the temperature and thereby avoiding the use of deleterious proteases. Thermoresponsive materials and surfaces are powerful tools for creating tissue-like constructs that imitate native tissue geometry and mimic its spatial cellular organization. Here we review and compare the most representative methods of producing thermoresponsive substrates for cell sheet engineering.</description>
		</publications>
		
		<publications id="0006">
			<title>Starch-based biodegradable hydrogels with potential biomedical applications as drug delivery systems</title>
			<date>2002</date>
			<citation>386</citation>
			<co_authors>
				<author>C Elvira</author>
				<author>J San Roman</author>
				<author>RL Reis</author>
			</co_authors>
			<publication_source>
				<journal>Biomaterials</journal>
				<volume>23</volume>
				<issue>9</issue>
				<pag_min>1955</pag_min>
				<pag_max>1966</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>The design and preparation of novel biodegradable hydrogels developed by the free radical polymerization of acrylamide and acrylic acid, and some formulations with bis-acrylamide, in the presence of a corn starch/ethylene-co-vinyl alcohol copolymer blend (SEVA-C), is reported. The redox system benzoyl peroxide (BPO) and 4-dimethylaminobenzyl alcohol (DMOH) initiated the polymerization at room temperature. Xerogels were characterized by 1H NMR and FTIR spectroscopies. Swelling studies were performed as a function of pH in different buffer solutions determining the water-transport mechanism that governs the swelling behaviour. Degradation studies of the hydrogels were performed in simulated physiological solutions for time up to 90 days, determining the respective weight loss, and analyzing the solution residue by 1H NMR. The mechanical properties of the xerogels were characterized by tensile and compressive tests, as well as by dynamo-mechanical analysis (DMA). Dynamo-mechanical parameters are also reported for hydrated samples.</description>
		</publications>
		
		<publications id="0007">
			<title>Controlling cell behavior through the design of polymer surfaces</title>
			<date>2010</date>
			<citation>347</citation>
			<co_authors>
				<author>Natalia M Alves</author>
				<author>Iva Pashkuleva</author>
				<author>Rui L Reis</author>
			</co_authors>
			<publication_source>
				<source>Small</source>
				<volume>6</volume>
				<issue>20</issue>
				<pag_min>2208</pag_min>
				<pag_max>2220</pag_max>
			</publication_source>
			<publisher>WILEY‐VCH Verlag</publisher>
			<description>Polymers have gained a remarkable place in the biomedical field as materials for the fabrication of various devices and for tissue engineering applications. The initial acceptance or rejection of an implantable device is dictated by the crosstalk of the material surface with the bioentities present in the physiological environment. Advances in microfabrication and nanotechnology offer new tools to investigate the complex signaling cascade induced by the components of the extracellular matrix and consequently allow cellular responses to be tailored through the mimicking of some elements of the signaling paths. Patterning methods and selective chemical modification schemes at different length scales can provide biocompatible surfaces that control cellular interactions on the micrometer and sub-micrometer scales on which cells are organized. In this review, the potential of chemically and topographically structured micro- and nanopolymer surfaces are discussed in hopes of a better understanding of cell-biomaterial interactions, including the recent use of biomimetic approaches or stimuli-responsive macromolecules. Additionally, the focus will be on how the knowledge obtained using these surfaces can be incorporated to design biocompatible materials for various biomedical applications, such as tissue engineering, implants, cell-based biosensors, diagnostic systems, and basic cell biology. The review focusses on the research carried out during the last decade.</description>
		</publications>
	</researchers>




	
	<researchers id="0006">
		<name>Rute Ferreira</name>
		<area>Physics</area>
		<affiliation>University of Aveiro</affiliation>
		<research_interests>
			<interest>Luminescence</interest> 
			<interest>Organic-Inorganic Hybrids</interest> 
			<interest>Photonics</interest> 
			<interest>Solar Energy</interest> 
			<interest>Optical Sensors</interest> 
		</research_interests>
		<email>rferreira@ua.pt</email>
		<number_articles>680</number_articles>
		
		<publications id="0001">
			<title>Lanthanide‐containing light‐emitting organic–inorganic hybrids: a bet on the future</title>
			<date>2009</date>
			<citation>960</citation>
			<co_authors>
				<author>Luis D Carlos</author>
				<author>Veronica de Zea Bermudez</author>
				<author>Sidney JL Ribeiro</author>
			</co_authors>
			<publication_source>
				<journal>Advanced Materials</journal>
				<volume>21</volume>
				<issue>5</issue>
				<pag_min>509</pag_min>
				<pag_max>534</pag_max>
			</publication_source>
			<publisher>WILEY‐VCH Verlag</publisher>
			<description>Interest in lanthanide-containing organic–inorganic hybrids has grown considerably during the last decade, with the concomitant fabrication of materials with tunable attributes offering modulated properties. The potential of these materials relies on exploiting the synergy between the intrinsic characteristics of sol–gel derived hosts (highly controlled purity, versatile shaping and patterning, excellent optical quality, easy control of the refractive index, photosensitivity, encapsulation of large amounts of isolated emitting centers protected by the host) and the luminescence features of trivalent lanthanide ions (high luminescence quantum yield, narrow bandwidth, long-lived emission, large Stokes shifts, ligand-dependent luminescence sensitization). Promising applications may be envisaged, such as light-emitting devices, active waveguides in the visible and near-IR spectral regions, active coatings, and bio-medical actuators and sensors, opening up exciting directions in materials science and related technologies with significant implications in the integration, miniaturization, and multifunctionalization of devices. This review provides an overview of the latest advances in Ln3+-containing siloxane-based hybrids, with emphasis on the different possible synthetic strategies, photoluminescence features, empirical determination.</description>
		</publications>
		
		<publications id="0002">
			<title>Ratiometric Nanothermometer Based on an Emissive Ln3+-Organic Framework</title>
			<date>2013</date>
			<citation>351</citation>
			<co_authors>
				<author>Amandine Cadiau</author>
				<author>Carlos DS Brites</author>
				<author>Pedro MFJ Costa</author>
				<author>Joao Rocha</author>
				<author>Luis D Carlos</author>
			</co_authors>
			<publication_source>
				<journal>ACS nano</journal>
				<volume>7</volume>
				<issue>8</issue>
				<pag_min>7213</pag_min>
				<pag_max>7218</pag_max>
			</publication_source>
			<publisher>American Chemical Society</publisher>
			<description>Luminescent thermometers working at the nanoscale with high spatial resolution, where the conventional methods are ineffective, have emerged over the last couple of years as a very active field of research. Lanthanide-based materials are among the most versatile thermal probes used in luminescent nanothermometers. Here, nanorods of metal organic framework Tb0.99Eu0.01(BDC)1.5(H2O)2 (BDC = 1-4-benzendicarboxylate) have been prepared by the reverse microemulsion technique and characterized and their photoluminescence properties studied from room temperature to 318 K. Aqueous suspensions of these nanoparticles display an excellent performance as ratiometric luminescent nanothermometers in the physiological temperature (300–320 K) range.</description>
		</publications>
		
		<publications id="0003">
			<title>Upconverting nanoparticles working as primary thermometers in different media</title>
			<date>2017</date>
			<citation>207</citation>
			<co_authors>
				<author>Sangeetha Balabhadra</author>
				<author>Mengistie L Debasu</author>
				<author>Carlos DS Brites</author>
				<author>Luis D Carlos</author>
			</co_authors>
			<publication_source>
				<journal>The Journal of Physical Chemistry C</journal>
				<volume>121</volume>
				<issue>25</issue>
				<pag_min>13962</pag_min>
				<pag_max>13968</pag_max>
			</publication_source>
			<publisher>American Chemical Society</publisher>
			<description>In the past decade, noninvasive luminescent thermometry has become popular due to the limitations of traditional contact thermometers to operate at scales below 100 μm, as required by current demands in disparate areas. Generally, the calibration procedure requires an independent measurement of the temperature to convert the thermometric parameter (usually an intensity ratio) to temperature. A new calibration procedure is necessary whenever the thermometer operates in a different medium. However, recording multiple calibrations is a time-consuming task, and not always possible to perform, e.g., in living cells and in electronic devices. Typically, a unique calibration relation is assumed to be valid, independent of the medium, which is a bottleneck of the secondary luminescent thermometers developed up to now. Here we report a straightforward method to predict the temperature calibration curve of any upconverting thermometer based on two thermally coupled electronic levels independently of the medium, demonstrating that these systems are intrinsically primary thermometers. SrF2:Yb/Er powder and water suspended nanoparticles were used as an illustrative example.</description>
		</publications>
		
		<publications id="0004">
			<title>Full‐color phosphors from amine‐functionalized crosslinked hybrids lacking metal activator ions</title>
			<date>2001</date>
			<citation>157</citation>
			<co_authors>
				<author>Luis D Carlos</author>
				<author>Veronica De Zea Bermudez</author>
				<author>Sidney JL Ribeiro</author>
			</co_authors>
			<publication_source>
				<journal>Advanced Functional Materials</journal>
				<volume>11</volume>
				<issue>2</issue>
				<pag_min>111</pag_min>
				<pag_max>115</pag_max>
			</publication_source>
			<publisher>WILEY‐VCH Verlag GmbH</publisher>
			<description>Sol–gel derived hybrids that contain OCH2CH2 (polyethylene glycol, PEG) repeat units grafted onto a siliceous backbone by urea, –NHC(=O)NH–, or urethane, –NHC(=O)O–, bridges have been prepared. It is demonstrated that the white light PL of these materials results from an unusual convolution of a longer lived emission that originates in the NH groups of the urea/urethane bridges with shorter lived electron–hole recombinations occurring in the nanometer-sized siliceous domains. The PL efficiencies reported here (maximum quantum yields at room temperature of ≈ 0.20 ± 0.02 at a 400 nm excitation wavelength) are in the same range as those for tetramethoxysilane–formic acid, and APTES–acetic acid, sol–gel derived phosphors. The high quantum yields combined with the possibility of tuning the emission to colors across the chromaticity diagram present a wide range of potential applications for these hybrid materials.</description>
		</publications>
		
		<publications id="0005">
			<title>Citric Acid-Assisted Hydrothermal Synthesis of Luminescent TbPO4:Eu Nanocrystals: Controlled Morphology and Tunable Emission</title>
			<date>2008</date>
			<citation>104</citation>
			<co_authors>
				<author>Weihua Di</author>
				<author>Marc-Georg Willinger</author>
				<author>Xinguang Ren</author>
				<author>Shaozhe Lu</author>
				<author>Nicola Pinna</author>
			</co_authors>
			<publication_source>
				<journal>The Journal of Physical Chemistry C</journal>
				<volume>112</volume>
				<issue>48</issue>
				<pag_min>18815</pag_min>
				<pag_max>18820</pag_max>
			</publication_source>
			<publisher>American Chemical Society</publisher>
			<description>This work reports a simple hydrothermal route using citric acid as a “shape modifier” for the controlled synthesis of luminescent TbPO4:Eu nanocrystals. The size and morphology of products change remarkably when the proportion of citric acid involved in the reaction increases. The multiple roles that citric acid plays during the controlled synthesis are discussed to try to understand the crystallization and growth dynamics of TbPO4 crystals. The photoluminescence properties of TbPO4:Eu are investigated. The excitation spectra and the variation of the 5D4 lifetime values as a function of the Eu3+ concentration points out the occurrence of Tb3+-to-Eu3+ energy transfer, resulting in a maximum absolute emission quantum yield of 0.14. The possibility to tune the size, the shape, and the optical properties of the nanocrystals reported in this work might be useful for applications in optoelectronics or biolabeling. Moreover, this simple approach might also be applied for the synthesis of other luminescent phosphates.</description>
		</publications>
		
		<publications id="0006">
			<title>Organic–inorganic hybrid materials towards passive and active architectures for the next generation of optical networks</title>
			<date>2010</date>
			<citation>93</citation>
			<co_authors>
				<author>PS Andre</author>
				<author>Luis D Carlos</author>
			</co_authors>
			<publication_source>
				<journal>Optical Materials</journal>
				<volume>32</volume>
				<issue>11</issue>
				<pag_min>1397</pag_min>
				<pag_max>1409</pag_max>
			</publication_source>
			<publisher>North-Holland</publisher>
			<description>The advances in optoelectronics over the last three decades have been quite dramatic, namely the mass manufacturing of low cost integrated circuits, revolutionizing the speed and the capability of computing and communication. However, today’s ever-increasing demand for high-bandwidth data is outgrowing the performance of electronics in many applications, such as in telecommunications where the traffic demand has been increasing steadily and, therefore, the transmission technology requires bandwidth that exceeds the one provided by actual copper based networks. In this context, the fabrication of low-cost integrated optics (IO) devices using sol–gel derived organic–inorganic hybrid (OIH) materials has received increasing attention in the last years. This review will focus on examples of OIHs that can be used in IO devices for the next generation of optical networks. Emphasis will be given to passive (planar and channel waveguides, couplers and multimode interference splitters) and active (lasers and optical amplifiers) optical architectures for long haul/metro and access/indoor networks.</description>
		</publications>
		
		<publications id="0007">
			<title>Lanthanide-based lamellar nanohybrids: synthesis, structural characterization, and optical properties</title>
			<date>2006</date>
			<citation>86</citation>
			<co_authors>
				<author>Mohamed Karmaoui</author>
				<author>Ankush T Mane</author>
				<author>Luis D Carlos</author>
				<author>Nicola Pinna</author>
			</co_authors>
			<publication_source>
				<journal>Chemistry of materials</journal>
				<volume>18</volume>
				<issue>18</issue>
				<pag_min>4493</pag_min>
				<pag_max>4499</pag_max>
			</publication_source>
			<publisher>American Chemical Society</publisher>
			<description>A general nonaqueous route has been applied for the preparation of lanthanide ordered nanocrystalline hybrid structures. In a simple one-pot reaction process, Ln(III) isopropoxides (Ln = Gd, Sm, Nd) were dissolved in benzyl alcohol and reacted in an autoclave between 250 and 300 °C. This approach leads to crystalline lanthanide oxide layers regularly separated from each other by organic layers of intercalated benzoate molecules. They display good thermal stability for temperature up to 400 °C. The gadolinium-based nanohybrids showed outstanding optical emission properties when doped with terbium(III) and europium(III).</description>
		</publications>
	</researchers>
	
	
	
	
	
	<researchers id="0007">
		<name>Mario Figueiredo</name>
		<area>Electrical and Computer Engineering</area>
		<affiliation>Instituto Superior Tecnico University of Lisbon</affiliation>
		<affiliation>Instituto de Telecomunicaçoes</affiliation>
		<research_interests>
			<interest>Machine Learning</interest> 
			<interest>Optimization</interest> 
			<interest>Signal Processing</interest> 
			<interest>Image Processing</interest> 
			<interest>Information Theory</interest> 
		</research_interests>
		<email>mario.figueiredo@tecnico.ulisboa.pt</email>
		<number_articles>386</number_articles>
		
		<publications id="0001">
			<title>Unsupervised learning of finite mixture models</title>
			<date>2002</date>
			<citation>2888</citation>
			<co_authors>
				<author>Anil K. Jain</author>
			</co_authors>
			<publication_source>
				<journal>Pattern Analysis and Machine Intelligence IEEE Transactions on</journal>
				<volume>24</volume>
				<issue>3</issue>
				<pag_min>381</pag_min>
				<pag_max>396</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>This paper proposes an unsupervised algorithm for learning a finite mixture model from multivariate data. The adjective "unsupervised" is justified by two properties of the algorithm: 1) it is capable of selecting the number of components and 2) unlike the standard expectation-maximization (EM) algorithm, it does not require careful initialization. The proposed method also avoids another drawback of EM for mixture fitting: the possibility of convergence toward a singular estimate at the boundary of the parameter space. The novelty of our approach is that we do not use a model selection criterion to choose one among a set of preestimated candidate models; instead, we seamlessly integrate estimation and model selection in a single algorithm. Our technique can be applied to any type of parametric mixture model for which it is possible to write an EM algorithm; in this paper, we illustrate it with experiments involving Gaussian mixtures. These experiments testify for the good performance of our approach.</description>
		</publications>
		
		<publications id="0002">
			<title>A new TwIST: two-step iterative shrinkage/thresholding algorithms for image restoration</title>
			<date>2007</date>
			<citation>2258</citation>
			<co_authors>
				<author>Jose M Bioucas-Dias</author>
			</co_authors>
			<publication_source>
				<journal>Image Processing IEEE Transactions on</journal>
				<volume>16</volume>
				<issue>12</issue>
				<pag_min>2992</pag_min>
				<pag_max>3004</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>Iterative shrinkage/thresholding (1ST) algorithms have been recently proposed to handle a class of convex unconstrained optimization problems arising in image restoration and other linear inverse problems. This class of problems results from combining a linear observation model with a nonquadratic regularizer (e.g., total variation or wavelet-based regularization). It happens that the convergence rate of these 1ST algorithms depends heavily on the linear observation operator, becoming very slow when this operator is ill-conditioned or ill-posed. In this paper, we introduce two-step 1ST (TwIST) algorithms, exhibiting much faster convergence rate than 1ST for ill-conditioned problems. For a vast class of nonquadratic convex regularizers (lscr P norms, some Besov norms, and total variation), we show that TwIST converges to a minimizer of the objective function, for a given range of values of its parameters. For noninvertible observation operators, we introduce a monotonic version of TwIST (MTwIST); although the convergence proof does not apply to this scenario, we give experimental evidence that MTwIST exhibits similar speed gains over IST. The effectiveness of the new methods are experimentally confirmed on problems of image deconvolution and of restoration with missing samples.</description>
		</publications>
		
		<publications id="0003">
			<title>On the role of sparse and redundant representations in image processing</title>
			<date>2010</date>
			<citation>855</citation>
			<co_authors>
				<author>Michael Elad</author>
				<author>Yi Ma</author>
			</co_authors>
			<publication_source>
				<journal>Proceedings of the IEEE</journal>
				<volume>98</volume>
				<issue>6</issue>
				<pag_min>972</pag_min>
				<pag_max>982</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>Much of the progress made in image processing in the past decades can be attributed to better modeling of image content and a wise deployment of these models in relevant applications. This path of models spans from the simple l 2 -norm smoothness through robust, thus edge preserving, measures of smoothness (e.g. total variation), and until the very recent models that employ sparse and redundant representations. In this paper, we review the role of this recent model in image processing, its rationale, and models related to it. As it turns out, the field of image processing is one of the main beneficiaries from the recent progress made in the theory and practice of sparse and redundant representations. We discuss ways to employ these tools for various image-processing tasks and present several applications in which state-of-the-art results are obtained.</description>
		</publications>
		
		<publications id="0004">
			<title>Restoration of Poissonian images using alternating direction optimization</title>
			<date>2010</date>
			<citation>509</citation>
			<co_authors>
				<author>Jose M Bioucas-Dias</author>
			</co_authors>
			<publication_source>
				<journal>Image Processing IEEE Transactions on</journal>
				<volume>19</volume>
				<issue>12</issue>
				<pag_min>3133</pag_min>
				<pag_max>3145</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>Much research has been devoted to the problem of restoring Poissonian images, namely for medical and astronomical applications. However, the restoration of these images using state-of-the-art regularizers (such as those based upon multiscale representations or total variation) is still an active research area, since the associated optimization problems are quite challenging. In this paper, we propose an approach to deconvolving Poissonian images, which is based upon an alternating direction optimization method. The standard regularization [or maximum a posteriori (MAP)] restoration criterion, which combines the Poisson log-likelihood with a (nonsmooth) convex regularizer (log-prior), leads to hard optimization problems: the log-likelihood is nonquadratic and nonseparable, the regularizer is nonsmooth, and there is a nonnegativity constraint. Using standard convex analysis tools, we present sufficient conditions for existence and uniqueness of solutions of these optimization problems, for several types of regularizers: total-variation, frame-based analysis, and frame-based synthesis. We attack these problems with an instance of the alternating direction method of multipliers (ADMM), which belongs to the family of augmented Lagrangian algorithms. We study sufficient conditions for convergence and show that these are satisfied, either under total-variation or frame-based (analysis and synthesis) regularization. The resulting algorithms are shown to outperform alternative state-of-the-art methods, both in terms of speed and restoration accuracy.</description>
		</publications>
		
		<publications id="0005">
			<title>Multiplicative noise removal using variable splitting and constrained optimization</title>
			<date>2010</date>
			<citation>369</citation>
			<co_authors>
				<author>Jose M Bioucas-Dias</author>
			</co_authors>
			<publication_source>
				<journal>Image Processing IEEE Transactions on</journal>
				<volume>19</volume>
				<issue>7</issue>
				<pag_min>1720</pag_min>
				<pag_max>1730</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>Multiplicative noise (also known as speckle noise) models are central to the study of coherent imaging systems, such as synthetic aperture radar and sonar, and ultrasound and laser imaging. These models introduce two additional layers of difficulties with respect to the standard Gaussian additive noise scenario: (1) the noise is multiplied by (rather than added to) the original image; (2) the noise is not Gaussian, with Rayleigh and Gamma being commonly used densities. These two features of multiplicative noise models preclude the direct application of most state-of-the-art algorithms, which are designed for solving unconstrained optimization problems where the objective has two terms: a quadratic data term (log-likelihood), reflecting the additive and Gaussian nature of the noise, plus a convex (possibly nonsmooth) regularizer (e.g., a total variation or wavelet-based regularizer/prior). In this paper, we address these difficulties by: (1) converting the multiplicative model into an additive one by taking logarithms, as proposed by some other authors; (2) using variable splitting to obtain an equivalent constrained problem; and (3) dealing with this optimization problem using the augmented Lagrangian framework. A set of experiments shows that the proposed method, which we name MIDAL (multiplicative image denoising by augmented Lagrangian), yields state-of-the-art results both in terms of speed and denoising performance.</description>
		</publications>
		
		<publications id="0006">
			<title>Wavelet-based image estimation: an empirical Bayes approach using Jeffrey's noninformative prior</title>
			<date>2001</date>
			<citation>314</citation>
			<co_authors>
				<author>Robert D Nowak</author>
			</co_authors>
			<publication_source>
				<journal>Image Processing IEEE Transactions on</journal>
				<volume>10</volume>
				<issue>9</issue>
				<pag_min>1322</pag_min>
				<pag_max>1331</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>The sparseness and decorrelation properties of the discrete wavelet transform have been exploited to develop powerful denoising methods. However, most of these methods have free parameters which have to be adjusted or estimated. In this paper, we propose a wavelet-based denoising technique without any free parameters; it is, in this sense, a "universal" method. Our approach uses empirical Bayes estimation based on a Jeffreys' noninformative prior; it is a step toward objective Bayesian wavelet-based denoising. The result is a remarkably simple fixed nonlinear shrinkage/thresholding rule which performs better than other more computationally demanding methods.</description>
		</publications>
		
		<publications id="0007">
			<title>A Bayesian approach to joint feature selection and classifier design</title>
			<date>2004</date>
			<citation>214</citation>
			<co_authors>
				<author>Balaji Krishnapuram</author>
				<author>AJ Harternink</author>
				<author>Lawrence Carin</author>
			</co_authors>
			<publication_source>
				<journal>Pattern Analysis and Machine Intelligence IEEE Transactions on</journal>
				<volume>26</volume>
				<issue>9</issue>
				<pag_min>1105</pag_min>
				<pag_max>1111</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>This paper adopts a Bayesian approach to simultaneously learn both an optimal nonlinear classifier and a subset of predictor variables (or features) that are most relevant to the classification task. The approach uses heavy-tailed priors to promote sparsity in the utilization of both basis functions and features; these priors act as regularizers for the likelihood function that rewards good classification on the training data. We derive an expectation- maximization (EM) algorithm to efficiently compute a maximum a posteriori (MAP) point estimate of the various parameters. The algorithm is an extension of recent state-of-the-art sparse Bayesian classifiers, which in turn can be seen as Bayesian counterparts of support vector machines. Experimental comparisons using kernel classifiers demonstrate both parsimonious feature selection and excellent classification accuracy on a range of synthetic and benchmark data sets.</description>
		</publications>
	</researchers>
	
	
	
	
	
	<researchers id="0008">
		<name>Isabel Marrucho</name>
		<area>Chemical Engineering</area>
		<affiliation>Instituto Superior Tecnico University of Lisbon</affiliation>
		<affiliation>Coordinator of Information and Data Science at the Instituto de Telecomunicaçoes</affiliation>
		<research_interests>
			<interest>Ionic liquids</interest> 
			<interest>Deep eutectic solvents</interest> 
			<interest>CO2 separation</interest>  
			<interest>Membranes</interest>  
		</research_interests>
		<email>isabel.marrucho@tecnico.ulisboa.pt</email>
		<number_articles>360</number_articles>
		
		<publications id="0001">
			<title>Quest for green‐solvent design: from hydrophilic to hydrophobic (deep) eutectic solvents</title>
			<date>2019</date>
			<citation>290</citation>
			<co_authors>
				<author>Catarina Florindo</author>
				<author>Luis C Branco</author>
			</co_authors>
			<publication_source>
				<source>ChemSusChem</source>
				<volume>12</volume>
				<issue>8</issue>
				<pag_min>1549</pag_min>
				<pag_max>1559</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>Deep eutectic solvents (DESs) consist of a mixture of two or more solid components, which gives rise to a lower melting point compared to the starting materials. Until recently only hydrophilic DESs were available, and despite their revolutionary role in the alternative-solvents field, important issues in chemistry, and chemical engineering (such as water-related problems and the replacement of toxic volatile organic compounds) could not be tackled. Hydrophobic (deep)—here in parenthesis due to the different depths of the eutectic melting points—eutectic solvents are a subclass of DESs where both components are hydrophobic. The low toxicity, high biodegradability, and straightforward preparation without further purification steps of naturally occurring low-cost compounds are among the key advantages. Although research on hydrophobic DESs is scarce (the first report was only published in 2015), some interesting features and applications have been reported and deserve to be evaluated and comparisons established. This Minireview is divided into two parts: The first part provides a brief general introduction to DESs and the second part discusses the nomenclature using solid–liquid phase diagram analysis, chemical stability, thermophysical properties comparison, and finally the most important emerging fields of application.</description>
		</publications>
		
		<publications id="0002">
			<title>Development of hydrophobic deep eutectic solvents for extraction of pesticides from aqueous environments</title>
			<date>2017</date>
			<citation>321</citation>
			<co_authors>
				<author>C Florindo</author>
				<author>LC Branco</author>
			</co_authors>
			<publication_source>
				<journal>Fluid Phase Equilibria</journal>
				<volume>448</volume>
				<pag_min>135</pag_min>
				<pag_max>142</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>Wastewater treatment plants do not properly address the removal of emerging micropollutants, such as pesticides, and thus these compounds contaminate water sources of public drinking water systems. In this context, this work focuses on the development of hydrophobic deep eutectic solvents (DESs), as cheap extractants for the removal of four neonicotinoids, Imidacloprid, Acetamiprid, Nitenpyram and Thiamethoxam, from diluted aqueous solutions. In particular, two different families of DESs, one based on natural neutral ingredients (DL-Menthol and natural organic acids) and the other based on quaternary ammonium salts and organic acids were prepared and their water stability carefully studied through 1H NMR. Only the chemically stable DESs were selected to be used as solvents in the extraction of the four neonicotinoids so that no contamination of the water cycle is attained, while reuse of the DES is possible. The final results were compared with those obtained for liquid-liquid extraction using hydrophobic imidazolium-based ionic liquids as solvents.</description>
		</publications>
		
		<publications id="0003">
			<title>Aqueous biphasic systems: a boost brought about by using ionic liquids</title>
			<date>2012</date>
			<citation>791</citation>
			<co_authors>
				<author>Mara G Freire</author>
				<author>Ana Filipa M Claudio</author>
				<author>Joao MM Araujo</author>
				<author>Jose N Canongia Lopes</author>
				<author>Luis Paulo N Rebelo</author>
				<author>Joao AP Coutinho</author>
			</co_authors>
			<publication_source>
				<journal>Chemical Society Reviews</journal>
				<volume>41</volume>
				<issue>14</issue>
				<pag_min>4966</pag_min>
				<pag_max>4995</pag_max>
			</publication_source>
			<publisher>Royal Society of Chemistry</publisher>
			<description>During the past decade, ionic-liquid-based Aqueous Biphasic Systems (ABS) have been the focus of a significant amount of research. Based on a compilation and analysis of the data hitherto reported, this critical review provides a judicious assessment of the available literature on the subject. We evaluate the quality of the data and establish the main drawbacks found in the literature. We discuss the main issues which govern the phase behaviour of ionic-liquid-based ABS, and we highlight future challenges to the field. In particular, the effect of the ionic liquid structure and the various types of salting-out agents (inorganic or organic salts, amino acids and carbohydrates) on the phase equilibria of ABS is discussed, as well as the influence of secondary parameters such as temperature and pH. More recent approaches using ionic liquids as additives or as replacements for common salts in polymer-based ABS are also presented and discussed to emphasize the expanding number of aqueous two-phase systems that can actually be obtained. Finally, we address two of the main applications of ionic liquid-based ABS: extraction of biomolecules and other added-value compounds, and their use as alternative approaches for removing and recovering ionic liquids from aqueous media.</description>
		</publications>
		
		<publications id="0004">
			<title>Ionic liquids in separations of azeotropic systems–A review</title>
			<date>2012</date>
			<citation>495</citation>
			<co_authors>
				<author>AB Pereiro</author>
				<author>JMM Araújo</author>
				<author>JMSS Esperança</author>
				<author>LPN Rebelo</author>
			</co_authors>
			<publication_source>
				<source>The Journal of Chemical Thermodynamics</source>
				<volume>46</volume>
				<pag_min>2</pag_min>
				<pag_max>28</pag_max>
			</publication_source>
			<publisher>Academic Press</publisher>
			<description>Efforts to make existing separation methods more efficient and eco-friendly may get a boost from the use of a relatively new class of compounds known as ionic liquids (ILs). The separation of azeotropic mixtures has conventionally been one of the most challenging tasks in industrial processes due to the fact that their separation by simple distillation is basically impossible. This paper provides a critical review of methods using ILs as azeotrope breakers. Three separation processes were addressed: liquid–liquid extraction, extractive distillation, and supported liquid membranes. We examine the azeotrope breaking potential of ILs and compare their performance to that of conventional solvents. A systematic analysis of the influence of the structure of ILs on their azeotrope breaking capacity contributes to the establishment of guidelines for selecting the most suitable ILs for the separation of specific azeotropic mixtures.</description>
		</publications>
		
		<publications id="0005">
			<title>Menthol-based eutectic mixtures: hydrophobic low viscosity solvents</title>
			<date>2015</date>
			<citation>443</citation>
			<co_authors>
				<author>Bernardo D Ribeiro</author>
				<author>Catarina Florindo</author>
				<author>Lucas C Iff</author>
				<author>Maria AZ Coelho</author>
			</co_authors>
			<publication_source>
				<journal>ACS Sustainable Chemistry and Engineering</journal>
				<volume>3</volume>
				<issue>10</issue>
				<pag_min>2469</pag_min>
				<pag_max>2477</pag_max>
			</publication_source>
			<publisher>American Chemical Society</publisher>
			<description>Inspired by one of the major problems in the pharmaceutical industry, we advantageously used the formation of eutectic mixtures to synthesize new solvents. The aim of this work is to identify low viscosity, cheap, biodegradable and hydrophobic eutectic solvents from natural resources. Consequently, novel eutectic mixtures based on dl-menthol and naturally occurring acids, namely pyruvic acid, acetic acid, l-lactic acid, and lauric acid, were synthesized and are here reported for the first time. The obtained dl-menthol-based eutectic mixtures were analyzed using NMR and FTIR spectroscopy in order to check their structures and purities and to confirm the interaction of the two compounds leading to the eutectic formation. Important solvent thermophysical properties, such as density and viscosity, of the prepared eutectic solvents with different water contents (dried and water-saturated) were measured. Finally, taking advantage of their hydrophobic character, namely the formation of two phases with water at room temperature, four different biomolecules, caffeine, tryptophan, isophthalic acid, and vanillin, were extracted and the extraction efficiencies of the prepared eutectic solvents compared.</description>
		</publications>
		
		<publications id="0006">
			<title>Ionic liquids: first direct determination of their cohesive energy</title>
			<date>2007</date>
			<citation>345</citation>
			<co_authors>
				<author>Luis MNBF Santos</author>
				<author>Jose N Canongia Lopes</author>
				<author>Joao AP Coutinho</author>
				<author>Jose MSS Esperança</author>
				<author>Ligia R Gomes</author>
				<author>Luis PN Rebelo</author>
			</co_authors>
			<publication_source>
				<journal>Journal of the American Chemical Society</journal>
				<volume>129</volume>
				<issue>2</issue>
				<pag_min>284</pag_min>
				<pag_max>285</pag_max>
			</publication_source>
			<publisher>American Chemical Society</publisher>
			<description>Molar enthalpies of vaporization, of a homologous series of 1-alkyl-3-methylimidazolium bis(trifluoromethyl sulfonyl)imides, [Cnmim][NTf2], (with 2 ≤ n ≤ 8) were directly determined for the first time. The results obtained in this study emphasize and constitute clear experimental support for the novel concept that these liquid salts are composed of polar and nonpolar domains; that is, they exhibit microphase segregation.</description>
		</publications>
		
		<publications id="0007">
			<title>Ionic liquids in pharmaceutical applications</title>
			<date>2014</date>
			<citation>395</citation>
			<co_authors>
				<author>LC Branco</author>
				<author>LPN Rebelo</author>
			</co_authors>
			<publication_source>
				<source>Annual review of chemical and biomolecular engineering</source>
				<volume>5</volume>
				<pag_min>527</pag_min>
				<pag_max>546</pag_max>
			</publication_source>
			<publisher>Annual Reviews</publisher>
			<description>In the past several years, ionic liquids (ILs) have been at the cutting edge of the most promising science and technology. ILs not only have found applications in classical areas of knowledge but also are important candidates to solve classical problems within several societal challenges, such as clean and efficient energy, through the development of a broad swath of energy technologies, such as advanced batteries, dye-sensitized solar cells, double-layer capacitors, actuators, fuel cells, thermo-cells, and water splitting, essentially related to highly efficient carbon capture and storage technologies and resource efficiency to date. This review focuses on the application of IL methodologies to solve critical pharmaceutical problems, in particular, the low solubility and thus bioavailability of pharmaceutical compounds and the presence of polymorphs, which severely hamper the efficacy of important commercially available drugs. The development of strategies to use ILs as carriers of pharmaceutical active compounds is an extremely promising and wide avenue. Further, the synthesis of liquid salts through the discerning combination of cations and anions with several distinct pharmaceutical roles provides answers to some of today's pharmaceutical industrial challenges.</description>
		</publications>
	</researchers>
	
	
	
	
	
	<researchers id="0009">
		<name>Maria Joao Bebianno</name>
		<area>Marine and Environmental Sciences</area>
		<affiliation>University of Algarve</affiliation>
		<research_interests>
			<interest>Environmetal Sciences</interest> 
			<interest>Marine Sciences</interest> 
			<interest>Ecotoxicology</interest>  
			<interest>Analytical Chemistry</interest>  
			<interest>Nanoparticles</interest> 
		</research_interests>
		<email>mbebian@ualg.pt</email>
		<number_articles>327</number_articles>
		
		<publications id="0001">
			<title>Metal handling strategies in molluscs</title>
			<date>1998</date>
			<citation>357</citation>
			<co_authors>
				<author>William J Langston</author>
				<author>Gary R Burt</author>
			</co_authors>
			<publication_source>
				<book>Metal metabolism in aquatic environments</book>
				<pag_min>219</pag_min>
				<pag_max>283</pag_max>
			</publication_source>
			<publisher>Springer US</publisher>
			<description>The concentration and distribution of metals in molluscan tissues, as in any other organism, is highly dependent on the biochemical processes of metal metabolism occurring within cells. Many of these processes and associated metal-binding systems are common to a broad range of phyla, reflecting their evolutionary success in regulating the essential elements (e.g. Cu, Zn, Fe, Mn, Co) for physiological, biochemical and morphological purposes. The involvement of metallothionein (MT) in buffering intracellular metal ions is one of the best examples of such a ubiquitous metal sequestration system. Proteins of a metallothionein-like nature are represented in many branches of the phylogenetic tree, from microorganisms to humans, and consequently are regarded as a central constituent of metal metabolism. Other sequestration mechanisms are relatively unique to individual taxonomie groups so that even within a single phylum, exemplified here by molluscs, there may be a large diversity in the nature and expression of metal metabolism, according to metal requirements. The result is manifested by a wide range of behaviour in terms of uptake, detoxification and storage — the major components of metal bioaccumulation.</description>
		</publications>
		
		<publications id="0002">
			<title>Influence of metal exposure on metallothionein synthesis and lipid peroxidation in two bivalve mollusks: the oyster (Crassostrea gigas) and the mussel (Mytilus edulis)</title>
			<date>2002</date>
			<citation>242</citation>
			<co_authors>
				<author>Florence Geret</author>
				<author>Agnès Jouan</author>
				<author>Vincent Turpin</author>
				<author>Richard P Cosson</author>
			</co_authors>
			<publication_source>
				<journal>Aquatic Living Resources</journal>
				<volume>15</volume>
				<issue>1</issue>
				<pag_min>61</pag_min>
				<pag_max>66</pag_max>
			</publication_source>
			<publisher>EDP Sciences</publisher>
			<description>The impact of metals (silver, cadmium, copper, mercury and zinc) on metallothionein (MT) and malondialdehyde (MDA) levels of the oyster (Crassostrea gigas) and the mussel (Mytilus edulis) was studied after 4 or 21 days of metal exposure. Moreover, total protein levels were determined. After 4 days of metal exposure, although C. gigas and M. edulis accumulated cadmium and mercury concentrations in the gills and digestive gland, no significant variation of total protein level was occurred. After 21 days of exposure, metals were bioaccumulated in the gills and the digestive gland of both mollusks. A decrease of total protein concentrations in the gills of oysters and the digestive gland of mussels and an increase on metallothionein concentrations in the gills of both mollusks were observed. An increase of MDA levels was noticed for the gills and the digestive gland of mussels exposed for 21 days to either cadmium, silver or mercury whereas a decrease of MDA levels was observed in the gills of the oysters exposed for the same time to the same metals. The levels of proteins, MDA and MT were metal, species or organ dependent.</description>
		</publications>
		
		<publications id="0003">
			<title>Effects of Copper Nanoparticles Exposure in the Mussel Mytilus galloprovincialis</title>
			<date>2011</date>
			<citation>269</citation>
			<co_authors>
				<author>Tania Gomes</author>
				<author>Jose P Pinheiro</author>
				<author>Ibon Cancio</author>
				<author>Catarina G Pereira</author>
				<author>Catia Cardoso</author>
			</co_authors>
			<publication_source>
				<journal>Environmental science and technology</journal>
				<volume>45</volume>
				<issue>21</issue>
				<pag_min>9356</pag_min>
				<pag_max>9362</pag_max>
			</publication_source>
			<publisher>American Chemical Society</publisher>
			<description>CuO NPs are widely used in various industrial and commercial applications. However, little is known about their potential toxicity or fate in the environment. In this study the effects of copper nanoparticles were investigated in the gills of mussels Mytilus galloprovincialis, comparative to Cu2+. Mussels were exposed to 10 μgCu·L–1 of CuO NPs and Cu2+ for 15 days, and biomarkers of oxidative stress, metal exposure and neurotoxicity evaluated. Results show that mussels accumulated copper in gills and responded differently to CuO NPs and Cu2+, suggesting distinct modes of action. CuO NPs induced oxidative stress in mussels by overwhelming gills antioxidant defense system, while for Cu2+ enzymatic activities remained unchanged or increased. CuO NPs and Cu2+ originated lipid peroxidation in mussels despite different antioxidant efficiency. Moreover, an induction of MT was detected throughout the exposure in mussels exposed to nano and ionic Cu, more evident in CuO NPs exposure. Neurotoxic effects reflected as AChE inhibition were only detected at the end of the exposure period for both forms of copper. In overall, these findings show that filter-feeding organisms are significant targets for nanoparticle exposure and need to be included when evaluating the overall toxicological impact of nanoparticles in the aquatic environment.</description>
		</publications>
		
		<publications id="0004">
			<title>Genotoxicity of copper oxide and silver nanoparticles in the mussel Mytilus galloprovincialis</title>
			<date>2013</date>
			<citation>220</citation>
			<co_authors>
				<author>Tania Gomes</author>
				<author>Olinda Araújo</author>
				<author>Rita Pereira</author>
				<author>Ana C Almeida</author>
				<author>Alexandra Cravo</author>
			</co_authors>
			<publication_source>
				<journal>Marine environmental research</journal>
				<volume>84</volume>
				<pag_min>51</pag_min>
				<pag_max>59</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>Though there is some information on cytotoxicity of copper nanoparticles and silver nanoparticles on human cell lines, there is no information on their genotoxic and cytotoxic behaviour in bivalve molluscs. The aim of this study was to investigate the genotoxic impact of copper oxide and silver nanoparticles using mussels Mytilus galloprovincialis. Mussels were exposed to 10 μg L−1 of CuO nanoparticles and Cu2+ and Ag nanoparticles and Ag+ for 15 days to assess genotoxic effects in hemocytes using the comet assay. The results obtained indicated that copper and silver forms (nanoparticles and ionic) induced DNA damage in hemolymph cells and a time-response effect was evident when compared to unexposed mussels. Ionic forms presented higher genotoxicity than nanoparticles, suggesting different mechanisms of action that may be mediated through oxidative stress. DNA strand breaks proved to be a useful biomarker of exposure to genotoxic effects of CuO and Ag nanoparticles in marine molluscs.</description>
		</publications>
		
		<publications id="0005">
			<title>Effects of non-steroidal anti-inflammatory drug (NSAID) diclofenac exposure in mussel Mytilus galloprovincialis</title>
			<date>2014</date>
			<citation>196</citation>
			<co_authors>
				<author>Maria Gonzalez-Rey</author>
			</co_authors>
			<publication_source>
				<journal>Aquatic toxicology</journal>
				<volume>148</volume>
				<pag_min>221</pag_min>
				<pag_max>230</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>In recent years, research studies have increasingly focused on assessing the occurrence of active pharmaceutical ingredients (APIs) in ecosystems. However, much remains unknown concerning the potential effects on APIs on non-target organisms due to the complexity of the mode of action, reactivity and bioconcentration potential for each specific drug. The non-steroidal anti-inflammatory drug (NSAID) diclofenac (DCF) is one of the most frequently detected APIs in surface waters worldwide and has recently been included in the list of priority substances under the European Commission. In this study, mussels (Mytilus galloprovincialis) were exposed to an environmentally relevant nominal concentration of DCF (250 ng L−1) over 15 days. The responses of several biomarkers were assessed in the mussel tissues: condition index (CI); superoxide dismutase (SOD), catalase (CAT), glutathione reductase (GR) and phase II glutathione-S-transferase (GST) activities, lipid peroxidation levels (LPO) associated with oxidative stress, acetylcholinesterase (AChE) activity related to neurotoxic effects and vitellogenin-like proteins linked to endocrine disruption. This study demonstrated significant induction of SOD and GR activities in the gills in addition to high CAT activity and LPO levels in the digestive gland. Phase II GST remained unaltered in both tissues, while the up-regulation of the AChE activity was directly related to the vitellogenin-like protein levels in exposed females, indicating an alteration in the estrogenic activity, rather than a breakdown in cholinergic neurotransmission function. This study confirmed that DCF at a concentration often observed in surface water induces tissue-specific biomarker responses. Finally, this study also revealed the importance of a multi-biomarker approach when assessing the potentially deleterious effects in a species that may be vulnerable to the continuously discharge of APIs into the ecosystems; this approach provides crucial new information regarding the unknown effects of DCF.</description>
		</publications>
		
		<publications id="0006">
			<title>Metallothionein induction inMytilus edulis exposed to cadmium</title>
			<date>1991</date>
			<citation>186</citation>
			<co_authors>
				<author>WJ Langston</author>
			</co_authors>
			<publication_source>
				<journal>Marine Biology</journal>
				<volume>108</volume>
				<pag_min>91</pag_min>
				<pag_max>96</pag_max>
			</publication_source>
			<publisher>Springer-Verlag</publisher>
			<description>The exposure of mussels,Mytilus edulis, collected from Whitsand Bay, southwest England, in August 1988, to sublethal concentrations of cadmium (400µg l−1) for 65 d resulted in the induction of metallothionein (MT) synthesis in the soft tissues. In cadmium-exposed mussels, metallothionein concentrations, measured by differential pulse polarography, increased by a factor of three, from 2 to 3 mg g−1 to a maximum of 9 mg g−1 after 30 d. No significant changes could be detected in controls. Cadmium accumulated in the soft tissues of mussels correlated significantly with metallothionein concentrations and can be described by the relationship: MT (mg g−1)=0.045 Cd (µg g−1)+3.03 (r=0.803,P lower than 0.001). Gel chromatography of heat-treated cytosolic extracts showed that the accumulated cadmium is bound principally to the newly formed metallothioneins. Copper and zinc were also analysed in the whole soft-tissues and in subcellular fractions of cadmium-exposed mussels. Although copper concentrations were not affected by cadmium-exposure, zinc levels were significantly reduced. The results demonstrate that the induction of metallothioneins inM. edulis is a quantifiable biological response to sublethal levels of cadmium exposure.</description>
		</publications>
		
		<publications id="0007">
			<title>Antioxidant and lipid peroxidation responses in Mytilus galloprovincialis exposed to mixtures of benzo (a) pyrene and copper</title>
			<date>2011</date>
			<citation>167</citation>
			<co_authors>
				<author>Vera L Maria</author>
			</co_authors>
			<publication_source>
				<journal>Comparative Biochemistry and Physiology Part C: Toxicology and Pharmacology</journal>
				<volume>154</volume>
				<issue>1</issue>
				<pag_min>56</pag_min>
				<pag_max>63</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>This study aimed to assess the antioxidant system potential and lipid peroxidative effects, in the gill and digestive gland of Mytilus galloprovincialis exposed to individual and binary mixtures of benzo(a)pyrene (BaP) and Cu for 7 days. Data demonstrated that in mussels exposed to BaP antioxidant enzymes (catalase – CAT, total glutathione peroxidase – tGPx, glutathione S-transferase – GST and glutathione reductase – GR) and lipid peroxidation (LPO) increased in the gill. On the contrary, in the digestive gland inhibitory antioxidant effects (superoxide dismutase-SOD, GR, metallothioneins-MT) and no changes in LPO levels were detected. Cu was also a potent oxidant agent since MT and LPO levels increased in mussel gill, despite no LPO effect in the digestive gland. For both single contaminants the organ specificity and distinct physiologic/metabolism roles were evident in terms of antioxidant capacity. Gill SOD inhibition, MT and GST unchanged was a result of “simple independent action” of exposure to BaP and Cu. “Interactions” in the binary mixtures, led to absence of changes in LPO effects. In the digestive gland, BaP and Cu interactions were also responsible for the GST and LPO enhancement (antagonistic effects). The current findings demonstrate the differences in antioxidant responses where the organ dependency highlights each contaminant particular mode of action. Generally, in the gill “non-interactive” effects occurred with the lowest Cu concentration while “interactions” exist for the mixture with the highest Cu concentrations. In the digestive gland, “interactions” and “no interaction” effects occurred in all the binary mixtures. Complex contaminant mixtures interact differently based on target tissue which may lead to an imbalance in the mussels health status.</description>
		</publications>
	</researchers>
	
	
	
	
	<researchers id="0010">
		<name>Adelino Canario</name>
		<area>Biology</area>
		<area>Physiology</area>
		<affiliation>University of Algarve</affiliation>
		<affiliation>Shanghai Ocean University</affiliation>
		<research_interests>
			<interest>Comparative Endocrinology</interest> 
			<interest>Behaviour</interest> 
			<interest>Reproduction</interest>  
			<interest>Osmoregulation</interest>  
			<interest>Chemical Communication</interest> 
		</research_interests>
		<email>acanario@ualg.pt</email>
		<number_articles>460</number_articles>
	
		<publications id="0001">
			<title>Watching fights raises fish hormone levels</title>
			<date>2001</date>
			<citation>240</citation>
			<co_authors>
				<author>Rui F Oliveira</author>
				<author>Marco Lopes</author>
				<author>Luis A Carneiro</author>
			</co_authors>
			<publication_source> 
				<journal>Nature</journal>
				<volume>409</volume>
				<issue>6819</issue>
				<pag_min>475</pag_min>
				<pag_max>475</pag_max>
			</publication_source> 
			<publisher>Nature Publishing Group UK</publisher>
			<description>Cichlid fish wrestling for dominance induce an androgen surge in male spectators.</description>
		</publications>
		
		<publications id="0002">
			<title>Social modulation of sex steroid concentrations in the urine of male cichlid fishOreochromis mossambicus</title>
			<date>1996</date>
			<citation>240</citation>
			<co_authors>
				<author>Rui F Oliveira</author>
				<author> Vitor C Almada</author>
			</co_authors>
			<publication_source> 
				<journal>Hormones and Behavior</journal>
				<volume>30</volume>
				<issue>1</issue>
				<pag_min>2</pag_min>
				<pag_max>12</pag_max>
			</publication_source> 
			<publisher>Academic Press</publisher>
			<description>The relationship between urinary concentrations (free + sulfates + glucuronides) of the steroids testosterone (T), 11-ketotestosterone (11KT), 17 alpha, 20 beta-dihydroxy-4-pregnen-3-one (17, 20 beta-P) and 17 alpha, 20 alpha-dihydroxy-4-pregnen-3-one (17, 20 alpha-P), and the social behavior of males of the cichlid fish Oreochromis mossambicus was investigated. After 8 days of isolation none of the steroids were good predictors of social dominance developed after subsequent formation of all-male groups. One day after group formation dominance indexes were good predictors of the urine concentrations of all sex steroids. Dominance indexes and androgen concentrations measured after all-male group formation were positively correlated with territoriality, courtship rate, and nest size. Similar relationships were found for progestins with the exception that they were not correlated with courtship rate. All-male group formation was also accompanied by an increase in urinary sex steroid concentrations in fish that became territorial and a decrease in non-territorial fish with the exception of T, which increased in both groups. Addition of ovulating females caused steroid concentrations to return to levels near isolation, except for 17, 20 alpha-P in territorials, which underwent a large increase. Thus, social interactions may have an important modulatory effect on sex steroid concentrations in O. mossambicus.</description>
		</publications>	
		
		<publications id="0003">
			<title>Two estrogen receptors expressed in the teleost fish, Sparus aurata: cDNA cloning, characterization and tissue distribution</title>
			<date>2000</date>
			<citation>165</citation>
			<co_authors>
				<author>Silvia Socorro</author>
				<author>DM Powe</author>
				<author>PE Olsson</author>
			</co_authors>
			<publication_source> 
				<journal>Journal of Endocrinology</journal>
				<volume>166</volume>
				<issue>2</issue>
				<pag_min>293</pag_min>
				<pag_max>306</pag_max>
			</publication_source> 
			<publisher>England [etc.] Oxford University Press [etc.]</publisher>
			<description>Estrogen is an essential hormone for many reproductive and non-reproductive functions. The function of estrogen in the reproductive cycle of seabream (Sparus aurata), a protandrous hermaphrodite teleost fish, is complex but it is understood to be involved in sex inversion, a process that occurs in some individuals during the second reproductive season. Estrogen action is mediated by two estrogen receptor (ER) subtypes designated alpha and beta. As a step to understanding the mechanisms of estrogen action during natural and induced sex reversal in seabream, we have isolated two cDNAs encoding distinct forms of ER homologous to mammalian ERalpha and ERbeta. The seabream ERalpha clone (sbERalpha1), which was truncated in the A/B domain, corresponded to a variant differing in five amino acids from another recently cloned sbERalpha. The ERbeta clone (sbERbeta) encoded a protein 559 amino acids long and showed only 40% identity to sbERalpha. Northern blot analysis of liver and ovary mRNA indicated the presence of several transcripts of the two receptor subtypes. PCR analysis showed that the two receptors differed in their expression pattern. sbERalpha had a more restricted distribution, occurring mainly in testis, liver and heart, and sbERbeta was present in most tissues, being more abundant in ovary, testis, liver, intestine and kidney. The presence in seabream of two ERs with several ER transcripts and their pattern of distribution are consistent with the widespread effects of estrogen in different tissues.</description>
		</publications>
		
		<publications id="0004">
			<title>No hormonal response in tied fights</title>
			<date>2005</date>
			<citation>164</citation>
			<co_authors>
				<author>Rui F Oliveira</author>
				<author>Luis A Carneiro</author>
			</co_authors>
			<publication_source> 
				<journal>Nature</journal>
				<volume>437</volume>
				<issue>7056</issue>
				<pag_min>207</pag_min>
				<pag_max>208</pag_max>
			</publication_source> 
			<publisher>Nature Publishing Group UK</publisher>
			<description>Androgens are the principal sex steroids controlling reproduction and aggression in male fish, but their production can also be affected by social interactions,. Here we show that androgen concentrations are not significantly increased in cichlid fish (Oreochromis mossambicus) that are fighting their own image in a mirror, despite their aggressive behaviour towards the virtual intruder. Our results indicate that the hormonal response normally triggered in male contests is not induced under these circumstances by the act of fighting itself, and that it may therefore depend on some indicator of relative fighting ability that cannot be delivered by a mirror-image challenger.</description>
		</publications>
		
		<publications id="0005">
			<title>17α, 20β-dihydroxy-4-pregnen-3-one 20-sulphate: a major new metabolite of the teleost oocyte maturation-inducing steroid</title>
			<date>1992</date>
			<citation>128</citation>
			<co_authors>
				<author>AP Scott</author>
			</co_authors>
			<publication_source> 
				<journal>General and Comparative Endocrinology</journal>
				<volume>85</volume>
				<issue>1</issue>
				<pag_min>91</pag_min>
				<pag_max>100</pag_max>
			</publication_source> 
			<publisher>Academic Press</publisher>
			<description>This paper describes the discovery of 17α,20β-dihydroxy-4-pregnen-3-one 20-sulphate (17,20β-P-sulphate) in urine of male and female plaice Pleuronectes platessa, in female Atlantic salmon Salmo salar and in female Dover sole Solea solea. 17α,20β-dihydroxy-4-pregnen-3-one (17,20β-P) induces oocyte final maturation in teleosts and, whereas levels of the free steroid in maturing/ovulating female plaice are generally less than 1 ng ml−1 and poorly associated with the stage of maturation, the levels of 17,20β-P-sulphate are around 1500 ng ml−1 urine, 11 ng ml−1 blood plasma and six-fold higher in maturing than in nonmaturing fish. There are also high levels in spermiating male plaice (ca. 2300 ng ml−1 urine and 20 ng ml−1 blood plasma). 17,20β-P-sulphate cannot be hydrolysed by snail (Helix pomatia) sulphatase, but can be completely solvolysed by treatment with trifluoroacetic acid (TFA)/ethyl acetate () at 45° for 18 hr. A number of other sulphated steroids have been identified in plaice urine: cortisol, 11-deoxycortisol and 3α,17α,21-trihydroxy-5β-pregnan-20-one (which can all be hydrolysed by snail juice); 17α,20α-dihydroxy-4-pregnen-3-one and 17α,20β,21-trihydroxy-4-pregnen-3-one (which can both be solvolysed by TFA/ethyl acetate).</description>
		</publications>
		
		<publications id="0006">
			<title>Calcium balance in sea bream (Sparus aurata): the effect of oestradiol-17 beta</title>
			<date>2002</date>
			<citation>120</citation>
			<co_authors>
				<author>PM Guerreiro</author>
				<author>J Fuentes</author>
				<author>Deborah Power</author>
			</co_authors>
			<publication_source> 
				<journal>Journal of Endocrinology</journal>
				<volume>173</volume>
				<issue>2</issue>
				<pag_min>377</pag_min>
				<pag_max>385</pag_max>
			</publication_source> 
			<publisher>Society for Endocrinology</publisher>
			<description>In all teleost fishes vitellogenesis is triggered and maintained by oestradiol-17beta (E2) and is accompanied by an increase of blood plasma calcium and phosphate. The action of this hormone on calcium metabolism was investigated by treating fast-growing immature juvenile sea bream (Sparus aurata) with coconut butter implants alone (control) or implants containing 10 microg/g E2. Treatment with E2 induced the production of circulating vitellogenin, a 2.5-fold increase in plasma ionic Ca2+ and a 10-fold increase in plasma total calcium, largely bound to protein. In contrast to freshwater species, which obtain most of their calcium from the environment directly through the gills, the intestinal component of calcium uptake of the salt water-living sea bream represented up to 60-70% of the total uptake. The whole body calcium uptake, expressed as the sum of calcium obtained via intestinal and extra-intestinal (likely branchial) routes increased significantly in response to E2. Combined influx and unchanged efflux rates resulted in a significant 31% increase in net calcium uptake. There was no evidence for an effect of E2 on the calcium and phosphate content of the scales or the tartrate-resistant acid phosphatase activity (an index for bone/scale osteoclast activity). While most freshwater fish appear to rely on internal stores of calcium, i.e. bone and/or scales to increase calcium availability, the marine sea bream accommodates calcium-transporting mechanisms to obtain calcium from the environment and preserve internal stores. These observations suggest that a fundamental difference may exist in the E2-dependent calcium regulation between freshwater and marine teleosts.</description>
		</publications>
		
		<publications id="0007">
			<title>Five gonadotrophin-releasing hormone receptors in a teleost fish: isolation, tissue distribution and phylogenetic relationships</title>
			<date>2005</date>
			<citation>113</citation>
			<co_authors>
				<author>Natalia Moncaut</author>
				<author>Gustavo Somoza</author>
				<author> Deborah M Power</author>
			</co_authors>
			<publication_source> 
				<journal>Journal of molecular endocrinology</journal>
				<volume>34</volume>
				<issue>3</issue>
				<pag_min>767</pag_min>
				<pag_max>780</pag_max>
			</publication_source> 
			<publisher>Bristol, UK: Journal of Endocrinology (Ltd. by Guarantee), c1988-</publisher>
			<description>Gonadotrophin-releasing hormone (GnRH) is the main neurohormone controlling gonadotrophin release in all vertebrates, and in teleost fish also of growth hormone and possibly of other adenohypophyseal hormones. Over 20 GnRHs have been identified in vertebrates and protochoordates and shown to bind cognate G-protein couple receptors (GnRHR). We have searched the puffer fish, Fugu rubripes, genome sequencing database, identified five GnRHR genes and proceeded to isolate the corresponding complementary DNAs in European sea bass, Dicentrachus labrax. Phylogenetic analysis clusters the European sea bass, puffer fish and all other vertebrate receptors into two main lineages corresponding to the mammalian type I and II receptors. The fish receptors could be subdivided in two GnRHR1 (A and B) and three GnRHR2 (A, B and C) subtypes. Amino acid sequence identity within receptor subtypes varies between 70 and 90% but only 50–55% among the two main lineages in fish. All European sea bass receptor mRNAs are expressed in the anterior and mid brain, and all but one are expressed in the pituitary gland. There is differential expression of the receptors in peripheral tissues related to reproduction (gonads), chemical senses (eye and olfactory epithelium) and osmoregulation (kidney and gill). This is the first report showing five GnRH receptors in a vertebrate species and the gene expression patterns support the concept that GnRH and GnRHRs play highly diverse functional roles in the regulation of cellular functions, besides the ‘‘classical’’role of pituitary function regulation.</description>
		</publications>
	</researchers>
	
	
	
	
	<researchers id="0011">
		<name>Henrique Madeira</name>
		<area>Software and Systems Engineering</area>
		<affiliation>University of Coimbra</affiliation>
		<affiliation>CISUC</affiliation> 
		<research_interests> 
			<interest>Dependable computing</interest>
			<interest>fault injection</interest>
			<interest>dependability benchmarking</interest>
			<interest>biofeedback</interest>
			<interest>human error</interest>
		</research_interests>
		<email>henrique@dei.uc.pt</email>
		<number_articles>357</number_articles>
	
		<publications id="0001">
			<title>Xception: A technique for the experimental evaluation of dependability in modern computers</title>
			<date>1998</date>
			<citation>578</citation>
			<co_authors>
				<author>Joao Carreira</author>
				<author>Joao Gabriel Silva</author>
			</co_authors>
			<publication_source> 
				<journal>Software Engineering, IEEE Transactions on</journal>
				<volume>24</volume>
				<issue>2</issue>
				<pag_min>125</pag_min>
				<pag_max>136</pag_max>
			</publication_source> 
			<publisher>IEEE</publisher>
			<description>An important step in the development of dependable systems is the validation of their fault tolerance properties. Fault injection has been widely used for this purpose, however with the rapid increase in processor complexity, traditional techniques are also increasingly more difficult to apply. This paper presents a new software-implemented fault injection and monitoring environment, called Xception, which is targeted at modern and complex processors. Xception uses the advanced debugging and performance monitoring features existing in most modern processors to inject quite realistic faults by software, and to monitor the activation of the faults and their impact on the target system behavior in detail. Faults are injected with minimum interference with the target application. The target application is not modified, no software traps are inserted, and it is not necessary to execute the target application in special trace mode (the application is executed at full speed). Xception provides a comprehensive set of fault triggers, including spatial and temporal fault triggers, and triggers related to the manipulation of data in memory. Faults injected by Xception can affect any process running on the target system (including the kernel), and it is possible to inject faults in applications for which the source code is not available. Experimental, results are presented to demonstrate the accuracy and potential of Xception in the evaluation of the dependability properties of the complex computer systems available nowadays.</description>
		</publications>
		
		<publications id="0002">
			<title>Dependability benchmarking of web-servers</title>
			<date>2004</date>
			<citation>90</citation>
			<co_authors>
				<author>Joao Duraes</author>
				<author>Marco Vieira</author>
			</co_authors>
			<publication_source> 
				<journal>Computer Safety, Reliability, and Security</journal>
				<pag_min>297</pag_min>
				<pag_max>310</pag_max>
			</publication_source> 	
			<publisher>Springer Berlin/Heidelberg</publisher>
			<description>The assessment of the dependability properties of a system (dependability benchmarking) is a critical step when choosing among similar components/products. This paper presents a proposal for the benchmarking of the dependability properties of web-servers. Our benchmark is composed of the three key components: measures, workload, and faultload. We use the SPECWeb99 benchmark as starting point, adopting the workload and performance measures from this performance benchmark, and we added the faultload and new measures related to dependability. We illustrate the use of the proposed benchmark through a case-study involving two widely used web servers (Apache and Abyss) running on top of three different operating systems. The faultloads used encompass software faults, hardware faults and network faults. We show that by using the proposed dependability benchmark it is possible to observe clear differences regarding dependability properties of the web-servers.</description>
		</publications>
		
		<publications id="0003">
			<title>Analysis of field data on web security vulnerabilities</title>
			<date>2014</date>
			<citation>80</citation>
			<co_authors>
				<author>Marco Vieira</author>
				<author></author>
				<author>Jose Fonseca</author>
				<author>Nuno Seixas</author>
			</co_authors>
			<publication_source> 
				<journal>IEEE Transactions on Dependable and Secure Computing</journal>
				<volume>11</volume>
				<issue>2</issue>
				<pag_min>89</pag_min>
				<pag_max>100</pag_max>
			</publication_source> 
			<publisher>IEEE</publisher>
			<description>Most web applications have critical bugs (faults) affecting their security, which makes them vulnerable to attacks by hackers and organized crime. To prevent these security problems from occurring it is of utmost importance to understand the typical software faults. This paper contributes to this body of knowledge by presenting a field study on two of the most widely spread and critical web application vulnerabilities: SQL Injection and XSS. It analyzes the source code of security patches of widely used Web applications written in weak and strong typed languages. Results show that only a small subset of software fault types, affecting a restricted collection of statements, is related to security. To understand how these vulnerabilities are really exploited by hackers, this paper also presents an analysis of the source code of the scripts used to attack them. The outcomes of this study can be used to train software developers and code inspectors in the detection of such faults and are also the foundation for the research of realistic vulnerability and attack injectors that can be used to assess security mechanisms, such as intrusion detection systems, vulnerability scanners, and static code analyzers.</description>
		</publications>
	
		<publications id="0004">
			<title>Verification and validation of (real time) COTS products using fault injection techniques</title>
			<date>2007</date>
			<citation>41</citation>
			<co_authors>
				<author>R Barbosa </author>
				<author>N Silva</author>
				<author>J Duraes</author>
			</co_authors>
			<publication_source> 
				<conference>Commercial-off-the-Shelf (COTS)-Based Software Systems, 2007. ICCBSS'07. Sixth International IEEE Conference on</conference>
				<pag_min>233</pag_min>
				<pag_max>242</pag_max>
			</publication_source> 
			<publisher>IEEE</publisher>
			<description>With the goal of reducing time to market and project costs, the current trend of real time business and mission critical systems is evolving from the development of custom made applications to the use of commercial off the shelf (COTS) products. Obviously, the same confidence and quality of the custom made software components is expected from the commercial applications. In most cases, such products (COTS) are not designed with stringent timing and/or safety requirements as priorities. Thus, to decrease the gap between the use of custom made components and COTS components, this paper presents a methodology for evaluating COTS products in the scope of dependable, real time systems, through the application of fault injection techniques at key points of the software engineering process. By combining the use of robustness testing (fault injection at interface level) with software fault injection (using educated fault injection operators), a COTS component can be assessed in the context of the system it will belong to, with special emphasis given to timing and safety constraints that are usually imposed by the target real time dependable environment. In the course of this work, three case studies have been performed to assess the methodology using realistic scenarios that used common COTS products. Results for one case study are presented</description>
		</publications>
		
		<publications id="0005">
			<title>On-line signature learning and checking</title>
			<date>1992</date>
			<citation>44</citation>
			<co_authors>
				<author>Joao G Silva</author>
			</co_authors>
			<publication_source> 
				<journal>Dependable Computing for Critical Applications, Springer-Verlag, JF and RD Schlichting (eds</journal>
				<volume>6</volume>
				<pag_min>395</pag_min>
				<pag_max>420</pag_max>
			</publication_source> 
			<description>This paper presents a new approach to concurrent error detection in multiple processor systems using on-line signature analysis. In this new technique, called On-line Signature Learning and Checking (OSLC), the block identification and the reference signature generation are performed at run time. Many hardware control signals are included in the signatures, which improves the error detection coverage, and the alterations and/or extensions in the compilers, assemblers and loaders are avoided. In OSLC the signatures are stored in the local memory of a watchdog processor, the Checker, which is based on a new principle that reduces the storage requirements of control flow information to less than 2% of the signature overhead. Furthermore, the Checker is very simple and can check several processors concurrently. A demonstration system of this technique has been designed and built. Results of fault injection experiments have shown that 99.4% of instruction type faults can be detected by OSLC with a very short latency (26 μSec). The coverage for general faults is 94.5% and the average latency is 464 sec.</description>
		</publications>
		
		<publications id="0006">
			<title>Approximate query answering using data warehouse striping</title>
			<date>2002</date>
			<citation>64</citation>
			<co_authors>
				<author>Jorge R Bernardino</author>
				<author>Pedro S Furtado</author>
			</co_authors>
			<publication_source> 
				<journal>Journal of Intelligent Information Systems</journal>
				<volume>19</volume>
				<issue>2</issue>
				<pag_min>145</pag_min>
				<pag_max>167</pag_max>
			</publication_source> 
			<publisher>Springer Netherlands</publisher>
			<description>This paper presents and evaluates a simple but very effective method to implement large data warehouses on an arbitrary number of computers, achieving very high query execution performance and scalability. The data is distributed and processed in a potentially large number of autonomous computers using our technique called data warehouse striping (DWS). The major problem of DWS technique is that it would require a very expensive cluster of computers with fault tolerant capabilities to prevent a fault in a single computer to stop the whole system. In this paper, we propose a radically different approach to deal with the problem of the unavailability of one or more computers in the cluster, allowing the use of DWS with a very large number of inexpensive computers. The proposed approach is based on approximate query answering techniques that make it possible to deliver an approximate answer to the user even when one or more computers in the cluster are not available. The evaluation presented in the paper shows both analytically and experimentally that the approximate results obtained this way have a very small error that can be negligible in most of the cases.</description>
		</publications>
		
		<publications id="0007">
			<title>Effective Detection of SQL/XPath Injection Vulnerabilities in Web Services</title>
			<date>2009</date>
			<citation>108</citation>
			<co_authors>
				<author>Nuno Antunes</author>
				<author>Nuno Laranjeiro</author>
				<author>Marco Vieira</author>
			</co_authors>
			<publication_source> 
				<conference>Services Computing, 2009. SCC'09. IEEE International Conference on</conference>
				<pag_min>260</pag_min>
				<pag_max>267</pag_max>
			</publication_source> 
			<publisher>IEEE</publisher>
			<description>This paper proposes a new automatic approach for the detection of SQL Injection and XPath Injection vulnerabilities, two of the most common and most critical types of vulnerabilities in Web services. Although there are tools that allow testing Web applications against security vulnerabilities, previous research shows that the effectiveness of those tools in Web services environments is very poor. In our approach a representative workload is used to exercise the Web service and a large set of SQL/XPath injection attacks are applied to disclose vulnerabilities. Vulnerabilities are detected by comparing the structure of the SQL/XPath commands issued in the presence of attacks to the ones previously learned when running the workload in the absence of attacks. Experimental evaluation shows that our approach performs much better than known tools (including commercial ones), achieving extremely high detection coverage while maintaining the false positives rate very low.</description>
		</publications>
	</researchers>




	<researchers id="0012">
		<name>Joao Castelhano</name>
		<area>Health sciences</area>
		<area>Biomedical sciences</area>
		<affiliation>ICNAS/CIBIT University of Coimbra</affiliation>
		<research_interests> 
			<interest>Cognitive Neuroscience</interest>
			<interest>Multimodal Medical Imaging</interest>
			<interest>Biomedical Engineering</interest>
			<interest>Visual perception</interest>
			<interest>Data Analyst</interest>
		</research_interests>
		<email>joaocastelhano@uc.pt</email>
		<number_articles>124</number_articles>
	
		<publications id="0001">
			<title>The role of the insula in intuitive expert bug detection in computer code: an fMRI study</title>
			<date>2019</date>
			<citation>63</citation>
			<co_authors>
				<author>Isabel C Duarte</author>
				<author>Carlos Ferreira</author>
				<author>Joao Duraes</author>
				<author>Miguel Castelo-Branco</author>
			</co_authors>
			<publication_source> 
				<journal>Brain imaging and behavior</journal>
				<volume>13</volume>
				<pag_min>623</pag_min>
				<pag_max>637</pag_max>
			</publication_source> 
			<publisher>Springer US</publisher>
			<description>Software programming is a complex and relatively recent human activity, involving the integration of mathematical, recursive thinking and language processing. The neural correlates of this recent human activity are still poorly understood. Error monitoring during this type of task, requiring the integration of language, logical symbol manipulation and other mathematical skills, is particularly challenging. We therefore aimed to investigate the neural correlates of decision-making during source code understanding and mental manipulation in professional participants with high expertise. The present fMRI study directly addressed error monitoring during source code comprehension, expert bug detection and decision-making. We used C code, which triggers the same sort of processing irrespective of the native language of the programmer. We discovered a distinct role for the insula in bug monitoring and detection and a novel connectivity pattern that goes beyond the expected activation pattern evoked by source code understanding in semantic language and mathematical processing regions. Importantly, insula activity levels were critically related to the quality of error detection, involving intuition, as signalled by reported initial bug suspicion, prior to final decision and bug detection. Activity in this salience network (SN) region evoked by bug suspicion was predictive of bug detection precision, suggesting that it encodes the quality of the behavioral evidence. Connectivity analysis provided evidence for top-down circuit "reutilization" stemming from anterior cingulate cortex (BA32), a core region in the SN that evolved for complex error monitoring such as required for this type of recent human activity. Cingulate (BA32) and anterolateral (BA10) frontal regions causally modulated decision processes in the insula, which in turn was related to activity of math processing regions in early parietal cortex. In other words, earlier brain regions used during evolution for other functions seem to be reutilized in a top-down manner for a new complex function, in an analogous manner as described for other cultural creations such as reading and literacy.</description>
		</publications>
	
		<publications id="0002">
			<title>Spotting Problematic Code Lines using Nonintrusive Programmers' Biofeedback</title>
			<date>2019</date>
			<citation>19</citation>
			<co_authors>
				<author>Ricardo Couceiro</author>
				<author>Raul Barbosa</author>
				<author>Joao Duraes</author>
				<author>Gonçalo Duarte</author>
				<author>Catarina Duarte</author>
				<author>Cesar Teixeira</author>
				<author>Nuno Laranjeiro</author>
				<author>Júlio Medeiros</author>
				<author> Paulo Carvalho</author>
				<author>Miguel Castelo Branco</author>
			</co_authors>
			<publication_source> 
				<conference>2019 IEEE 30th International Symposium on Software Reliability Engineering (ISSRE)</conference>
				<pag_min>93</pag_min>
				<pag_max>103</pag_max>
			</publication_source> 
			<publisher>IEEE</publisher>
			<description>Recent studies have shown that programmers' cognitive load during typical code development activities can be assessed using wearable and low intrusive devices that capture peripheral physiological responses driven by the autonomic nervous system. In particular, measures such as heart rate variability (HRV) and pupillography can be acquired by nonintrusive devices and provide accurate indication of programmers' cognitive load and attention level in code related tasks, which are known elements of human error that potentially lead to software faults. This paper presents an experimental study designed to evaluate the possibility of using HRV and pupillography together with eye tracking to identify and annotate specific code lines (or even finer grain lexical tokens) of the program under development (or under inspection) with information on the cognitive load of the programmer while dealing with such lines of code. The experimental data is discussed in the paper to assess different alternatives for using code annotations representing programmers' cognitive load while producing or reading code. In particular, we propose the use of biofeedback code highlighting techniques to provide online programmer's warnings for potentially problematic code lines that may need a second look at (to remove possible bugs), and biofeedback-driven software testing to optimize testing effort, focusing the tests on code areas with higher bug probability.</description>
		</publications>
	
		<publications id="0003">
			<title>Neural correlates of visual integration in Williams syndrome: Gamma oscillation patterns in a model of impaired coherence</title>
			<date>2013</date>
			<citation>18</citation>
			<co_authors>
				<author>Ines Bernardino</author>
				<author>Reza Farivar</author>
				<author>Eduardo D Silva</author>
				<author>Miguel Castelo-Brancoo</author>
			</co_authors>
			<publication_source>
				<journal>Neuropsychologia</journal>
				<volume>51</volume>
				<issue>7</issue>
				<pag_min>1287</pag_min>
				<pag_max>1295</pag_max>
			</publication_source>
			<publisher>Pergamon</publisher>
			<description>Williams syndrome (WS) is a clinical model of dorsal stream vulnerability and impaired visual integration. However, little is still known about the neurophysiological correlates of perceptual integration in this condition. We have used a 3D structure-from-motion (SFM) integrative task to characterize the neuronal underpinnings of 3D perception in WS and to probe whether gamma oscillatory patterns reflect changed holistic perception. Coherent faces were parametrically modulated in 3D depth (three different depth levels) to vary levels of stimulus ambiguity. We have found that the electrophysiological (EEG/ERP) correlates of such holistic percepts were distinct across groups. Independent component analysis demonstrated the presence of a novel component with a late positivity around 200 ms that was absent in controls. Source localization analysis of ERP signals showed a posterior occipital shift in WS and reduced parietal dorsal stream sources. Interestingly, low gamma-band oscillations (20–40 Hz) induced by this 3D perceptual integration task were significantly stronger and sustained during the stimulus presentation in WS whereas high gamma-band oscillations (60–90 Hz) were reduced in this clinical model of impaired visual coherence, as compared to controls. These observations suggest that dorsal stream processing of 3D SFM stimuli has distinct neural correlates in WS and different cognitive strategies are employed by these patients to reach visual coherence. Importantly, we found evidence for the presence of different sub-bands (20–40 Hz/60–90 Hz) within the gamma range which can be dissociated concerning the respective role on the coherent percept formation, both in typical and atypical development.</description>
		</publications>
		
		<publications id="0004">
			<title>WAP: Understanding the brain at software debugging</title>
			<date>2016</date>
			<citation>47</citation>
			<co_authors>
				<author>Joao Duraes</author>
				<author>Henrique Madeira</author>
				<author>C Duarte</author>
				<author>M Castelo Branco</author>
			</co_authors>
			<publication_source>
				<conference>2016 IEEE 27th International Symposium on Software Reliability Engineering (ISSRE)</conference>
				<pag_min>87</pag_min>
				<pag_max>92</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>We propose that understanding functional patterns of activity in mapped brain regions associated with code comprehension tasks and, more specifically, to the activity of finding bugs in traditional code inspections could reveal useful insights to improve software reliability and to improve the software development process in general. This includes helping to select the best professionals for the debugging effort, improving the conditions for code inspections, and identify new directions to follow for training code reviewers. This paper presents an interdisciplinary study to analyze the brain activity during code inspection tasks using functional magnetic resonance imaging (fMRI), which is a well-established tool in cognitive neuroscience research. We used several programs where realistic bugs representing the most frequent types of software faults found in the field were injected. The code inspectors involved in the research include programmers with different levels of expertise and experience in real code reviews. The goal is to understand brain activity patterns associated with code comprehension tasks and, more specifically, the brain activity when the code reviewer identifies a bug in the code ('eureka' moment), which can be a true positive or a false positive. Our results confirmed that brain areas associated with language processing and mathematics are highly active during code reviewing and shows that there are specific brain activity patterns that can be related to the decision-making moment of suspicion/bug detection. Importantly, the activity at the anterior insula region that we find to play a relevant role in the process of identifying software bugs is positively correlated to the precision of bug detection by the inspectors. This finding provides a new perspective on the role of this region on error awareness and monitoring and of its potential predictive value in predicting the quality of bug removing.</description>
		</publications>
	
	
		<publications id="0005">
			<title>Methylglyoxal-induced glycation changes adipose tissue vascular architecture, flow and expansion, leading to insulin resistance</title>
			<date>2017</date>
			<citation>46</citation>
			<co_authors>
				<author>Tiago Rodrigues</author>
				<author>Paulo Matafome</author>
				<author>Jose Sereno</author>
				<author>Jose Almeida</author>
				<author>Luis Gamas</author>
				<author>Christian Neves</author>
				<author>Sonia Gonçalves</author>
				<author>Catarina Carvalho</author>
				<author>Amina Arslanagic</author>
				<author>Elinor Wilcken</author>
				<author>Rita Fonseca</author>
				<author>Ilda Simoes</author>
				<author>Silvia Vilares Conde</author>
				<author>Miguel Castelo-Branco</author>
				<author>Raquel Seiça</author>
			</co_authors>
			<publication_source>
				<journal>Scientific reports</journal>
				<volume>7</volume>
				<issue>1</issue>
				<pag_min>1698</pag_min>
				<pag_max>1698</pag_max>
			</publication_source>
			<publisher>Nature Publishing Group UK</publisher>
			<description>Microvascular dysfunction has been suggested to trigger adipose tissue dysfunction in obesity. This study investigates the hypothesis that glycation impairs microvascular architecture and expandability with an impact on insulin signalling. Animal models supplemented with methylglyoxal (MG), maintained with a high-fat diet (HFD) or both (HFDMG) were studied for periepididymal adipose (pEAT) tissue hypoxia and local and systemic insulin resistance. Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) was used to quantify blood flow in vivo, showing MG-induced reduction of pEAT blood flow. Increased adipocyte size and leptin secretion were observed only in rats feeding the high-fat diet, without the development of hypoxia. In turn, hypoxia was only observed when MG was combined (HFDMG group), being associated with impaired activation of the insulin receptor (Tyr1163), glucose intolerance and systemic and muscle insulin resistance. Accordingly, the adipose tissue angiogenic assay has shown decreased capillarization after dose-dependent MG exposure and glyoxalase-1 inhibition. Thus, glycation impairs adipose tissue capillarization and blood flow, hampering its expandability during a high-fat diet challenge and leading to hypoxia and insulin resistance. Such events have systemic repercussions in glucose metabolism and may lead to the onset of unhealthy obesity and progression to type 2 diabetes.</description>
		</publications>
	
		<publications id="0006">
			<title>A longitudinal multimodal in vivo molecular imaging study of the 3xTg-AD mouse model shows progressive early hippocampal and taurine loss</title>
			<date>2019</date>
			<citation>42</citation>
			<co_authors>
				<author>Samuel Chiquita</author>
				<author>Mario Ribeiro</author>
				<author>Francisco Oliveira</author>
				<author>Jose Sereno</author>
				<author>Marta Batista</author>
				<author>Antero Abrunhosa</author>
				<author>Ana C Rodrigues-Neves</author>
				<author>Rafael Carecho</author>
				<author>Filipa Baptista</author>
				<author>Catarina Gomes</author>
				<author> Paula I Moreira</author>
				<author>Antonio F Ambrosio</author>
				<author>Miguel Castelo-Branco</author>
			</co_authors>
			<publication_source>
				<journal>Human molecular genetics</journal>
				<volume>28</volume>
				<issue>13</issue>
				<pag_min>2174</pag_min>
				<pag_max>2188</pag_max>
			</publication_source>
			<publisher>Oxford University Press</publisher>
			<description>The understanding of the natural history of Alzheimer's disease (AD) and temporal trajectories of in vivo molecular mechanisms requires longitudinal approaches. A behavioral and multimodal imaging study was performed at 4/8/12 and 16 months of age in a triple transgenic mouse model of AD (3xTg-AD). Behavioral assessment included the open field and novel object recognition tests. Molecular characterization evaluated hippocampal levels of amyloid β (Aβ) and hyperphosphorylated tau. Magnetic resonance imaging (MRI) included assessment of hippocampal structural integrity, blood-brain barrier (BBB) permeability and neurospectroscopy to determine levels of the endogenous neuroprotector taurine. Longitudinal brain amyloid accumulation was assessed using 11C Pittsburgh compound B positron emission tomography (PET), and neuroinflammation/microglia activation was investigated using 11C-PK1195. We found altered locomotor activity at months 4/8 and 16 months and recognition memory impairment at all time points. Substantial early reduction of hippocampal volume started at month 4 and progressed over 8/12 and 16 months. Hippocampal taurine levels were significantly decreased in the hippocampus at months 4/8 and 16. No differences were found for amyloid and neuroinflammation with PET, and BBB was disrupted only at month 16. In summary, 3xTg-AD mice showed exploratory and recognition memory impairments, early hippocampal structural loss, increased Aβ and hyperphosphorylated tau and decreased levels of taurine. In sum, the 3xTg-AD animal model mimics pathological and neurobehavioral features of AD, with early-onset recognition memory loss and MRI-documented hippocampal damage. The early-onset profile suggests temporal windows and opportunities for therapeutic intervention, targeting endogenous neuroprotectors such as taurine.</description>
		</publications>
	
		<publications id="0007">
			<title>Functional parcellation of the operculo-insular cortex in perceptual decision making: An fMRI study</title>
			<date>2012</date>
			<citation>26</citation>
			<co_authors>
				<author>Jose Rebola</author>
				<author>Carlos Ferreira</author>
				<author>Miguel Castelo-Branco</author>
			</co_authors>
			<publication_source>
				<journal>Neuropsychologia</journal>
				<volume>50</volume>
				<issue>14</issue>
				<pag_min>3693</pag_min>
				<pag_max>3701</pag_max>
			</publication_source>
			<publisher>Pergamon</publisher>
			<description>A current challenge in cognitive neuroscience is to provide an explicit separation of the neural correlates of abstract global decision variables from sensory and integrative ones. In particular, the insular cortex and the adjacent frontal operculum seem to have a crucial but still unclear role in evidence accumulation and decision signaling in perceptual decision-making tasks. Here, we have used a visual decision-making paradigm based on the detection of ambiguous two-tone (Mooney) face stimuli to assess the emergence of holistic percepts. These are constructed using global gestalt rules and not by gradual spatiotemporal increases in sensory evidence. Our paradigm (neurochronometric approach) enabled the experimental separation between multiple cognitive components in perceptual decision validated by both model-driven and data-driven analysis. This strategy allowed for the functional dissection of operculo-insular networks into task related complexes such as anterior (accumulator), middle (decision) and posterior (somatosensory/sensorimotor). We conclude that global perceptual integration based on holistic rules requires a distributed operculo-insular network.</description>
		</publications>
	</researchers>
	
	
	
	
	<researchers id="0013">
		<name>Joao Gabriel Silva</name>
		<area>Electrotechnical Engineering</area>
		<area>Informatics Engineering</area>
		<affiliation>University of Coimbra</affiliation> 
		<research_interests> 
			<interest>Dependable Computing</interest>
		</research_interests>
		<email>jgabrielipn.p</email>
		<number_articles>171</number_articles>
		
		<publications id="0001">
			<title>RIFLE: A general purpose pin-level fault injector</title>
			<date>1994</date>
			<citation>242</citation>
			<co_authors>
				<author>Henrique Madeira</author>
				<author>Mario Rela</author>
				<author>Francisco Moreira</author>
			</co_authors>
			<publication_source>
				<conference>Dependable Computing—EDCC-1: First European Dependable Computing Conference Berlin, Germany, October 4–6, 1994 Proceedings 1</conference>
				<pag_min>197</pag_min>
				<pag_max>216</pag_max>
			</publication_source>
			<publisher>Springer Berlin Heidelberg</publisher>
			<description>This paper discusses the problems of pin-level fault injection for dependability validation and presents the architecture of a pin-level fault injector called RIFLE. This system can be adapted to a wide range of target systems and the faults are mainly injected in the processor pins. The injection of the faults is deterministic and can be reproduced if needed. Faults of different nature can be injected and the fault injector is able to detect whether the injected fault has produced an error or not without the requirement of feedback circuits. RIFLE can also detect specific circumstances in which the injected faults do not affect the target system. Sets of faults with specific impact on the target system can be generated. The paper also presents fault injection results showing the coverage and latency achieved with a set of simple behavior based error detection mechanisms. It is shown that up to 72,5% of the errors can be detected with fairly simple mechanisms. Furthermore, for over 90% of the faults the target system has behaved according to the fail-silent model, which suggests that a traditional computer equipped with simple error detection mechanisms is relatively close to a fail-silent computer.</description>
		</publications>
	
		<publications id="0002">
			<title>Global checkpointing for distributed programs</title>
			<date>1992</date>
			<citation>218</citation>
			<co_authors>
				<author>LME Silva</author>
			</co_authors>
			<publication_source>
				<conference></conference>
				<pag_min>155</pag_min>
				<pag_max>162</pag_max>
			</publication_source>
			<publisher>IEEE Computer Society</publisher>
			<description>A novel algorithm for checkpointing and rollback recovery in distributed systems is presented. Processes belonging to the same program must take periodically a nonblocking coordinated global checkpoint, but only a minimum overhead is imposed during normal computation. Messages can be delivered out of order, and the processes are not required to be deterministic. The nonblocking structure is an important characteristic for avoiding laying a heavy burden on the application programs. The method also includes the damage assessment phase, unlike previous schemes that either assume that an error is detected immediately after it occurs (fail-stop) or simply ignore the damage caused by imperfect detection mechanisms. A possible way to evaluate the error detection latency, which enables one to assess the damage made and avoid the propagation of errors, is presented.</description>
		</publications>
		
		<publications id="0003">
			<title>Fault-tolerant execution of mobile agents</title>
			<date>2000</date>
			<citation>125</citation>
			<co_authors>
				<author>Luis Moura Silva</author>
				<author>Victor Batista</author>
			</co_authors>
			<publication_source>
				<conference></conference>
				<pag_min>135</pag_min>
				<pag_max>143</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>In this paper, we will address the list of problems that have to be solved in mobile agent systems and we will present a set of fault-tolerance techniques that can increase the robustness of agent-based applications without introducing a high performance overhead. The framework includes a set of schemes for failure detection, checkpointing and restart, software rejuvenation, a resource-aware atomic migration protocol, a reconfigurable itinerary, a protocol that avoids agents to get caught in node failures and a simple scheme to deal with network partitions. At the end, we will present some performance results that show the effectiveness of these fault-tolerance techniques.</description>
		</publications>
		
		<publications id="0004">
			<title>Fault injection spot-checks computer system dependability</title>
			<date>1999</date>
			<citation>100</citation>
			<co_authors>
				<author>Joao Viegas Carreira</author>
				<author>Diamantino Costa</author>
			</co_authors>
			<publication_source>
				<journal>IEEE Spectrum</journal>
				<volume>36</volume>
				<issue>8</issue>
				<pag_min>50</pag_min>
				<pag_max>55</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>Computer-based systems are expected to be more and more dependable. For that, they have to operate correctly even in the presence of faults, and this fault tolerance of theirs must be thoroughly tested by the injection of faults both real and artificial. Users should start to request reports from manufacturers on the outcomes of such experiments, and on the mechanisms built into systems to handle faults. To inject artificial physical faults, fault injection offers a reasonably mature option today, with Swift tools being preferred for most applications because of their flexibility and low cost. To inject software bugs, although some promising ideas are being researched, no established technique yet exists. In any case, establishing computer system dependability benchmarks would make tests much easier and enable comparison of results across different machines.</description>
		</publications>
		
		<publications id="0005">
			<title>Web‐based metacomputing with JET</title>
			<date>1997</date>
			<citation>43</citation>
			<co_authors>
				<author>Hernani Pedroso</author>
				<author> Luis M Silva</author>
			</co_authors>
			<publication_source>
				<journal>Concurrency: Practice and Experience</journal>
				<volume>9</volume>
				<issue>11</issue>
				<pag_min>1169</pag_min>
				<pag_max>1173</pag_max>
			</publication_source>
			<publisher>John Wiley and Sons, Ltd</publisher>
			<description>One of the most interesting challenges to the high-performance community is to exploit the existing computing resources for executing long-running number-crunching applications. Several important issues have to be addressed, like portability, robustness, security, heterogeneity, load-balancing and fault-tolerance. Java is an emerging language that is receiving an extraordinary enthusiasm and acceptance from several fields of programming. Interestingly, it presents some nice characteristics that partially solve some of those problems. This paper briefly describes JET, a parallel library implemented on Java that supports the execution of parallel applications over the Web. It is oriented to master/worker applications which present a coarse-grain task distribution. The library provides a high-level programming interface, support for fault-tolerance and some schemes to mask the latency of the communication. It can be used to execute massively distributed applications using several computers connected to the Internet.</description>
		</publications>
		
		<publications id="0006">
			<title>Software Aging and Rejuvenation in a SOAP-based Server</title>
			<date>2006</date>
			<citation>92</citation>
			<co_authors>
				<author>Luis Silva</author>
				<author>Henrique Madeira</author>
			</co_authors>
			<publication_source>
				<conference>Fifth IEEE International Symposium on Network Computing and Applications (NCA'06)</conference>
				<pag_min>55</pag_min>
				<pag_max>65</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>Web-services and service-oriented architectures are gaining momentum in the area of distributed systems and Internet applications. However, as we increase the abstraction level of the applications we are also increasing the complexity of the underlying middleware. In this paper, we present a dependability benchmarking study to evaluate and compare the robustness of some of the most popular SOAP-RPC implementations that are intensively used in the industry. The study was focused on Apache Axis where we have observed a high susceptibility of software aging. Building on these results we propose a new SLA-oriented software rejuvenation technique that proved to be a simple way to increase the dependability of the SOAP-server, the degree of self-healing and to maintain a sustained level of performance in the applications</description>
		</publications>
		
		<publications id="0007">
			<title>Providing applications with mobile agent technology</title>
			<date>2001</date>
			<citation>42</citation>
			<co_authors>
				<author>Paulo Marques</author>
				<author>Paulo Simoes</author>
				<author> Luis Silva</author>
				<author>Fernando Boavida</author>
			</co_authors>
			<publication_source>
				<conference>2001 IEEE Open Architectures and Network Programming Proceedings. OPENARCH 2001 (Cat. No. 01EX484)</conference>
				<pag_min>129</pag_min>
				<pag_max>136</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>Over the last couple of years we have been working on the development of mobile agents systems and their application to the areas of telecommunications and network management. This work has produced positive results: a competitive mobile agent platform was built, the run-time benefits of mobile agents were proved, and our industrial partners have developed practical applications that are being integrated into commercial products. However, despite the positive results, we feel that mobile agent technology is still not ready to enter the path of mainstream software development. In our perspective, one of the main reasons for this situation arises from the traditional approach to mobile agent technology. This approach, based on the familiar concept of the mobile-agent distributed platform as an extension of the operating system, focuses too much on the mobile agents and associated issues (mobility, agent lifecycle, security, coordination, etc.) and provides poor support for the development of applications where mobile agents are just one of several available technologies. Learning from past experience, we are now working on a new approach where the focus is brought back to the applications and mobile agents become just one the tools available to develop distributed systems. This provides a much lighter framework for application-based mobile agent systems. This paper presents the lessons learned from our previous project and discusses the new concept we are developing: application-centric mobile agent systems.</description>
		</publications>
	</researchers>
	
	
	
	
	
	<researchers id="0014">
		<name>Joao Carvalho</name>
		<area>Experimental Particle Physics</area>
		<affiliation>University of Coimbra</affiliation> 
		<research_interests> 
			<interest>Particle Physics</interest>
			<interest>Computational Biology</interest>
			<interest>data simulation and analysis</interest>
		</research_interests>
		<email> jcarlos@uc.pt</email>
		<number_articles>1539</number_articles>
	
		<publications id="0001">
			<title>Particle identification with HERA-B RICH</title>
			<date>2003</date>
			<citation>7</citation>
			<co_authors>
				<author>Marko Staric</author>
			</co_authors>
			<publication_source>
				<journal>Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment</journal>
				<volume>502</volume>
				<issue>1</issue>
				<pag_min>289</pag_min>
				<pag_max>293</pag_max>
			</publication_source>
			<publisher>North-Holland</publisher>
			<description>The particle identification capabilities of the HERA-B RICH are presented on the measured data from the latest run period.</description>
		</publications>
		
		<publications id="0002">
			<title>Evidence for light-by-light scattering in heavy-ion collisions with the ATLAS detector at the LHC</title>
			<date>2017</date>
			<citation>397</citation>
			<publication_source>
				<journal>Nature physics</journal>
				<volume>13</volume>
				<issue>9</issue>
				<pag_min>852</pag_min>
				<pag_max>858</pag_max>
			</publication_source>
			<publisher>Nature Publishing Group UK</publisher>
			<description>Light-by-light scattering (γγ → γγ) is a quantum-mechanical process that is forbidden in the classical theory of electrodynamics. This reaction is accessible at the Large Hadron Collider thanks to the large electromagnetic field strengths generated by ultra-relativistic colliding lead ions. Using 480 μb−1 of lead–lead collision data recorded at a centre-of-mass energy per nucleon pair of 5.02 TeV by the ATLAS detector, here we report evidence for light-by-light scattering. A total of 13 candidate events were observed with an expected background of 2.6 ± 0.7 events. After background subtraction and analysis corrections, the fiducial cross-section of the process Pb + Pb (γγ) → Pb(∗) + Pb(∗)γγ, for photon transverse energy ET > 3 GeV, photon absolute pseudorapidity |η|≤ 2.4, diphoton invariant mass greater than 6 GeV, diphoton transverse momentum lower than 2 GeV and diphoton acoplanarity below 0.01, is measured to be 70 ± 24 (stat.) ± 17 (syst.) nb, which is in agreement with the standard model predictions.</description>
		</publications>
		
		<publications id="0003">
			<title>Probing anomalous Wtb couplings in top pair decays</title>
			<date>2007</date>
			<citation>241</citation>
			<co_authors>
				<author>Juan Antonio Aguilar-Saavedra</author>
				<author> N Castro</author>
				<author>Antonio Onofre</author>
				<author>F Veloso</author>
			</co_authors>
			<publication_source>
				<journal>The European Physical Journal C</journal>
				<volume>50</volume>
				<pag_min>519</pag_min>
				<pag_max>533</pag_max>
			</publication_source>
			<publisher>Springer-Verlag</publisher>
			<description>We investigate several quantities, defined in the decays of top quark pairs, which can be used to explore non-standard Wtb interactions. Two new angular asymmetries are introduced in the leptonic decay of top (anti)quarks. Both are very sensitive to anomalous Wtb couplings, and their measurement allows for a precise determination of the W helicity fractions. We also examine other angular and energy asymmetries, the W helicity fractions and their ratios, as well as spin correlation asymmetries, analysing their dependence on anomalous Wtb couplings and identifing the quantities which are most sensitive to them. It is explicitly shown that spin correlation asymmetries are less sensitive to new interactions in the decay of the top quark; therefore, when combined with the measurement of other observables, they can be used to determine the t tbar spin correlation even in the presence of anomalous Wtb couplings. We finally discuss some asymmetries which can be used to test CP violation in t tbar production and complex phases in the effective Wtb vertex.</description>
		</publications>
		
		<publications id="0004">
			<title>Compilation of cross sections for proton–nucleus interactions at the HERA energy</title>
			<date>2003</date>
			<citation>37</citation>
			<publication_source>
				<journal>Nuclear Physics A</journal>
				<volume>725</volume>
				<pag_min>269</pag_min>
				<pag_max>275</pag_max>
			</publication_source>
			<publisher>North-Holland</publisher>
			<description>This work compiles information concerning the total, inelastic and diffractive cross sections for proton–nucleus interactions. The values are fitted together with the expression σ=σ0Aα, where A is the target atomic mass number. Also discussed is the amplitude of the diffractive components of the inelastic cross section.</description>
		</publications>
		
		<publications id="0005">
			<title>The neutral kaon decays to: a detailed analysis of the CPLEAR data</title>
			<date>1998</date>
			<citation>34</citation>
			<co_authors>
				<author>CPLEAR Collaboration</author>
				<author>A Angelopoulos</author>
			</co_authors>
			<publication_source>
				<journal>The European Physical Journal C-Particles and Fields</journal>
				<volume>5</volume>
				<issue>3</issue>
				<pag_min>389</pag_min>
				<pag_max>409</pag_max>
			</publication_source>
			<publisher>Springer-Verlag</publisher>
			<description>A detailed analysis of neutral kaons decaying to $\pi^{+}\pi^{-}\pi^{0}$ is presented based on the complete data set containing half a million events. Time-dependent decay rate asymmetries are measured between initially tagged ${\mathrm{{K}\mbox{}^0}}$ and ${\mathrm{\overline{K}\mbox{}^0}}$ and for different regions of the phase space. These asymmetries, resulting from the interference between the CP-conserving decay amplitude of ${\mathrm{K_L}}$ and the decay amplitude of ${\mathrm{K_S}}$ - either CP-violating or CP-conserving - allow the determination of the ${\mathrm{K_S}}$ parameters $\eta_{+-0}$ (CP-violating) and $\lambda$ (CP-conserving), and also of the main isospin components of the ${\mathrm{K_S}}$ decay amplitude. The branching ratio of ${\mathrm{K_S}}\rightarrow \pi^{+}\pi^{-}\pi^{0}$ (CP-conserving) is deduced directly from $\lambda$. In addition, we extract the slope parameters describing the energy dependence of the ${\mathrm{K_L}} \rightarrow \pi^{+}\pi^{-}\pi^{0}$ Dalitz plot. The whole set of our results fits well within the current phenomenological picture of the neutral-kaon system including CP violation and Chiral Perturbation Theory (ChPT).</description>
		</publications>
		
		<publications id="0006">
			<title>Search for new phenomena in the WW to l nu l'nu'final state in pp collisions at sqrt (s)= 7 TeV with the ATLAS detector</title>
			<date>2012</date>
			<citation>87</citation>
			<co_authors>
				<author>ATLAS collaboration</author>
			</co_authors>
			<publication_source>
				<journal>arXiv preprint arXiv:1208.2880</journal>
			</publication_source>
			<description>This Letter reports a search for a heavy particle that decays to WW using events produced in pp collisions at sqrt(s) = 7 TeV. The data were recorded in 2011 by the ATLAS detector and correspond to an integrated luminosity of 4.7 fb-1. WW to l nu l' nu'(l, l' = e or mu) final states are considered and the distribution of the transverse mass of the WW candidates is found to be consistent with Standard Model expectations. Upper limits on the production cross section times branching ratio into W boson pairs are set for Randall-Sundrum and bulk Randall-Sundrum gravitons, which result in observed 95% CL lower limits on the masses of the two particles of 1.23 TeV and 0.84 TeV, respectively.</description>
		</publications>
		
		<publications id="0007">
			<title>Two pion Bose-Einstein correlations in pp annihilations at rest</title>
			<date>1993</date>
			<citation>18</citation>
			<co_authors>
				<author>K Sarigiannis</author>
			</co_authors>
			<publication_source>
				<journal>NUCLEAR PHYSICS A</journal>
				<volume>558</volume>
				<issue>1</issue>
				<pag_min>43</pag_min>
				<pag_max>43</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>Bose-Einstein (BE) correlations between like-sign charged pions were studied in pp annihilations at rest into four-prong events, using data taken with the CPLEAR detector at LEAR (CERN). A strong enhancement was found in the production of pairs of like-sign pions of similar momenta, with respect to the pairs of unlike-sign pions. The observed BE- enhancement was used to extract the values for the strength λ of the effect and the radius r of the pion emitting source. The extracted value of λ >l is of relevant importance and clearly does not depend on the assumed parametrization of the correlation function. The influence of the normalization and fitting procedure, the detector resolution, the resonances production and decay and the neutral-pion multiplicity cuts, on the size of the pion source and the strength of the effect was investigated.</description>
		</publications>
	</researchers>
	
	
	
	
	
	<researchers id="0015">
		<name>Jose Carlos Ferreira Maia Neves</name>
		<area>Computer Science</area>
		<affiliation>University of Minho</affiliation> 
		<research_interests> 
			<interest>Artificial Intelligence</interest>
		</research_interests>
		<email> jneves@di.uminho.pt</email>
		<number_articles>1053</number_articles>
	
		<publications id="0001">
			<title>The fully informed particle swarm: simpler, maybe better</title>
			<date>2004</date>
			<citation>2197</citation>
			<co_authors>
				<author>Rui Mendes</author>
				<author>James Kennedy,</author>
			</co_authors>
			<publication_source>
				<journal>IEEE transactions on evolutionary computation</journal>
				<volume>8</volume>
				<issue>3</issue>
				<pag_min>204</pag_min>
				<pag_max>210</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>The canonical particle swarm algorithm is a new approach to optimization, drawing inspiration from group behavior and the establishment of social norms. It is gaining popularity, especially because of the speed of convergence and the fact that it is easy to use. However, we feel that each individual is not simply influenced by the best performer among his neighbors. We, thus, decided to make the individuals "fully informed." The results are very promising, as informed individuals seem to find better solutions in all the benchmark functions.</description>
		</publications>
	
		<publications id="0002">
			<title>Particle swarms for feedforward neural network training</title>
			<date>2002</date>
			<citation>413</citation>
			<co_authors>
				<author>Rui Mendes</author>
				<author>Paulo Cortez</author>
				<author>Miguel Rocha</author>
			</co_authors>
			<publication_source>
				<conference>Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN'02 (Cat. No. 02CH37290)</conference>
				<volume>2</volume>
				<pag_min>1895</pag_min>
				<pag_max>1899</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>Particle swarm is an optimization paradigm for real-valued functions, based on the social dynamics of group interaction. We propose its application to the training of neural networks. Comparative tests were carried out, for classification and regression tasks.</description>
		</publications>
	
		<publications id="0003">
			<title>Watch thy neighbor or how the swarm can learn from its environment</title>
			<date>2003</date>
			<citation>213</citation>
			<co_authors>
				<author>Rui Mendes</author>
				<author>James Kennedy</author>
			</co_authors>
			<publication_source>
				<conference>Proceedings of the 2003 IEEE Swarm Intelligence Symposium. SIS'03 (Cat. No. 03EX706)</conference>
				<pag_min>88</pag_min>
				<pag_max>94</pag_max>
			</publication_source>
			<publisher>IEEE</publisher>
			<description>Particle swarm optimization is a novel algorithm where a population of candidate problem solution vectors evolves "social" norms by being influenced by their topological neighbors. Until now, an individual was influenced by its best performance acquired in the past and the best experience observed in its neighborhood. In this paper, we introduce new ways an individual can be influenced by its neighbors.</description>
		</publications>
	
		<publications id="0004">
			<title>Evolving time series forecasting ARMA models</title>
			<date>2004</date>
			<citation>114</citation>
			<co_authors>
				<author>Miguel Rocha</author>
				<author>Paulo Cortez</author>
			</co_authors>
			<publication_source>
				<journal>Journal of Heuristics</journal>
				<volume>10</volume>
				<pag_min>415</pag_min>
				<pag_max>429</pag_max>
			</publication_source>
			<publisher>Kluwer Academic Publishers</publisher>
			<description>Time Series Forecasting (TSF) allows the modeling of complex systems as “black-boxes”, being a focus of attention in several research arenas such as Operational Research, Statistics or Computer Science. Alternative TSF approaches emerged from the Artificial Intelligence arena, where optimization algorithms inspired on natural selection processes, such as Evolutionary Algorithms (EAs), are popular. The present work reports on a two-level architecture, where a (meta-level) binary EA will search for the best ARMA model, being the parameters optimized by a (low-level) EA, which encodes real values. The handicap of this approach is compared with conventional forecasting methods, being competitive.</description>
		</publications>
	
		<publications id="0005">
			<title>Diabetic foot infections: current diagnosis and treatment</title>
			<date>2012</date>
			<citation>112</citation>
			<co_authors>
				<author>JJ Mendes</author>
			</co_authors>
			<publication_source>
				<journal>Journal of Diabetic Foot Complications</journal>
				<pag_min>26</pag_min>
				<pag_max>45</pag_max>
			</publication_source>
			<publisher>Journal of Diabetic Foot Complications</publisher>
			<description>A prior DFU is an almost obligatory prerequisite for DFIs. This is true even though, in some cases, the wound may have closed over before DFI presentation 9. Numerous observational studies have indicated that DFUs have a multifactorial nature. It is well established that insulin deficiency (absolute or relative) is the basis of the biochemical abnormalities that lead to the organic complications of diabetes mellitus 12 (namely, neuropathy) and the biological deficits of tissue healing and regeneration. It has also been established that perfect and persistent glycemic control, with either insulin or oral agents, stop 13 and probably regress 14 these complications. DFUs result from a complex interaction of two major risk factors: neuropathy and peripheral vascular disease. Neuropathy, both symmetric and bilateral, plays the main role with varying degrees of alterations in autonomic, sensory, and motor functions. Playing a secondary role is peripheral vascular disease resulting from atherosclerosis (Figure 1). Approximately 50 to 60% of all DFUs can be classified as neuropathic. Signs or symptoms of vascular compromise are observed in 40 to 50% of all patients with the vast majority having neuroischemic ulcers, and only a minority of patients have purely ischemic ulcers 15. Diabetic foot infection (DFI) treatment accounts for up to one-quarter of all diabetic admissions in both Europe and the United States making it the single most common reason for DM-related hospital admission 6. In the longer term, costs are even higher as DFUs have recurrence rates of up to 70% in centers of excellence, resulting in repeated interventions and progressive disability7.</description>
		</publications>
	
		<publications id="0006">
			<title>Quality of service in healthcare units</title>
			<date>2010</date>
			<citation>97</citation>
			<co_authors>
				<author>Jose Machado</author>
				<author>Antonio Abelha</author>
				<author>Paulo Novais</author>
				<author>Joao Neves</author>
			</co_authors>
			<publication_source>
				<journal>International Journal of Computer Aided Engineering and Technology</journal>
				<volume>2</volume>
				<issue>4</issue>
				<pag_min>436</pag_min>
				<pag_max>449</pag_max>
			</publication_source>
			<publisher>Inderscience Publishers</publisher>
			<description>Healthcare systems have to be understood in terms of a wide variety of heterogeneous, distributed and ubiquitous systems, speaking different languages, integrating medical equipment and being customised by different entities, which in turn were set by people living in different contexts and aiming at different goals. Therefore, architecture has been envisaged to support the medical applications in terms of an agency for integration, diffusion and archiving of medical information and the electronic medical record, a form of a web spider of intelligent information processing system, its major subsystems, their functional roles and the flow of information and control among them, with adjustable autonomy. With such web-based simulated systems, quality of service will be improved (e.g., the available knowledge may be used for educational and training purposes).</description>
		</publications>
	
		<publications id="0007">
			<title>Ambient assisted living</title>
			<date>2009</date>
			<citation>89</citation>
			<co_authors>
				<author>Ricardo Costa</author>
				<author>Davide Carneiro</author>
				<author>Paulo Novais</author>
				<author>Luis Lima</author>
				<author>Jose Machado</author>
				<author>Alberto Marques</author>	
			</co_authors>
			<publication_source>
				<conference>3rd Symposium of Ubiquitous Computing and Ambient Intelligence 2008</conference>
				<pag_min>86</pag_min>
				<pag_max>94</pag_max>
			</publication_source>
			<publisher>Springer Berlin Heidelberg</publisher>
			<description>The quality of care practice is difficult to judge. Indeed, support and care provision is very personal, i.e., assessments are individual and lead to specific care packages, involving social services, health workers, care agencies. We expect privacy in our own affairs and confidentially from those to whom we disclose them. Therefore, we are in an urgent need for new, technological and formal approaches to problem solving, as the increase of population with special care requirements. Following this line of thought, it is one’s goal to present the VirtualECare framework, an intelligent multi-agent system able to monitor, interact and serve its customers, which are in need of care services, based in open standards, expecting not only to fulfil the objectives referred to above, but also to overcome the problems induced by the use of new technologies and formalisms.</description>
		</publications>
	</researchers>
	
	
	
	
	
	<researchers id="0016">
		<name>Artur Cavaco-Paulo</name>
		<area>textile engineering</area>
		<area>textile chemistry branch</area>
		<affiliation>Univesity of Minho</affiliation> 
		<research_interests> 
			<interest>Biotecnology</interest>
			<interest>Nanotecnology</interest>
		</research_interests>
		<email>artur@deb.uminho.pt</email>
		<number_articles>691</number_articles>
	
		<publications id="0001">
			<title>Bio-coloration of bacterial cellulose assisted by immobilized laccase</title>
			<date>2018</date>
			<citation>25</citation>
			<co_authors>
				<author>Ji Eun Song</author>
				<author>Jing Su</author>
				<author>Jennifer Noro</author>
				<author>Carla Silva</author>
				<author>Hye Rim Kim</author>
			</co_authors>
			<publication_source>
				<journal>Amb express</journal>
				<volume>8</volume>
				<issue>1</issue>
				<pag_min>1</pag_min>
				<pag_max>11</pag_max>
			</publication_source>
			<publisher>SpringerOpen</publisher>
			<description>In this work a process for the bio-coloration of bacterial cellulose (BC) membranes was developed. Laccase from Myceliophthora thermophila was immobilized onto BC membranes and retained up to 88% of residual activity after immobilization. Four compounds belonging to the flavonoids family were chosen to test the in situ polymerase activity of immobilized laccase. All the flavonoids were successfully polymerized by laccase giving rise to yellow, orange and dark brown oligomers which conferred color to the BC support. The optimal bio-coloration conditions were studied for two of the tested flavonoids, catechol and catechin, by varying the concentration and time of incubation. High color depth and resistance to washing were obtained for both compounds. The highly porous bacterial cellulose material demonstrated great performance as a bio-coloration support, in contrast to other materials cited in literature, like cotton or wool. The process developed is presented as an environmentally friendly alternative for bacterial cellulose bio-coloration and will contribute deeply for the development of new fashionable products within this material.</description>
		</publications>
		
		<publications id="0002">
			<title>Practical insights on enzyme stabilization</title>
			<date>2018</date>
			<citation>182</citation>
			<co_authors>
				<author>Carla Silva</author>
				<author>Madalena Martins</author>
				<author>Su Jing</author>
				<author>Jiajia Fu</author>
			</co_authors>
			<publication_source>
				<source>Critical reviews in biotechnology</source>
				<volume>38</volume>
				<issue>3</issue>
				<pag_min>335</pag_min>
				<pag_max>350</pag_max>
			</publication_source>
			<publisher>Taylor Francis</publisher>
			<description>Enzymes are efficient catalysts designed by nature to work in physiological environments of living systems. The best operational conditions to access and convert substrates at the industrial level are different from nature and normally extreme. Strategies to isolate enzymes from extremophiles can redefine new operational conditions, however not always solving all industrial requirements. The stability of enzymes is therefore a key issue on the implementation of the catalysts in industrial processes which require the use of extreme environments that can undergo enzyme instability. Strategies for enzyme stabilization have been exhaustively reviewed, however they lack a practical approach. This review intends to compile and describe the most used approaches for enzyme stabilization highlighting case studies in a practical point of view.</description>
		</publications>
		
		<publications id="0003">
			<title>Albumin-based nanodevices as drug carriers</title>
			<date>2016</date>
			<citation>177</citation>
			<co_authors>
				<author>Ana Loureiro</author>
				<author>Nuno G Azoia</author>
				<author>Andreia C Gomes</author>
			</co_authors>
			<publication_source>
				<journal>Current pharmaceutical design</journal>
				<volume>22</volume>
				<issue>10</issue>
				<pag_min>1371</pag_min>
				<pag_max>1390</pag_max>
			</publication_source>	
			<publisher>Bentham Science Publishers</publisher>
			<description>Nanomedicine, the application of nanotechnology to medicine, is being increasingly used to improve and exploit the advantages of efficient drug delivery. Different nanodevices have been developed in recent years, among them protein-based nanoparticles which have gained considerable interest. Albumin is a versatile protein carrier with several characteristics that make it an ideal candidate for drug delivery, such as its availability, its biocompatibility, its biodegradability, and its lack of toxicity and immunogenicity. This review embodies an overview of different methods available for production of albumin-based nanoparticles, with focus on high-energy emulsification methods. A comparison between production by using sonication, which involves acoustic cavitation, and the high pressure homogenization method, where occurs hydrodynamic cavitation, is presented. Taking into account important properties of nanoparticles required for intravenous administration, the use of poloxamers, tri-block copolymer surfactants is discussed as it improves blood circulation time and bioavailability of nanoparticles. Thus, nanoparticles can be engineered to provide adequate features to therapeutic applications, in which can be included surface functionalization with targeting agents. Different albumin-based formulations and their therapeutic applications are presented in this review, with emphasis on applications in cancer therapy, where albumin-based strategies are promising for targeted drug delivery in innovative clinical strategies.</description>
		</publications>
		
		<publications id="0004">
			<title>Novel silk fibroin/elastin wound dressings</title>
			<date>2012</date>
			<citation>248</citation>
			<co_authors>
				<author>Andreia Vasconcelos</author>
				<author>Andreia C Gomes</author>
			</co_authors>
			<publication_source>
				<journal>Acta biomaterialia</journal>
				<volume>8</volume>
				<issue>8</issue>
				<pag_min>3049</pag_min>
				<pag_max>3060</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>Silk fibroin (SF) and elastin (EL) scaffolds were successfully produced for the first time for the treatment of burn wounds. The self-assembly properties of SF, together with the excellent chemical and mechanical stability and biocompatibility, were combined with elastin protein to produce scaffolds with the ability to mimic the extracellular matrix (ECM). Porous scaffolds were obtained by lyophilization and were further crosslinked with genipin (GE). Genipin crosslinking induces the conformational transition from random coil to β-sheet of SF chains, yielding scaffolds with smaller pore size and reduced swelling ratios, degradation and release rates. All results indicated that the composition of the scaffolds had a significant effect on their physical properties, and that can easily be tuned to obtain scaffolds suitable for biological applications. Wound healing was assessed through the use of human full-thickness skin equivalents (EpidermFT). Standardized burn wounds were induced by a cautery and the best re-epithelialization and the fastest wound closure was obtained in wounds treated with 50SF scaffolds; these contain the highest amount of elastin after 6 days of healing in comparison with other dressings and controls. The cytocompatibility demonstrated with human skin fibroblasts together with the healing improvement make these SF/EL scaffolds suitable for wound dressing applications.</description>
		</publications>
		
		<publications id="0005">
			<title>Hydrogen peroxide generation with immobilized glucose oxidase for textile bleaching</title>
			<date>2002</date>
			<citation>188</citation>
			<co_authors>
				<author>Tzanko Tzanov</author>
				<author>Silgia A Costa</author>
				<author>Georg M Gübitz</author>
			</co_authors>
			<publication_source>
				<journal>Journal of Biotechnology</journal>
				<volume>93</volume>
				<issue>1</issue>
				<pag_min>87</pag_min>
				<pag_max>94</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>Glucose oxidase was covalently immobilized on commercially available alumina and glass supports, with a high level of protein recovery. The operational stability of the alumina carrier was an advantage over the glass support, though the rate of generation of hydrogen peroxide in the case of the latter was higher. The immobilization technique provided repeated application of the enzyme even in low concentration, and the hydrogen peroxide generated in the enzymatic reaction was successively used for textile bleaching.</description>
		</publications>
		
		<publications id="0006">
			<title>Immobilized laccase for decolourization of Reactive Black 5 dyeing effluent</title>
			<date>2003</date>
			<citation>182</citation>
			<co_authors>
				<author>Andrea Zille</author>
				<author>Tzanko Tzanov</author>
				<author>Georg M Gübitz</author>
			</co_authors>
			<publication_source>
				<journal>Biotechnology Letters</journal>
				<volume>25</volume>
				<pag_min>1473</pag_min>
				<pag_max>1477</pag_max>
			</publication_source>
			<publisher>Kluwer Academic Publishers-Plenum Publishers</publisher>
			<description>Reactive Black 5 industrial dyeing effluent was decolourized by free and immobilized laccase. The stability of the enzyme (194 h free and 79 h immobilized) depended on the dyeing liquor composition and the chemical structure of the dye. In the decolourization experiments with immobilized laccase, two phenomenons were observed – decolourization due to adsorption on the support (79%) and dye degradation due to the enzyme action (4%). Dyeing in the enzymatically recycled effluent provided consistency of the colour with both bright and dark dyes.</description>
		</publications>
		
		<publications id="0007">
			<title>Design of liposomal formulations for cell targeting</title>
			<date>2015</date>
			<citation>164</citation>
			<co_authors>
				<author>Eugenia Nogueira</author>
				<author>Andreia C Gomes</author>
				<author>Ana Preto</author>
			</co_authors>
			<publication_source>
				<source>Colloids and surfaces B: Biointerfaces</source>
				<volume>136</volume>
				<pag_min>514</pag_min>
				<pag_max>526</pag_max>
				</publication_source>
			<publisher>Elsevier</publisher>
			<description>Liposomes have gained extensive attention as carriers for a wide range of drugs due to being both nontoxic and biodegradable as they are composed of substances naturally occurring in biological membranes. Active targeting for cells has explored specific modification of the liposome surface by functionalizing it with specific targeting ligands in order to increase accumulation and intracellular uptake into target cells. None of the Food and Drug Administration-licensed liposomes or lipid nanoparticles are coated with ligands or target moieties to delivery for homing drugs to target tissues, cells or subcellular organelles. Targeted therapies (with or without controlled drug release) are an emerging and relevant research area. Despite of the numerous liposomes reviews published in the last decades, this area is in constant development. Updates urgently needed to integrate new advances in targeted liposomes research. This review highlights the evolution of liposomes from passive to active targeting and challenges in the development of targeted liposomes for specific therapies.</description>
		</publications>
	</researchers>
	
	
	
	
	
	<researchers id="0017">
		<name>Manuela E. Gomes</name>
		<area>Materials Science and Engineering</area>
		<area>Tissue Engineering</area>
		<area>Hybrid Materials</area>
		<affiliation>University of Minho</affiliation> 
		<research_interests> 
			<interest>Tissue engineering</interest>
			<interest>Tendon tissue regeneration</interest>
			<interest>magnetic based tissue</interest>
			<interest>3D bioprinting</interest>
		</research_interests>
		<email>megomes@i3bs.uminho.pt</email>
		<number_articles>439</number_articles>
		
		<publications id="0001">
			<title>The potential of cellulose nanocrystals in tissue engineering strategies</title>
			<date>2014</date>
			<citation>482</citation>
			<co_authors>
				<author>Rui MA Domingues</author>
				<author>Rui L Reis</author>
			</co_authors>
			<publication_source>
				<source>Biomacromolecules</source>
				<volume>15</volume>
				<issue>7</issue>
				<pag_min>2327</pag_min>
				<pag_max>2346</pag_max>
			</publication_source>
			<publisher>American Chemical Society</publisher>
			<description>Cellulose nanocrystals (CNCs) are a renewable nanosized raw material that is drawing a tremendous level of attention from the materials community. These rod-shaped nanocrystals that can be produced from a variety of highly available and renewable cellulose-rich sources are endowed with exceptional physicochemical properties which have promoted their intensive exploration as building blocks for the design of a broad range of new materials in the past few decades. However, only recently have these nanosized substrates been considered for bioapplications following the knowledge on their low toxicity and ecotoxicological risk. This Review provides an overview on the recent developments on CNC-based functional biomaterials with potential for tissue engineering (TE) applications, focusing on nanocomposites obtained through different processing technologies usually employed in the fabrication of TE scaffolds into various formats, namely, dense films and membranes, hierarchical three-dimensional (3D) porous constructs (micro/nanofibers mats, foams and sponges), and hydrogels. Finally, while highlighting the major achievements and potential of the reviewed work on cellulose nanocrystals, alternative applications for some of the developed materials are provided, and topics for future research to extend the use of CNCs-based materials in the scope of the TE field are identified.</description>
		</publications>
		
		<publications id="0002">
			<title>Cytocompatibility and response of osteoblastic-like cells to starch-based polymers: effect of several additives and processing conditions</title>
			<date>2001</date>
			<citation>217</citation>
			<co_authors>
				<author>Rui L Reis</author>
				<author>Antonio M Cunha</author>
				<author>CA Blitterswijk</author>
				<author>JD De Bruijn</author>
			</co_authors>
			<publication_source>
				<journal>Biomaterials</journal>
				<volume>22</volume>
				<issue>13</issue>
				<pag_min>1911</pag_min>
				<pag_max>1917</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>This work reports on the biocompatibility evaluation of new biodegradable starch-based polymers that are under consideration for use in orthopaedic temporary applications and as tissue engineering scaffolds. It has been shown in previous works that by using these polymers it is both possible to produce polymer/hydroxyapatite (HA) composites (with or without the use of coupling agents) with mechanical properties matching those of the human bone, and to obtain 3D structures generated by solid blowing agents, that are suitable for tissue engineering applications. This study was focused on establishing the influence of several additives (ceramic fillers, blowing agents and coupling agents) and processing methods/conditions on the biocompatibility of the materials described above. The cytotoxicity of the materials was evaluated using cell culture methods, according to ISO/EN 109935 guidelines. A cell suspension of human osteosarcoma cells (HOS) was also seeded on a blend of corn starch with ethylene vinyl alcohol (SEVA-C) and on SEVA-C/HA composites, in order to have a preliminary indication on cell adhesion and proliferation on the materials surface. In general, the obtained results show that all the different materials based on SEVA-C, (which are being investigated for use in several biomedical applications), as well as all the additives (including the novel coupling agents) and different processing methods required to obtain the different properties/products, can be used without inducing a cytotoxic behaviour to the developed biomaterials.</description>
		</publications>
		
		<publications id="0003">
			<title>Cell delivery systems using alginate–carrageenan hydrogel beads and fibers for regenerative medicine applications</title>
			<date>2011</date>
			<citation>192</citation>
			<co_authors>
				<author>Elena G Popa</author>
				<author>Rui L Reis</author>
				<author></author>
			</co_authors>
			<publication_source>
				<journal>Biomacromolecules</journal>
				<volume>12</volume>
				<issue>11</issue>
				<pag_min>3952</pag_min>
				<pag_max>3961</pag_max>
			</publication_source>
			<publisher>American Chemical Society</publisher>
			<description>The present work was focused on the development and characterization of new hydrogel systems based on natural origin polymers, namely, alginate and carrageenan, into different formats and with adequate properties to sustain the viability of encapsulated cells, envisioning their application as cell delivery vehicles for tissue regeneration. Different formulations of alginate and carrageenan hydrogels and different processing parameters were considered to determine the best conditions required to achieve the most adequate response in terms of the mechanical stability, cell viability, and functionality of the developed systems. The morphology, size, and structure of the hydrogels and their degradation behavior and mechanical properties were evaluated during this study. In addition to cytotoxicity studies, preliminary experiments were carried out to investigate the ability of alginate–carrageenan beads/fibers to encapsulate chondrocytes. The results obtained indicated that the different formulations, both in the form of beads and fibers, have considerable potential as cell-carrier materials for cell delivery in tissue engineering/regenerative medicine applications.</description>
		</publications>
		
		<publications id="0004">
			<title>Injectable gellan gum hydrogels with autologous cells for the treatment of rabbit articular cartilage defects</title>
			<date>2010</date>
			<citation>148</citation>
			<co_authors>
				<author>Joao T Oliveira</author>
				<author>Leandro S Gardel</author>
				<author>Tommaso Rada</author>
				<author>Luis Martins</author>
				<author>Rui L Reis</author>
			</co_authors>
			<publication_source>
				<journal>Journal of Orthopaedic Research</journal>
				<volume>28</volume>
				<issue>9</issue>
				<pag_min>1193</pag_min>
				<pag_max>1199</pag_max>
			</publication_source>
			<publisher>Wiley Subscription Services, Inc., A Wiley Company</publisher>
			<description>In this work, the ability of gellan gum hydrogels coupled with autologous cells to regenerate rabbit full-thickness articular cartilage defects was tested. Five study groups were defined: (a) gellan gum with encapsulated chondrogenic predifferentiated rabbit adipose stem cells (ASC + GF); (b) gellan gum with encapsulated nonchondrogenic predifferentiated rabbit adipose stem cells (ASC); (c) gellan gum with encapsulated rabbit articular chondrocytes (AC) (standard control); (d) gellan gum alone (control); (e) empty defect (control). Full-thickness articular cartilage defects were created and the gellan gum constructs were injected and left for 8 weeks. The macroscopic aspect of the explants showed a progressive increase of similarity with the lateral native cartilage, stable integration at the defect site, more pronouncedly in the cell-loaded constructs. Tissue scoring showed that ASC + GF exhibited the best results regarding tissue quality progression. Alcian blue retrieved similar results with a better outcome for the cell-loaded constructs. Regarding real-time PCR analyses, ASC + GF had the best progression with an upregulation of collagen type II and aggrecan, and a downregulation of collagen type I. Gellan gum hydrogels combined with autologous cells constitute a promising approach for the treatment of articular cartilage defects, and adipose derived cells may constitute a valid alternative to currently used articular chondrocytes.</description>
		</publications>
		
		<publications id="0005">
			<title>Engineering tendon and ligament tissues: present developments towards successful clinical products</title>
			<date>2013</date>
			<citation>162</citation>
			<co_authors>
				<author>Marcia T Rodrigues</author>
				<author> Rui L Reis</author>
			</co_authors>
			<publication_source>
				<source>Journal of tissue engineering and regenerative medicine</source>
				<volume>7</volume>
				<issue>9</issue>
				<pag_min>673</pag_min>
				<pag_max>686</pag_max>
			</publication_source>
			<description>Musculoskeletal diseases are one of the leading causes of disability worldwide. Among them, tendon and ligament injuries represent an important aspect to consider in both athletes and active working people. Tendon and ligament damage is an important cause of joint instability, and progresses into early onset of osteoarthritis, pain, disability and eventually the need for joint replacement surgery. The social and economical burden associated with these medical conditions presents a compelling argument for greater understanding and expanding research on this issue. The particular physiology of tendons and ligaments (avascular, hypocellular and overall structural mechanical features) makes it difficult for currently available treatments to reach a complete and long-term functional repair of the damaged tissue, especially when complete tear occurs. Despite the effort, the treatment modalities for tendon and ligament are suboptimal, which have led to the development of alternative therapies, such as the delivery of growth factors, development of engineered scaffolds or the application of stem cells, which have been approached in this review.</description>
		</publications>
		
		<publications id="0006">
			<title>Tissue engineering and regenerative medicine: new trends and directions—a year in review</title>
			<date>2017</date>
			<citation>154</citation>
			<co_authors>
				<author>Marcia T Rodrigues</author>
				<author>Rui MA Domingues</author>
				<author>Rui L Reis</author>
			</co_authors>
			<publication_source>
				<source>Tissue Engineering Part B: Reviews</source>
				<volume>23</volume>
				<issue>3</issue>
				<pag_min>211</pag_min>
				<pag_max>224</pag_max>
			</publication_source>
			<publisher>Mary Ann Liebert, Inc.</publisher>
			<description>Tissue engineering (TE) is continuously evolving assimilating inputs from adjacent scientific areas and their technological advances, including nanotechnology developments that have been spawning the range of available options for the precise manipulation and control of cells and cellular environments. Simultaneously, with the maturation of the field, TE has a growing and marked impact in other fields, such as cancer and other diseases research, enabling tri-dimensional (3D) tumor/tissue models of increased complexity that more closely resemble living tissue dynamics, playing a decisive role in the development of new and improved therapies. Nevertheless, TE is still struggling with translational issues. On this matter, the advent of personalized and precision medicine has opened new perspectives, particularly with the striking evolutions enabled by 3D bioprinting technologies. Based on a modified methodology grounded in the past years' approach, we have identified and reviewed some of the most high-impact publications on the topics that are revolutionizing TE and helping to define the future directions of the field, namely: (1) New trends in TE: Personalized/precision regenerative medicine and 3D bioprinting, (2) Contributions of TE to other fields: microfabricated tissue-engineered 3D models for cancer and other diseases research, and (3) Diagnostic and theranostic tools: monitoring and real-time control of TE systems.</description>
		</publications>
		
		<publications id="0007">
			<title>Cell adhesion and proliferation onto chitosan-based membranes treated by plasma surface modification</title>
			<date>2011</date>
			<citation>103</citation>
			<co_authors>
				<author>Sandra M Luna</author>
				<author>Simone S Silva</author>
				<author>Joao F Mano</author>
				<author>Rui L Reis</author>
			</co_authors>
			<publication_source>
				<journal>Journal of biomaterials applications</journal>
				<volume>26</volume>
				<issue>1</issue>
				<pag_min>101</pag_min>
				<pag_max>116</pag_max>
			</publication_source>
			<publisher>SAGE Publications</publisher>
			<description>Surface properties play a vital role in the functioning of a biomaterial. Cellular adherence and growth onto biomaterials can be enhanced in biomaterial modifications of their surface. In this work, the cell behavior on chitosan membranes modified by argon and nitrogen-plasma treatments was investigated. Characterization of the membranes was performed using atomic force microscopy, contact angle measurements, and X-ray photoelectron spectroscopy. Cytotoxicity assessment and direct contact assay were carried out for untreated and treated chitosan membranes using L929 fibroblast-like cells. Cell morphology and cell viability were assessed to evaluate the cell attachment and proliferation. Changes in terms of roughness, surface chemistry, and hydrophilicity/hydrophobic balance of chitosan-modified membranes were observed. Regarding cell studies, the findings revealed that the extracts of all membranes do not induce cytotoxic effects. Moreover, the in vitro assays evidenced an improvement of the L929 adhesion and attachment when compared to untreated chitosan membranes. Overall, the data obtained clearly demonstrated that plasma treatments constitute an effective way of improving the biocompatibility of chitosan membranes towards to their use in biomedical applications.</description>
		</publications>
	</researchers>
	
	
	
	
	<researchers id="0018">
		<name>Paulo M. Fernandes</name>
		<area>Geological Sciences</area>
		<area>Stratigraphy</area>
		<area>Organic Petrology</area>
		<area>Palynology</area>
		<affiliation>University of Tras-os-Montes e Alto Douro</affiliation> 
		<research_interests> 
			<interest>Fire</interest>
			<interest>Wildland Fire</interest>
			<interest>Fire Behaviour</interest>
			<interest>Fire Ecology</interest>
			<interest>Fire Management</interest>
		</research_interests>
		<email>pfern@utad.pt</email>
		<number_articles>303</number_articles>
		
		<publications id="0001">
			<title>A review of prescribed burning effectiveness in fire hazard reduction</title>
			<date>2003</date>
			<citation>782</citation>
			<co_authors>
				<author>Herminio S Botelho</author>
			</co_authors>
			<publication_source>
				<journal>International Journal of Wildland Fire</journal>
				<volume>12</volume>
				<issue>2</issue>
				<pag_min>117</pag_min>
				<pag_max>128</pag_max>
			</publication_source>
			<publisher>CSIRO PUBLISHING</publisher>
			<description>Wildfire hazard abatement is one of the major reasons to use prescribed burning. Computer simulation, case studies, and analysis of the fire regime in the presence of active prescribed burning programs in forest and shrubland generally indicate that this fuel management tool facilitates fire suppression efforts by reducing the intensity, size and damage of wildfires. However, the conclusions that can be drawn from the above approaches are limited, highlighting the need for more properly designed experiments addressing this question. Fuel accumulation rate frequently limits prescribed fire effectiveness to a short post-treatment period (2–4 years). Optimisation of the spatial pattern of fire application is critical but has been poorly addressed by research, and practical management guidelines are lacking to initiate this. Furthermore, adequate treatment efforts in terms of fire protection are constrained by operational, social and ecological issues. The best results of prescribed fire application are likely to be attained in heterogeneous landscapes and in climates where the likelihood of extreme weather conditions is low. Conclusive statements concerning the hazard-reduction potential of prescribed fire are not easily generalised, and will ultimately depend on the overall efficiency of the entire fire management process.</description>
		</publications>
		
		<publications id="0002">
			<title>The fire ecology and management of maritime pine (Pinus pinaster Ait.)</title>
			<date>2007</date>
			<citation>312</citation>
			<co_authors>
				<author>Eric Rigolot</author>
			</co_authors>
			<publication_source>
				<journal>Forest Ecology and Management</journal>
				<volume>241</volume>
				<issue>1</issue>
				<pag_min>1</pag_min>
				<pag_max>13</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>Maritime pine (Pinus pinaster Ait.) is an important conifer from the western Mediterranean Basin. Fire is the most significant threat to maritime pine plantations but also a disturbance that plays a vital role in the perpetuation of natural stands. The species has physical characteristics that allow survival after low-intensity fire, namely thick bark, and reproduction processes that facilitate recovery after stand replacement fire from seeds stored in serotinous cones. These traits are consistent with the opposing strategies of fire resistance and fire evasion and can be interpreted as evolutionary adaptations to fire, but their development and coexistence are highly variable between populations, thus invalidating the classification of maritime pine in a general fire regime category. When the two strategies are concurrent the species should be able to persist under a variable or mixed fire regime. The quality, quantity and structural arrangement of fuels in maritime pine stands explain why they are so flammable. Thorough descriptions of the litter and understorey fuel complex are available in the literature, which makes custom fire behaviour prediction possible with software tools based on Rothermel's fire spread model; empirical fire behaviour models developed from experimental fire data are also available and are preferred to plan prescribed burning operations. There is ample evidence, although largely anecdotal, that surface, ladder and canopy fuel treatments mitigate wildfire intensity and burn severity and avoid crown fire in maritime pine stands. The optimization of fuel hazard management is nevertheless curtailed by the current state of knowledge about crown fire behaviour and fuel dynamics in relation to stand development and silviculture. The conservation and sustainable management of maritime pine in fire-prone landscapes should integrate the active use of fire and understand that effective protection from high-severity wildfire is not possible without sacrificing some stand volume.</description>
		</publications>
		
		<publications id="0003">
			<title>Fire resistance of European pines</title>
			<date>2008</date>
			<citation>266</citation>
			<co_authors>
				<author>Jose A Vega</author>
				<author>Enrique Jimenez</author>
				<author>Eric Rigolot</author>
			</co_authors>
			<publication_source>
				<journal>Forest Ecology and Management</journal>
				<volume>256</volume>
				<issue>3</issue>
				<pag_min>246</pag_min>
				<pag_max>255</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>Pine resistance to low- to moderate-intensity fire arises from traits (namely related to tissue insulation from heat) that enable tree survival. Predictive models of the likelihood of tree mortality after fire are quite valuable to assist decision-making after wildfire and to plan prescribed burning. Data and models pertaining to the survival of European pines following fire are reviewed. The type and quality of the current information on fire resistance of the various European species is quite variable. Data from low-intensity fire experiments or regimes is comparatively abundant for Pinus pinaster and Pinus sylvestris, while tree survival after wildfire has been modelled for Pinus pinea and Pinus halepensis. P. pinaster and P. pinea, and Pinus canariensis in special, are better equipped to survive fire, but low-intensity fire is tolerated even by species often referred to as fire-sensitive (P. halepensis and Pinus radiata). The relative fire resistance of European pine species is assessed on the basis of (i) morphological and experimental data, and (ii) mortality modelling that considers fire behaviour. Limitations of these approaches to rate fire resistance are discussed, and the current knowledge gaps are indicated.</description>
		</publications>
		
		<publications id="0004">
			<title>Fire spread prediction in shrub fuels in Portugal</title>
			<date>2001</date>
			<citation>262</citation>
			<publication_source>
				<journal>Forest Ecology and Management</journal>
				<volume>144</volume>
				<issue>1</issue>
				<pag_min>67</pag_min>
				<pag_max>74</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>Expertise and knowledge of forest fire behaviour provide a sound basis to fire management activities. This study examines the possibility of describing fire spread in shrubland by means of a simple empirical model. Rates of fire spread up to 20mmin−1 and the associated weather and fuel conditions were measured on a set of experimental and prescribed burns in four different shrub fuel types in Portugal. Shrubland fire spread in flat terrain could be accurately predicted in terms of wind speed, aerial dead fuel moisture content and vegetation height. However, it was not possible to identify individual effects of the fuel-complex descriptors on fire propagation. Preliminary fire spread models are presented but their use should be restricted to mild to moderate burning conditions until more extensive experimentation is carried out.</description>
		</publications>
		
		<publications id="0005">
			<title>Post-fire tree mortality in mixed forests of central Portugal</title>
			<date>2010</date>
			<citation>176</citation>
			<co_authors>
				<author>FX Catry</author>
				<author>F Rego</author>
				<author>F Moreira</author>
				<author>JG Pausas</author>
			</co_authors>
			<publication_source>
				<journal>Forest Ecology and Management</journal>
				<volume>260</volume>
				<issue>7</issue>
				<pag_min>1184</pag_min>
				<pag_max>1192</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>Wildfires are a recurrent disturbance in the Mediterranean Basin. However, managers from this region are confronted with a lack of information on the effects of fire on most woody species, which is required for defining sustainable forest management strategies. Following a large wildfire in central Portugal (2003), we surveyed the area during the first year and assessed the vegetative condition of 1040 burned trees from 11 different species. Among those trees, 755 individuals were selected and monitored annually for 4 years. At the end of the study, almost all the broadleaved trees survived, while most coniferous died. In spite of the low mortality observed in broadleaves, most were top-killed and regenerated only from basal resprouts, which implies a slow recovering process. Quercus suber, however, showed vigorous post-fire crown resprouting and was the most resilient species. We fitted logistic regression models to predict the probability of individual tree mortality and top-kill from fire injury indicators and tree characteristics. Besides the differences between the two main functional groups (coniferous, broadleaved), bole char height and crown volume scorched or consumed were important predictors of tree responses. Additionally, the main factor determining crown mortality on broadleaved species was bark thickness. The selected models performed well when tested with independent data obtained on four other wildfires. These models have several potential applications and can be useful to managers making pre-fire or post-fire decisions in mixed forest stands in the western Mediterranean Basin.</description>
		</publications>
		
		<publications id="0006">
			<title>Potential for CO2 emissions mitigation in Europe through prescribed burning in the context of the Kyoto Protocol</title>
			<date>2007</date>
			<citation>132</citation>
			<co_authors>
				<author>Caroline Narayan</author>
				<author>Jo van Brusselen</author>	
				<author>Andreas Schuck</author>
			</co_authors>
			<publication_source>
				<journal>Forest Ecology and Management</journal>
				<volume>251</volume>
				<issue>3</issue>
				<pag_min>164</pag_min>
				<pag_max>173</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>The current paper analyses the potential for prescribed burning techniques for mitigating carbon dioxide (CO2) emissions from forest fires and attempts to show quantitatively that it can be a means of achieving a net reduction of carbon emissions in the context of the Kyoto Protocol. The limited number of available studies suggests that significant reductions in CO2 emissions can be obtained and that prescribed burning can be a viable option for mitigating emissions in fire-prone countries. The present analysis shows that the potential reduction attained by prescribed burning as a percentage of the reduction in emissions required by the Kyoto Protocol varies from country to country. Out of the 33 European countries investigated, only in one the requirements of the Kyoto Protocol could potentially be achieved by applying prescribed burning, while three other nations showed a potential net CO2 emissions reduction of about 4–8% of the Kyoto requirements and the majority showed a reduction of less than 2%. This implies that prescribed burning can only make a significant contribution in those countries with high wildland fire occurrence. Over a 5-year period the emissions from wildfires in the European region were estimated to be approximately 11 million tonnes of CO2 per year, while with prescribed burning application this was estimated to be 6 million tonnes, a potential reduction of almost 50%. This means that for countries in the Mediterranean region it may be worthwhile to account for the reduction in emissions obtained when such techniques are applied.</description>
		</publications>
		
		<publications id="0007">
			<title>Forest fires in Portugal: dynamics, causes and policies</title>
			<date>2014</date>
			<citation>127</citation>
			<co_authors>
				<author>Paulo Mateus</author>
			</co_authors>
			<publication_source>
				<journal>Forest context and policies in portugal: present and future challenges</journal>
				<pag_min>97</pag_min>
				<pag_max>115</pag_max>
			</publication_source>
			<publisher>Forest context and policies in portugal: present and future challenges</publisher>
			<description>With a mean annual fire incidence of 3% of its forest and wildland surface Portugal is the European country most affected by wildfire. Forest fires dynamics in Portugal in the last four decades are presented (the fire regime and the corresponding losses) as well as the corresponding socioeconomic, environmental and policy drivers. The 20th century and on-going changes in land use (afforestation and rural abandonment) and climate are described. The policy options, strategies and plans established and implemented after the extreme forest fire seasons of 2003-2005 are discussed. Fire suppression is currently prioritized over fire prevention. However, the fire problem is rooted in in the socioeconomic factors behind fire occurrence (namely land use conflicts) and in the prevalence of unmanaged and flammable vegetation types. Forest and land management and civil protection have different objectives and both need to be tackled for effective mitigation of wildfire impacts. Managing vegetation to induce higher fire-resilience and changing human behaviour are needed and must be fully encouraged and supported. It follows that the current relative allocation of resources should shift from fire suppression to fire prevention under an integrated fire management philosophy. Mitigation of the wildfire problem depends on institutional stability and persistence in following a coherent fire management policy.</description>
		</publications>
	</researchers>
	
	
	
	
	
	<researchers id="0019">
		<name>Patricia Poeta</name>
		<area>Veterinary Medicine</area>
		<affiliation>University of Tras-os-Montes and Alto Douro</affiliation> 
		<research_interests> 
			<interest>Microbiology</interest>
			<interest>Public Health</interest>
			<interest>Antibiotic Resistance</interest>
		</research_interests>
		<email>ppoeta@utad.pt</email>
		<number_articles>379</number_articles>
		
		<publications id="0001">
			<title>Mechanisms of quinolone action and resistance: where do we stand?</title>
			<date>2017</date>
			<citation>311</citation>
			<co_authors>
				<author>Susana Correia</author>
				<author>Michel Hebraud</author>
				<author>Jose Luis Capelo</author>
				<author>Gilberto Igrejas</author>
			</co_authors>
			<publication_source>
				<source>Journal of medical microbiology</source>
				<volume>66</volume>
				<issue>5</issue>
				<pag_min>551</pag_min>
				<pag_max>559</pag_max>
			</publication_source>
			<publisher>Microbiology Society</publisher>
			<description>Quinolone antibiotics represent one of the most important classes of anti-infective agents and, although still clinically valuable, their use has been compromised by the increasing emergence of resistant strains, which has become a prevalent clinical problem. Quinolones act by inhibiting the activity of DNA gyrase and topoisomerase IV - two essential bacterial enzymes that modulate the chromosomal supercoiling required for critical nucleic acid processes. The acquisition of quinolone resistance is recognized to be multifactorial and complex. The main resistance mechanism consists of one or a combination of target-site gene mutations that alter the drug-binding affinity of target enzymes. However, other mechanisms such as mutations that lead to reduced intracellular drug concentrations, by either decreased uptake or increased efflux, and plasmid-encoded resistance genes producing either target protection proteins, drug-modifying enzymes or multidrug efflux pumps are known to contribute additively to quinolone resistance. The understanding of these different resistance mechanisms has improved significantly in recent years; however, many details remain to be clarified and the contribution of less-studied mechanisms still needs to be better elucidated in order to fully understand this phenotype.</description>
		</publications>
		
		<publications id="0002">
			<title>Antimicrobial resistance and the mechanisms implicated in faecal enterococci from healthy humans, poultry and pets in Portugal</title>
			<date>2006</date>
			<citation>115</citation>
			<co_authors>
				<author>Daniela Costa</author>
				<author>Jorge Rodrigues</author>
				<author>Carmen Torres</author>
			</co_authors>
			<publication_source>
				<journal>International Journal of Antimicrobial Agents</journal>
				<volume>27</volume>
				<issue>2</issue>
				<pag_min>131</pag_min>
				<pag_max>137</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>Antimicrobial resistance and the mechanisms implicated were studied in 440 enterococci (227 Enterococcus faecium, 177 Enterococcus faecalis, 32 Enterococcus hirae and 4 Enterococcus durans) recovered from 220 faecal samples of healthy humans, poultry and pets in Portugal. Higher levels of resistance were detected for ampicillin, tetracycline, erythromycin and chloramphenicol in poultry isolates (10.5%, 97%, 87.5% and 16%, respectively) compared with human isolates (0%, 26%, 31.5% and 5%, respectively); intermediate levels of resistance for these antibiotics were found in pet isolates. Thirty-three per cent of the E. faecium isolates of poultry origin showed quinupristin/dalfopristin resistance. High-level resistance to gentamicin or streptomycin was detected in 1-7% of isolates in our series of enterococci. The aac(6')-aph(2''), aph(3')-IIIa, erm(B) and tet(M) genes were demonstrated in most of the gentamicin-, kanamycin-, erythromycin- and tetracycline-resistant isolates, respectively. The vat(E) gene was found in 39% of the quinupristin/dalfopristin-resistant E. faecium isolates of poultry origin.</description>
		</publications>
		
		<publications id="0003">
			<title>Commensal gut bacteria: distribution of Enterococcus species and prevalence of Escherichia coli phylogenetic groups in animals and humans in Portugal</title>
			<date>2012</date>
			<citation>96</citation>
			<co_authors>
				<author>Nuno Silva</author>
				<author>Gilberto Igrejas</author>
				<author>Alexandre Gonçalves</author>
			</co_authors>
			<publication_source>
				<source>Annals of microbiology</source>
				<volume>62</volume>
				<issue>2</issue>
				<pag_min>449</pag_min>
				<pag_max>459</pag_max>
			</publication_source>
			<publisher>BioMed Central</publisher>
			<description>The gastrointestinal tract is continuously in contact with commensal bacteria that are composed of more than 500 different species, and has an important role in human nutrition and health, by promoting nutrient supply, preventing pathogen colonization and shaping and maintaining normal mucosal immunity. The present review demonstrates the distribution of the intestinal commensal bacteria Enterococcus spp. and the prevalence of Escherichia coli phylogenetic groups in animals and humans in Portugal. The enterococcal population described in this review includes 1,909 enterococcal isolates recovered from a series of fecal samples of different animals (horses, swine, ostriches, partridges, mullet fish, garden dormice, seagulls, pets, poultry, wild boars, birds of prey, and wild rabbits) and healthy and clinical humans. We also compared the phylogenetic groups of Escherichia coli isolates (n = 203) recovered from healthy humans and animals (poultry, ostriches, seagulls, wild boars, birds of prey, and pigs). Phenotypic and molecular analysis allowed the identifying of Enterococcus faecium as the predominant species followed by Enterococcus faecalis. In addition, the Escherichia coli data from different studies showed that isolates of the A and B1 phylogenetic groups are predominant in the gut flora of animal origin and the phylogenetic group B2 isolates were the most common in healthy human samples.</description>
		</publications>
		
		<publications id="0004">
			<title>Enterococci, from harmless bacteria to a pathogen</title>
			<date>2020</date>
			<citation>82</citation>
			<co_authors>
				<author>Sonia Ramos</author>
				<author>Vanessa Silva</author>
				<author>Maria de Lurdes Enes Dapkevicius</author>
				<author>Gilberto Igrejas</author>
			</co_authors>
			<publication_source>
				<source>Microorganisms</source>
				<volume>8</volume>
				<issue>8</issue>
				<pag_min>1118</pag_min>
				<pag_max>1118</pag_max>
			</publication_source>
			<publisher>MDPI</publisher>
			<description>Enterococci are gastrointestinal commensals whose hardiness allowed them to colonize very diverse environments, including soils, water, food, and feed. This ability to overcome adverse conditions makes enterococci problematic once they colonize hospital niches. Together with the malleability of their genomes, the capacity to acquire and disseminate determinants of antibiotic resistance has contributed to converting what was once just another opportunistic pathogen into a first-class clinical problem. This review discusses the dimension of the emergence of enterococcal resistance to key antimicrobial agents, the dissemination of this resistance, and its significance in terms of public health, with the aim of raising awareness of the need to devise and implement surveillance programs and more effective antibiotic stewardship.</description>
		</publications>
		
		<publications id="0005">
			<title>Antimicrobial activity of essential oils from Mediterranean aromatic plants against several foodborne and spoilage bacteria</title>
			<date>2013</date>
			<citation>69</citation>
			<co_authors>
				<author>Nuno Silva</author>
				<author>Sofia Alves</author>
				<author>Alexandre Gonçalves</author>
				<author>Joana S Amaral</author>
			</co_authors>
			<publication_source>
				<journal>Food Science and Technology International</journal>
				<volume>19</volume>
				<issue>6</issue>
				<pag_min>503</pag_min>
				<pag_max>510</pag_max>
			</publication_source>
			<publisher>Sage Publications</publisher>
			<description>The antimicrobial activity of essential oils extracted from a variety of aromatic plants, often used in the Portuguese gastronomy was studied in vitro by the agar diffusion method. The essential oils of thyme, oregano, rosemary, verbena, basil, peppermint, pennyroyal and mint were tested against Gram-positive (Listeria monocytogenes, Clostridium perfringens, Bacillus cereus, Staphylococcus aureus, Enterococcus faecium, Enterococcus faecalis, and Staphylococcus epidermidis) and Gram-negative strains (Salmonella enterica, Escherichia coli, and Pseudomonas aeruginosa). For most essential oils examined, S. aureus, was the most susceptible bacteria, while P. aeruginosa showed, in general, least susceptibility. Among the eight essential oils evaluated, thyme, oregano and pennyroyal oils showed the greatest antimicrobial activity, followed by rosemary, peppermint and verbena, while basil and mint showed the weakest antimicrobial activity. Most of the essential oils considered in this study exhibited a significant inhibitory effect. Thyme oil showed a promising inhibitory activity even at low concentration, thus revealing its potential as a natural preservative in food products against several causal agents of foodborne diseases and food spoilage. In general, the results demonstrate that, besides flavoring the food, the use of aromatic herbs in gastronomy can also contribute to a bacteriostatic effect against pathogens.</description>
		</publications>	
		
		<publications id="0006">
			<title>Antimicrobial resistance and phylogenetic groups in isolates of Escherichia coli from seagulls at the Berlengas nature reserve</title>
			<date>2009</date>
			<citation>65</citation>
			<co_authors>
				<author>H Radhouani</author>
				<author>G Igrejas</author>
				<author>A Goncalves</author>
				<author>L Vinue</author>
				<author>C Torres</author>
			</co_authors>
			<publication_source>
				<journal>Veterinary record</journal>
				<volume>165</volume>
				<issue>5</issue>
				<pag_min>138</pag_min>
				<pag_max>142</pag_max>
			</publication_source>
			<publisher>BMJ Publishing Group Limited</publisher>
			<description>Fifty-three faecal samples from yellow-legged gulls (Larus cachinnans) at the Berlengas nature reserve in Portugal were cultured on Levine agar plates not supplemented with antimicrobial agents, and one Escherichia coli colony was isolated and identified from each sample. The percentages of resistant isolates for each of the drugs were ampicillin (43.4 per cent), tetracycline (39.6 per cent), nalidixic acid (34.0 per cent), streptomycin (32.1 per cent), trimethoprim-sulfamethoxazole (SXT) (26.4 per cent), ciprofloxacin (18.9 per cent), chloramphenicol (18.9 per cent), gentamicin (7.5 per cent), tobramycin (7.5 per cent) amikacin (5.7 per cent) and amoxicillin-clavulanic acid (1.9 per cent). All the isolates were susceptible to cefoxitin, ceftazidime, cefotaxime, aztreonam and imipenem. The following resistance genes were detected: bla(TEM) (17 of 23 ampicillin-resistant isolates), tet(A) and/or tet(B) (18 of 21 tetracycline-resistant isolates), aadA (12 of 17 streptomycin-resistant isolates), cmlA (all chloramphenicol-resistant isolates), aac(3)-II with or without aac(3)-IV (all four gentamicin-resistant isolates), and sul1 and/or sul2 and/or sul3 (all 14 SXT-resistant isolates). The intI1 gene was detected in 10 of 14 SXT-resistant isolates, and three of them also contained class 2 integrons; four different gene cassette arrangements were identified among class 1 integrons (aadA, dfrA1+aadA1, dfrA12+orfF+aadA2 and sat+psp+aadA2) and one among the class 2 integrons (dfrA1+sat+aadA1). Ninety per cent of the isolates were included in the A or B1 phylogenetic groups.</description>
		</publications>
		
		<publications id="0007">
			<title>Molecular Epidemiology of Staphylococcus aureus Lineages in Wild Animals in Europe: A Review</title>
			<date>2020</date>
			<citation>48</citation>
			<co_authors>
				<author>Vanessa Silva</author>
				<author>Jose L Capelo</author>
				<author>Gilberto Igrejas</author>
			</co_authors>
			<publication_source>
				<source>Antibiotics</source>
				<volume>9</volume>
				<issue>3</issue>
				<pag_min>122</pag_min>
				<pag_max>122</pag_max>
			</publication_source>
			<publisher>MDPI</publisher>
			<description>Staphylococcus aureus is an opportunist pathogen that is responsible for numerous types of infections. S. aureus is known for its ability to easily acquire antibiotic resistance determinants. Methicillin-resistant S. aureus (MRSA) is a leading cause of infections both in humans and animals and is usually associated with a multidrug-resistant profile. MRSA dissemination is increasing due to its capability of establishing new reservoirs and has been found in humans, animals and the environment. Despite the fact that the information on the incidence of MRSA in the environment and, in particular, in wild animals, is scarce, some studies have reported the presence of these strains among wildlife with no direct contact with antibiotics. This shows a possible transmission between species and, consequently, a public health concern. The aim of this review is to better understand the distribution, prevalence and molecular lineages of MRSA in European free-living animals.</description>
		</publications>
	</researchers>
	
	
	
	
	
	<researchers id="0020">
		<name>Amelia M. Silva</name>
		<area>Biochemistry</area>
		<affiliation>University of Tras-os-Montes e Alto Douro</affiliation> 
		<research_interests> 
			<interest>Biologia Celular</interest>
			<interest>Bioquimica</interest>
			<interest>Biofisica</interest>
			<interest>Compostos Bioativos</interest>
		</research_interests>
		<email>amsilva@utad.pt</email>
		<number_articles>270</number_articles>
		
		<publications id="0001">
			<title>Nanotoxicology applied to solid lipid nanoparticles and nanostructured lipid carriers–a systematic review of in vitro data</title>
			<date>2014</date>
			<citation>386</citation>
			<co_authors>
				<author>Slavomira Doktorovova</author>
				<author>Eliana B Souto</author>
			</co_authors>
			<publication_source>
				<source>European Journal of Pharmaceutics and Biopharmaceutics</source>
				<volume>87</volume>
				<issue>1</issue>
				<pag_min>1</pag_min>
				<pag_max>18</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>Solid lipid nanoparticles (SLN) and nanostructured lipid carriers (NLC) were developed as alternative to other colloidal carriers. They were designed to overcome lipid nanoemulsions and liposomes in stability and ability to control the release of an encapsulated substance, and at the same time to be better tolerated than polymeric nanoparticles. Since the patenting of SLN discovery, large amount of data became available on the behaviour of these systems in vitro. SLN/NLC have many prerequisites to be a well tolerated carrier – the currently available data seem to confirm it, but there are also some contradictory results. In this review, we collected the available data from cytotoxicity, oxidative stress and hemocompatibility studies in vitro and analysed their outcomes. We also provide a summary of the available data in a form of reference table.</description>
		</publications>
		
		<publications id="0002">
			<title>Linalool bioactive properties and potential applicability in drug delivery systems</title>
			<date>2018</date>
			<citation>199</citation>
			<co_authors>
				<author>Irina Pereira</author>
				<author>Patricia Severino</author>
				<author>Ana C Santos</author>
				<author>Eliana B Souto</author>
			</co_authors>
			<publication_source>
				<source>Colloids and Surfaces B: Biointerfaces</source>
				<volume>171</volume>
				<pag_min>566</pag_min>
				<pag_max>578</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>The medicinal properties of essential oils from aromatic plants are known since antiquity. Currently, the technological innovation enabled the reinvention of the ancient plant knowledge leading to the identification and extraction of organic compounds present in essential oils. These organic compounds belong mainly to the terpene group and are accountable for the wide range of bioactive properties attributed to essential oils. Linalool (C10H18O), so-called 3,7-dimethyl-1,6-octadien-3-ol, is a monoterpene alcohol broadly present as a major constituent of plant essential oils, particularly lavender and coriander. Linalool per se is non-toxic and, according to recent in vitro and in vivo scientific studies, it has demonstrated to have a comprehensive range of bioactive properties, which can be exploited for pharmaceutic and cosmetic applications. The present review focuses on the anti-inflammatory, anticancer, anti-hyperlipidemic, antimicrobial, antinoceptive, analgesic, anxiolytic, antidepressive and neuroprotective properties of linalool. The advantages of the loading in nanotechnology-based drug delivery systems, with the purpose of enhancing its bioactive properties are also discussed.</description>	
		</publications>
		
		<publications id="0003">
			<title>Effects of combined physical exercise training on DNA damage and repair capacity: role of oxidative stress changes</title>
			<date>2015</date>
			<citation>90</citation>
			<co_authors>
				<author>Jorge Pinto Soares</author>
				<author>Maria Manuel Oliveira</author>
				<author>Francisco Peixoto</author>
				<author>Isabel Gaivao</author>
				<author>Maria Paula Mota</author>
			</co_authors>
			<publication_source>
				<journal>Age</journal>
				<volume>37</volume>
				<pag_min>1</pag_min>
				<pag_max>12</pag_max>
			</publication_source>
			<publisher>Springer International Publishing</publisher>
			<description>Regular physical exercise has been shown to be one of the most important lifestyle influences on improving functional performance, decreasing morbidity and all causes of mortality among older people. However, it is known that acute physical exercise may induce an increase in oxidative stress and oxidative damage in several structures, including DNA. Considering this, the purpose of this study was to identify the effects of 16 weeks of combined physical exercise in DNA damage and repair capacity in lymphocytes. In addition, we aimed to investigate the role of oxidative stress involved in those changes. Fifty-seven healthy men (40 to 74 years) were enrolled in this study. The sample was divided into two groups: the experimental group (EG), composed of 31 individuals, submitted to 16 weeks of combined physical exercise training; and the control group (CG), composed of 26 individuals, who did not undergo any specifically orientated physical activity. We observed an improvement of overall physical performance in the EG, after the physical exercise training. A significant decrease in DNA strand breaks and FPG-sensitive sites was found after the physical exercise training, with no significant changes in 8-oxoguanine DNA glycosylase enzyme activity. An increase was observed in antioxidant activity, and a decrease was found in lipid peroxidation levels after physical exercise training. These results suggest that physical exercise training induces protective effects against DNA damage in lymphocytes possibly related to the increase in antioxidant capacity.</description>
		</publications>
		
		<publications id="0004">
			<title>Citrus reticulata Blanco peels as a source of antioxidant and anti-proliferative phenolic compounds</title>
			<date>2018</date>
			<citation>86</citation>
			<co_authors>
				<author>Sandrine S Ferreira</author>
				<author> Fernando M Nunes</author>
			</co_authors>
			<publication_source>
				<journal>Industrial Crops and Products</journal>
				<volume>111</volume>
				<pag_min>141</pag_min>
				<pag_max>148</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>Citrus reticulata Blanco industrial use worldwide generates millions of tons of by-products, mainly peels, with negative environmental impact. The main purpose of this work was to compare the extraction efficiency of water and 70% ethanol, for recovering C. reticulata peel phenolic compounds. A simple solid phase extraction method was used for obtaining enriched phenolic extracts. There were no significant differences in the extraction efficiency for the two solvents used. The main components were hesperidin, naringin, tangeritin, and rutin, that accounted for nearly 86% of the total phenolics extracted. Solid phase extraction allowed a 4.5-fold enrichment in phenolics and antioxidant activity of the extracts. The anti-proliferative activity of the extracts was found to be dose-dependent but also dependent on the cell line. The solid phase extraction enriched phenolic extracts after 48 h exposure presented an IC50 of 174.5 ± 5.8 μg/mL (BT-474), 391.9 ± 15 μg/mL (Caco-2) and >500.00 μg/mL (HepG2). These results show that C. reticulata peels are a cheap and abundant source of antioxidant and potentially bioactive phenolic compounds.</description>
		</publications>
		
		<publications id="0005">
			<title>Sambucus nigra L. Fruits and Flowers: Chemical Composition and Related Bioactivities</title>
			<date>2022</date>
			<citation>47</citation>
			<co_authors>
				<author>Sandrine S Ferreira</author>
				<author>Fernando M Nunes</author>
			</co_authors>
			<publication_source>
				<source>Fernando M Nunes</source>
				<volume>38</volume>
				<issue>6</issue>
				<pag_min>1237</pag_min>
				<pag_max>1265</pag_max>
			</publication_source> 
			<publisher>Taylor Francis</publisher>
			<description>The genus Sambucus includes up to 18 species, among them the widely known and used Sambucus nigra L. Elderberry fruits are rich in sugars, organic acids as well in anthocyanins and other polyphenols. The beneficial health-promoting effects of elderberries and elderflowers are well known, including beneficial effects against degenerative diseases (cardiovascular and inflammatory diseases), cancer, and diabetes, presenting antioxidant, anti-inflammatory, immune-stimulating, chemo-preventive, and atheroprotective effects. Although currently they are mainly used in the food industry as food colorants and flavouring agents due to their phytochemical composition and related bioactivities, elderberries and elderflowers or their extracts are becoming attractive for other uses such as food supplements, nutraceutical ingredients and as raw materials for the pharmaceutical industries.</description>
		</publications>
		
		<publications id="0006">
			<title>Surface-tailored anti-HER2/neu-solid lipid nanoparticles for site-specific targeting MCF-7 and BT-474 breast cancer cells</title>
			<date>2019</date>
			<citation>46</citation>
			<co_authors>
				<author>Eliana B Souto</author>
				<author>Slavomira Doktorovova</author>
				<author>Joana R Campos</author>
				<author>Paula Martins-Lopes</author>
			</co_authors>
			<publication_source>
				<journal>European Journal of Pharmaceutical Sciences</journal>
				<volume>128</volume>
				<pag_min>27</pag_min>
				<pag_max>35</pag_max>
			</publication_source>
			<publisher>Elsevier</publisher>
			<description>CAB51, a compact antibody against human epithelial growth receptor 2 (HER2, ErbB2), has been linked to cationic Solid Lipid Nanoparticles (SLN) via streptavidin-biotin interaction and their targeting potential evaluated against breast cancer cells. The amount of streptavidin and biotinylated antibody was optimised by monitoring the mean complex size (intensity weighed average diameter), polydispersity index and immediate stability in phosphate buffer saline (PBS). The effect on MCF-7 and BT-474 cells was evaluated at concentrations of 0.01 mg/mL and 0.1 mg/mL (counted as solid lipid). Streptavidin adsorption onto SLN surface had no influence on cell viability. Linking the antibody showed a synergistic effect on cell viability at lowest concentration tested (0.01 mg/mL) which was lower than that observed after exposure to SLN alone or antibody alone. At the higher tested concentration (0.1 mg/mL), the observed toxicity was entirely governed by the inherent toxicity of the SLN themselves. Streptavidin adsorption had no effect on accumulation in cells, while the antibody-containing complexes showed clearly increased internalisation in both cell lines. In HER2/neu positive BT-474 higher internalisation was observed than in HER2/neu negative MCF-7.</description>
		</publications>
		
		<publications id="0007">
			<title>Copper induced apoptosis in Caco-2 and Hep-G2 cells: Expression of caspases 3, 8 and 9, AIF and p53</title>
			<date>2016</date>
			<citation>40</citation>
			<co_authors>
				<author>Stefanie Santos</author>
				<author>Manuela Matos</author>
				<author>Sandra M Monteiro</author>
				<author>Ana R alvaro</author>
			</co_authors>
			<publication_source> 
				<journal>Comparative Biochemistry and Physiology Part C: Toxicology  Pharmacology</journal>
				<volume>185</volume>
				<pag_min>138</pag_min>
				<pag_max>146</pag_max>
			</publication_source> 
			<publisher>Elsevier</publisher>
			<description>Copper (Cu) is an essential trace metal needed to ensure cell function. However, when present at high concentrations it becomes toxic to organisms. Cell death, induced by toxic levels of copper, was previously observed in in vitro studies. However, there is no consensus about the cell death pathway induced by Cu and it is still not known whether this occurs as a result of the direct action of the metal or by indirect effects. In the present work, we intend to identify the influence of different Cu concentrations in the induction of apoptosis and to explore the potential signaling pathways, using two different in vitro cell culture models (Caco-2 and Hep-G2). Cells were exposed, during 6, 12, 24 and 48h, to Cu concentrations corresponding to IC50 and 1/8 of IC50, according to the viability assays. Then, considering the different apoptosis pathways, the expression of caspases 3, 8 and 9, apoptosis inducing factor (AIF) and p53 genes was analyzed by quantitative real time PCR. The results suggested that different Cu concentrations could trigger different apoptotic pathways, at different times of exposure. In both cell lines, apoptosis seems to be initiated by caspase independent pathway and intrinsic pathway, followed by extrinsic pathway. In conclusion, this study demonstrates that Cu induces the activation of apoptosis through caspase dependent and independent pathways, also suggesting that apoptosis activation mechanism is dependent on the concentration, time of exposure to Cu and cell type.</description>
		</publications>	
	</researchers>
</catalog>